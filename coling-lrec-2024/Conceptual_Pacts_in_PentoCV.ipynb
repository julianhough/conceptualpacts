{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Conceptual Pacts over time in PentoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep-translator in /Users/julianhough/miniforge3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from deep-translator) (4.12.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from deep-translator) (2.28.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n",
      "Requirement already satisfied: scikit-learn in /Users/julianhough/miniforge3/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: sciPy in /Users/julianhough/miniforge3/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from sciPy) (1.24.2)\n",
      "Requirement already satisfied: nltk in /Users/julianhough/miniforge3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/julianhough/miniforge3/lib/python3.10/site-packages (from nltk) (2023.5.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install google_trans_new\n",
    "!pip install -U deep-translator\n",
    "!pip install scikit-learn\n",
    "!pip install sciPy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# You need PentoRef python on your path\n",
    "sys.path.append(\"../../pentoref/code/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pentoref.IO as IO\n",
    "import sqlite3 as sqlite\n",
    "#from google_trans_new import google_translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#l = LabelEncoder()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "#nltk.download('punkt')  # if using stemming in German\n",
    "\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pentoref.IOutils import clean_utt\n",
    "from machine_learning_utils import calculate_mcnemar_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_reparanda(utt):\n",
    "    \"removes the content between ( and + though leaves those intact\"\n",
    "    repair_depth = 0\n",
    "    cleaned_utt = \"\"\n",
    "    for c in utt:\n",
    "        if c == \"+\":\n",
    "            repair_depth-=1\n",
    "        elif c == \"(\":\n",
    "            repair_depth+=1\n",
    "        elif repair_depth>0:\n",
    "            continue\n",
    "        cleaned_utt+=c\n",
    "    assert repair_depth==0, \"repair depth not 0:\" + utt\n",
    "    return cleaned_utt\n",
    "clean_utt(remove_reparanda(\"(( hello + hello) + {F um } hello)\"))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create databases if required\n",
    "if False:   # make True if you need to create the databases from the derived data\n",
    "    for corpus_name in [\"PENTOCV\"]: # \"TAKE\", \"TAKECV\", \n",
    "        data_dir = \"../../pentoref/{0}_PENTOREF\".format(corpus_name)\n",
    "        dfwords, dfutts, dfrefs, dfscenes, dfactions = IO.convert_subcorpus_raw_data_to_dataframes(data_dir)\n",
    "        IO.write_corpus_to_database(\"{0}.db\".format(corpus_name),\n",
    "                                    corpus_name, dfwords, dfutts, dfrefs, dfscenes, dfactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utts ['gameID', 'uttID', 'starttime', 'endtime', 'utt', 'utt_clean', 'role', 'speaker']\n",
      "words ['gameID', 'uttID', 'position', 'word', 'lemma', 'tag']\n",
      "refs ['refID', 'gameID', 'uttID', 'text', 'id', 'piece', 'location']\n",
      "scenes ['timestampID', 'gameID', 'pieceID', 'position_global', 'position_x', 'position_y', 'shape', 'shape_distribution', 'shape_orientation', 'shape_skewness_horizontal', 'shape_skewness_vertical', 'shape_edges', 'colour', 'colour_distribution', 'colour_hsv', 'colour_rgb']\n",
      "actions ['gameID', 'starttime', 'endtime', 'hand', 'action', 'piece']\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "CORPUS = \"PENTOCV\"\n",
    "db = sqlite.connect(\"{0}.db\".format(CORPUS))\n",
    "cursor = db.cursor()\n",
    "# get the table column header names\n",
    "print(\"utts\", [x[1] for x in cursor.execute(\"PRAGMA table_info(utts)\")])\n",
    "print(\"words\", [x[1] for x in cursor.execute(\"PRAGMA table_info(words)\")])\n",
    "print(\"refs\", [x[1] for x in cursor.execute(\"PRAGMA table_info(refs)\")])\n",
    "print(\"scenes\", [x[1] for x in cursor.execute(\"PRAGMA table_info(scenes)\")])\n",
    "print(\"actions\", [x[1] for x in cursor.execute(\"PRAGMA table_info(actions)\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get utterances from certain time periods in each experiment or for certain episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    translator = GoogleTranslator(source='de', target='en')\n",
    "\n",
    "    for row in db.execute(\"SELECT gameID, starttime, speaker, utt_clean, utt FROM utts\" + \\\n",
    "                       # \" WHERE starttime >= 200 AND starttime <= 300\" + \\\n",
    "                         ' WHERE gameID = \"r1_1_1_b\"' + \\\n",
    "                        \" ORDER BY gameID, starttime\"):\n",
    "        print(row)\n",
    "        line = row[3]\n",
    "        print(line)\n",
    "        if not line:\n",
    "            continue\n",
    "        translate_text = translator.translate(line,lang_src='de',lang_tgt='en') \n",
    "        print(translate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just focus on single pieces, not references to multiple pieces\n",
    "good_pieces = [\"X\", \"Y\", \"P\", \"N\", \"U\", \"F\", \"Z\", \"L\", \"T\", \"I\", \"W\", \"V\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8'])\n",
      "[('gelben stein', 'r3_B', 8886, 129.257, 'U'), ('das rote kreuz', 'r3_B', 8893, 141.68, 'X'), ('daran', 'r3_B', 8897, 148.215, 'U'), ('der orange stein', 'r3_B', 8897, 148.215, 'L'), ('das l', 'r3_A', 7900, 151.582, 'L'), ('daran', 'r3_B', 8902, 157.211, 'X'), ('das grüne t', 'r3_B', 8902, 157.211, 'T'), ('der blaue winkel', 'r3_B', 8931, 219.17, 'V'), ('das', 'r3_B', 8936, 226.213, 'V'), ('daran', 'r3_B', 8940, 235.547, 'V'), ('der gelbe stein', 'r3_B', 8940, 235.547, 'U'), ('er', 'r3_B', 8943, 244.98, 'U'), ('der graue stein', 'r3_B', 8946, 248.207, 'F'), ('den gelben', 'r3_B', 8953, 265.246, 'U'), ('der grüne stein', 'r3_B', 8959, 272.863, 'W'), ('den gelben', 'r3_B', 8967, 284.31, 'U'), ('den grauen', 'r3_B', 8967, 284.31, 'F'), ('das grüne t', 'r3_B', 8970, 287.759, 'T'), ('das grüne t', 'r3_B', 8977, 300.292, 'T'), ('der lila stein', 'r3_B', 8977, 300.292, 'N')]\n"
     ]
    }
   ],
   "source": [
    "references_per_pair = {}  # all data will be stored here with keys =pairnum\n",
    "for row in db.execute(\"SELECT id, gameID, text, uttID FROM refs\" + \\\n",
    "#for row in db.execute(\"SELECT shape, colour, orientation, gridPosition, gameID, pieceID FROM scenes\" + \\\n",
    "                     \" ORDER by gameID\"):\n",
    "    if False: \n",
    "        # TAKE\n",
    "        isTarget = db.execute('SELECT refID FROM refs WHERE gameID =\"' + row[4] + '\" AND pieceID =\"' + row[5] + '\"')\n",
    "        target = False \n",
    "        for r1 in isTarget:\n",
    "            target = True\n",
    "        if not target:\n",
    "            continue\n",
    "\n",
    "    #TAKE\n",
    "    #shape, colour, orientation, gridPosition, gameID, pieceID = row\n",
    "    #piece = colour  #+ \"_\" + shape #shape + \"_\" + colour\n",
    "    \n",
    "    #PENTOCV\n",
    "    piece, gameID, text, uttID = row\n",
    "    \n",
    "    # some manual corrections of disfluencies\n",
    "    error_found = False\n",
    "    if \"(der + (das + das + das) grüne $m)\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"(der + (das + (das + das ))) grüne $m)\"\n",
    "    elif \"(der {f äh:m:} + der) + der) winkel\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"((der {f äh:m:} + der) + der) winkel\"\n",
    "    elif \"das {br- + blaue} lange\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"das (br- + blaue) Lange\"\n",
    "    elif \"das: ({f äh} ja .) andere blaue $z\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"das: ({f äh} ja . + ) andere blaue $z\"\n",
    "    elif \"den (ist das rosa oben) block\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"den (ist das rosa oben + ) block\"\n",
    "    elif \"\"\"(<p=\"dieses\">die-</p> (genau) + das) element\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"(<p=\"dieses\">die-</p> (genau  +) + das) element\"\"\"\n",
    "    elif \"\"\"das $t (dieser blaue <p=\"senkrecht\">senk-</p>\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"das $t (dieser blaue <p=\"senkrecht\">senk-</p>+)\"\"\"\n",
    "    elif \"\"\"es\">wie's</v> jetzt <v=\"ist\">is'</v> und steckst <ref id=\"f\" piece=\"target\"> es\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"wie's jetzt <v=\"ist\">is'</v> und steckst <ref id=\"f\" piece=\"target\"> es\"\"\"\n",
    "    elif \"\"\"es\">wie's</v>\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"wie's\"\"\"\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    clean_text = clean_utt(remove_reparanda(text.lower()))\n",
    "    assert clean_text!=\"\", count\n",
    "    \n",
    "    \n",
    "    pair_num = gameID.split(\"_\")[0]\n",
    "    # get speaker\n",
    "    speaker = list(db.execute('SELECT speaker FROM utts WHERE uttID =' + str(uttID)))[0][0]\n",
    "    speaker = pair_num + \"_\" + speaker\n",
    "    \n",
    "    end_time =  float(list(db.execute('SELECT endtime FROM utts WHERE uttID =' + str(uttID)))[0][0])\n",
    "    \n",
    "    if error_found:\n",
    "        print(\"replacing with\", text, \"at\", gameID, uttID, end_time)\n",
    "    #if not pair_num == PAIR_NUM:\n",
    "    #    continue\n",
    "    \n",
    "        \n",
    "    if piece not in good_pieces:\n",
    "        continue\n",
    "        \n",
    "    if not references_per_pair.get(pair_num):\n",
    "        references_per_pair[pair_num] = []\n",
    "    \n",
    "    \n",
    "    if \"_s\" in gameID:\n",
    "        continue # just get the build phases for now due to inconsistent labelling\n",
    "\n",
    "    references_per_pair[pair_num].append((clean_text, speaker, uttID, end_time, piece))\n",
    "\n",
    "\n",
    "    # sort by end time\n",
    "for pair_num in references_per_pair.keys():\n",
    "    ref_list = references_per_pair[pair_num]\n",
    "    ref_list = sorted(ref_list, key=lambda x:x[3])\n",
    "    references_per_pair[pair_num] = ref_list\n",
    "\n",
    "print(references_per_pair.keys())\n",
    "print(references_per_pair['r3'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hyp 1: the referring expression is no less predictable given the other mentions for a given piece at a given time point\n",
    "# regardless of order (so like a language model, all mentions are equally likely and there's no correlation with time)\n",
    "# of how p(ref|previous) =< p(ref|all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUMPTION: only dealing with full names, not anaphors # TODO for this paper may actually leave them in\n",
    "if True: # False TODO for this paper may actually leave them in\n",
    "    anaphors = [\"es\", \"das\", 'er', \"da\", \"der\", \"ihn\", \"den\", \"sie\", \"die\", \"damit\", \"daran\", \"dem\"]\n",
    "    for pair_num in references_per_pair.keys():\n",
    "        refs = references_per_pair[pair_num]\n",
    "        references_per_pair[pair_num] = list(filter(lambda x:x[0] not in anaphors, refs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL REFERENCES 1899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('F', 134),\n",
       " ('I', 139),\n",
       " ('L', 161),\n",
       " ('N', 197),\n",
       " ('P', 147),\n",
       " ('T', 191),\n",
       " ('U', 144),\n",
       " ('V', 162),\n",
       " ('W', 170),\n",
       " ('X', 177),\n",
       " ('Y', 162),\n",
       " ('Z', 115)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats for whole corpus and pieces\n",
    "piece_counter = Counter()\n",
    "for pair_num in references_per_pair.keys():\n",
    "    list_refs = [ref[4] for ref in references_per_pair[pair_num]]\n",
    "    piece_counter.update(list_refs) \n",
    "print(\"TOTAL REFERENCES\", piece_counter.total())\n",
    "sorted(piece_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1 100 refs\n",
      "Counter({'P': 15, 'T': 14, 'L': 12, 'Y': 11, 'X': 9, 'U': 8, 'W': 8, 'I': 7, 'N': 6, 'F': 5, 'V': 3, 'Z': 2})\n",
      "r2 170 refs\n",
      "Counter({'W': 21, 'Y': 19, 'X': 17, 'T': 16, 'F': 16, 'I': 14, 'N': 14, 'Z': 13, 'L': 13, 'U': 11, 'P': 9, 'V': 7})\n",
      "r3 265 refs\n",
      "Counter({'T': 34, 'X': 26, 'V': 26, 'N': 24, 'L': 22, 'Z': 21, 'W': 20, 'I': 20, 'Y': 19, 'U': 18, 'P': 18, 'F': 17})\n",
      "r4 285 refs\n",
      "Counter({'U': 33, 'V': 31, 'X': 30, 'N': 29, 'T': 26, 'Y': 25, 'W': 23, 'I': 21, 'P': 20, 'F': 18, 'Z': 17, 'L': 12})\n",
      "r5 143 refs\n",
      "Counter({'Y': 17, 'X': 16, 'U': 15, 'N': 15, 'W': 14, 'P': 12, 'V': 11, 'L': 11, 'T': 10, 'Z': 10, 'F': 7, 'I': 5})\n",
      "r6 258 refs\n",
      "Counter({'N': 33, 'I': 26, 'P': 26, 'T': 26, 'L': 25, 'Y': 23, 'W': 23, 'V': 22, 'U': 17, 'F': 17, 'X': 12, 'Z': 8})\n",
      "r7 352 refs\n",
      "Counter({'N': 41, 'X': 38, 'L': 35, 'V': 33, 'I': 30, 'F': 29, 'W': 28, 'T': 25, 'P': 25, 'U': 23, 'Y': 23, 'Z': 22})\n",
      "r8 326 refs\n",
      "Counter({'T': 40, 'N': 35, 'W': 33, 'L': 31, 'V': 29, 'X': 29, 'F': 25, 'Y': 25, 'P': 22, 'Z': 22, 'U': 19, 'I': 16})\n",
      "237.375\n",
      "r1 137.375\n",
      "r2 67.375\n",
      "r3 27.625\n",
      "r4 47.625\n",
      "r5 94.375\n",
      "r6 20.625\n",
      "r7 114.625\n",
      "r8 88.625\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at how one pair talks about a piece over time\n",
    "all_piece_counts = []\n",
    "for pair_num in references_per_pair.keys():\n",
    "    print(pair_num, len(references_per_pair[pair_num]), \"refs\")\n",
    "    c = Counter([ref[4] for ref in references_per_pair[pair_num]])\n",
    "    print(c)\n",
    "    for c,v in c.items():\n",
    "        all_piece_counts.append(v)\n",
    "mean_length =np.mean([len(v) for k,v in references_per_pair.items()])\n",
    "print(mean_length)\n",
    "# select test as closest to mean length, heldout as second closest\n",
    "for pair_num in references_per_pair.keys():\n",
    "    print(pair_num, abs(len(references_per_pair[pair_num])-mean_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median 19.5\n",
      "lower 2\n",
      "upper 41\n"
     ]
    }
   ],
   "source": [
    "print(\"median\", np.median(all_piece_counts))\n",
    "print(\"lower\", np.min(all_piece_counts))\n",
    "print(\"upper\", np.max(all_piece_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create language models and language model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 'r6'  # take this out of X-val and save for final testing\n",
    "HELDOUT = 'r3'  # used mainly for holding out folds in getting the LM features\n",
    "EXTRA_HELDOUT = 'r7' # note this should probably be r4, in principle shouldn't affect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('das braune klötzchen', 'r1_B', 6003, 407.971, 'Y')\n",
      "('das braune', 'r1_B', 6011, 423.967, 'Y')\n",
      "('braune', 'r1_B', 6013, 425.93, 'Y')\n",
      "('das blaue', 'r1_B', 6041, 477.407, 'Y')\n",
      "('das braune', 'r1_B', 6055, 493.203, 'Y')\n",
      "('das braune', 'r1_B', 6056, 494.18, 'Y')\n",
      "('das braune klötzchen', 'r1_B', 6119, 608.51, 'Y')\n",
      "('das braune gebilde', 'r1_B', 6179, 690.209, 'Y')\n",
      "('das braune gebilde', 'r1_B', 6185, 701.471, 'Y')\n",
      "('braun', 'r1_B', 6193, 716.532, 'Y')\n",
      "('das braune', 'r1_B', 6290, 834.766, 'Y')\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at how one pair talks about a piece over time\n",
    "\n",
    "for ref in filter(lambda x:x[4] == 'Y', references_per_pair['r1']):\n",
    "    print(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_language_model():\n",
    "    \n",
    "    def __init__(self, order, smoothing_k=0.0001):\n",
    "        self.order = order\n",
    "        self.ngrams = {}\n",
    "        self.num_training_sents = 0\n",
    "        self.smoothing_k = smoothing_k\n",
    "        for o in range(1, order+1):\n",
    "            self.ngrams[o] = {}\n",
    "        self.unigram_denom = 0\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    \n",
    "    def add_ngram_to_model(self, ngram, order):\n",
    "        ngram_text = \"_\".join(ngram)\n",
    "        if not self.ngrams.get(order).get(ngram_text):\n",
    "            self.ngrams[order][ngram_text] = 0\n",
    "        self.ngrams[order][ngram_text]+=1\n",
    "\n",
    "        \n",
    "    def minus_ngram_from_model(self, ngram, order):\n",
    "        ngram_text = \"_\".join(ngram)\n",
    "        if not self.ngrams.get(order).get(ngram_text):\n",
    "            self.ngrams[order][ngram_text] = 0\n",
    "            \n",
    "        self.ngrams[order][ngram_text]-=1\n",
    "      \n",
    "    \n",
    "    def add_counts_from_other_model(self, other_lm):\n",
    "        assert(other_lm.order == self.order)\n",
    "        for n in range(1, self.order+1):\n",
    "            #print(\"ngrams before\", self.ngrams)\n",
    "            combined_keys = list(self.ngrams[n].keys() | other_lm.ngrams[n].keys())\n",
    "            #print(\"other after\", self.ngrams)\n",
    "            for key in combined_keys:\n",
    "                \n",
    "                count_self = self.ngrams[n].get(key)\n",
    "                if not count_self:\n",
    "                    count_self = 0\n",
    "                count_other = other_lm.ngrams[n].get(key)\n",
    "                if not count_other:\n",
    "                    count_other = 0\n",
    "                #print(key, count_self, count_other)\n",
    "                self.ngrams[n][key] = count_self + count_other\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(\"update\")\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def train(self, sents):\n",
    "        for sent in sents:\n",
    "            padded = [\"<s>\"] * (self.order -1) + sent + [\"</s>\"]\n",
    "            for i in range(self.order-1, len(padded)):\n",
    "                for n in range(1, self.order+1):\n",
    "                    target = padded[i]\n",
    "                    context = padded[i-(n-1):i]\n",
    "                    self.add_ngram_to_model(context + [target], n)\n",
    "                    if n> 1:\n",
    "                        self.add_ngram_to_model(context, n-1)\n",
    "                    \n",
    "            self.num_training_sents += 1\n",
    "        #print(self.ngrams)\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def de_train(self, sents):\n",
    "        # take away these counts\n",
    "        for sent in sents:\n",
    "            padded = [\"<s>\"] * (self.order -1) + sent + [\"</s>\"]\n",
    "            for i in range(self.order-1, len(padded)):\n",
    "                for n in range(1, self.order+1):\n",
    "                    target = padded[i]\n",
    "                    context = padded[i-(n-1):i]\n",
    "                    self.minus_ngram_from_model(context + [target], n)\n",
    "        #print(self.ngrams)\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def prob_lidstone(self, ngram, order):\n",
    "        \"\"\"Add-k smoothing using the discount parameter for this model.\"\"\"\n",
    "        ngram_text =  \"_\".join(ngram)\n",
    "        ngram_count = self.ngrams[order].get(ngram_text)\n",
    "        if not ngram_count:\n",
    "            ngram_count = 0\n",
    "        num = ngram_count + self.smoothing_k\n",
    "        if order == 1:\n",
    "            denom = self.unigram_denom + (self.smoothing_k * self.vocab_size)\n",
    "            if self.unigram_denom == 0:\n",
    "                #print(\"warning no training, returning k/40/10\")\n",
    "                denom = 150 + (self.smoothing_k * 15)\n",
    "            #print(denom)\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_text = \"_\".join(context)\n",
    "            #print(context_text)\n",
    "            context_count = self.ngrams[order-1].get(context_text)\n",
    "            if not context_count:\n",
    "                context_count = 0\n",
    "            #print(context_count)\n",
    "            all_contexts = len(self.ngrams[order-1].keys())\n",
    "            denom = context_count + (self.smoothing_k * all_contexts)\n",
    "            if self.unigram_denom == 0 or context_count == 0:\n",
    "                #print(\"warning no training, returning k/40/10\")\n",
    "                denom = 40 + (self.smoothing_k * 10)\n",
    "            #print(denom)\n",
    "        assert num/denom <=1 and num/denom >0, str(num) + \" \" + str(denom) + \" \" + str(num/denom) + \" \" + str(ngram) + \" n=\" + str(order) + \" k=\" + str(self.smoothing_k)\n",
    "        return num/denom\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'I': 4, '<s>': 2, 'like': 4, 'bill': 2, '</s>': 2, 'mary': 2}, 2: {'<s>_I': 4, '<s>_<s>': 2, 'I_like': 4, 'like_bill': 2, 'bill_</s>': 1, 'like_mary': 2, 'mary_</s>': 1}, 3: {'<s>_<s>_I': 2, '<s>_I_like': 2, 'I_like_bill': 1, 'like_bill_</s>': 1, 'I_like_mary': 1, 'like_mary_</s>': 1}}\n",
      "0.23404255319148937\n",
      "{1: {'I': 4, '<s>': 2, 'like': 4, 'bill': 2, 'today': 2, '</s>': 2, 'mary': 2}, 2: {'<s>_I': 4, '<s>_<s>': 2, 'I_like': 4, 'like_bill': 2, 'bill_today': 2, 'today_</s>': 1, 'like_mary': 2, 'mary_</s>': 1}, 3: {'<s>_<s>_I': 2, '<s>_I_like': 2, 'I_like_bill': 1, 'like_bill_today': 1, 'bill_today_</s>': 1, 'I_like_mary': 1, 'like_mary_</s>': 1}}\n",
      "{1: {'I': 8, '<s>': 4, 'like': 8, 'bill': 4, '</s>': 4, 'mary': 4, 'today': 2}, 2: {'<s>_I': 8, '<s>_<s>': 4, 'I_like': 8, 'like_bill': 4, 'bill_</s>': 1, 'like_mary': 4, 'mary_</s>': 2, 'today_</s>': 1, 'bill_today': 2}, 3: {'<s>_<s>_I': 4, '<s>_I_like': 4, 'I_like_bill': 2, 'like_bill_</s>': 1, 'I_like_mary': 2, 'like_mary_</s>': 2, 'bill_today_</s>': 1, 'like_bill_today': 1}}\n"
     ]
    }
   ],
   "source": [
    "m = mini_language_model(3, smoothing_k=0.1)\n",
    "m.train([[\"I\", \"like\", \"bill\"], [\"I\", \"like\", \"mary\"]])\n",
    "print(m.ngrams)\n",
    "print(m.prob_lidstone([\"I\", \"like\", \"mary\"], 3))\n",
    "\n",
    "m2 = mini_language_model(3)\n",
    "m2.train([[\"I\", \"like\", \"bill\", 'today'], [\"I\", \"like\", \"mary\"]])\n",
    "print(m2.ngrams)\n",
    "m.add_counts_from_other_model(m2)\n",
    "print(m.ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs_in_timerange_for_piece(pair_num, piece, time_uttend_start, time_uttend_end, references_per_pair):\n",
    "    \"\"\"Returns a list of the refs from a certain time start to a time end in order for a given piece.\n",
    "    NOTE FOR NOW refs in the same utterance are either all included or all excluded from this method\"\"\"\n",
    "    if not references_per_pair.get(pair_num):\n",
    "        return []\n",
    "    return list(filter(lambda x:(x[3]>=time_uttend_start and x[3]<time_uttend_end) and x[4]==piece,\n",
    "                       references_per_pair[pair_num]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('der orange stein', 'r3_B', 8897, 148.215, 'L'),\n",
       " ('das l', 'r3_A', 7900, 151.582, 'L'),\n",
       " ('der orange winkel', 'r3_B', 9019, 374.137, 'L'),\n",
       " ('der orange winkel', 'r3_B', 9042, 477.238, 'L')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_refs_in_timerange_for_piece('r3', 'L', 0, 687.481, references_per_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Is there an increasing similarity to the previous references as the dialogue progresses?\n",
    "# Test 2: does a reference have higher average similarity to other references in the same dialogue compared to those in different dialogyes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Create reference resolution training + test data with features from the language models and Optimize Params\n",
    "* TODO setting - exclude and re-compute based on pieces 'in play', or assume the robot doesn't know - try both\n",
    "* Try 1) global model only without active learning, 2) global + local standard 3) global and local incrementally/dynamically updating, see the difference in ref res. Measures on accuracy + surprisal.\n",
    "* Naive bayes with individual language models for each piece + marginalization? OR model can work independently with any probabilistic classifier as a joint probability?\n",
    "* Get an optimal weighting between the local, incremental model and global models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': 1, 'K': 2, 'Y': 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rank_dict_for_prob_dict(prob_dict):\n",
    "    rank_dict = {}\n",
    "    rank = 0\n",
    "    for item in sorted(prob_dict.items(), key=lambda x:x[1]):\n",
    "        rank+=1\n",
    "        rank_dict[item[0]] = rank\n",
    "    return rank_dict\n",
    "get_rank_dict_for_prob_dict({\"K\": 0.34, \"Y\": 0.35, \"X\": 0.33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a_list):\n",
    "    averages = []\n",
    "    total = 0\n",
    "    for i, a in enumerate(a_list):\n",
    "        total+=a\n",
    "        averages.append(total/(i+1))\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_on_test_folds(k, pair_piece_lms_orig, folds, prob_model):\n",
    "    \"\"\"Returns the mean of the cross entropy on the folds using the appropriate lm.\n",
    "    Assumes folds will be in correct order w.r.t. time of reference.\"\"\"\n",
    "    # firstly, duplicate the language model to avoid any side effects\n",
    "    #print(k)\n",
    "    pair_piece_lms = copy.deepcopy(pair_piece_lms_orig)\n",
    "    # Set the k for all to be the same\n",
    "    for pair_num in pair_piece_lms.keys():\n",
    "        for piece in pair_piece_lms[pair_num].keys():\n",
    "            pair_piece_lms[pair_num][piece].smoothing_k = k[0]\n",
    "            \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "            # assume we know the correct ref piece\n",
    "            piece = ref_info[4]\n",
    "            lm = pair_piece_lms[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            prob = 0\n",
    "            for j in range(lm.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                prob += log(lm.prob_lidstone(ngram, lm.order))\n",
    "            s += prob\n",
    "            count +=1\n",
    "            if prob_model == 'self':\n",
    "                # self language model, starting empty for a piece\n",
    "                # then will train as it encounters each piece\n",
    "                #print(pair_piece_lms[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "                pair_piece_lms[pair_num][piece].train([ref_info[0].split()])\n",
    "    \n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "    #print(k, cross_entropies, np.mean(cross_entropies))\n",
    "    return np.mean(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_for_lowest_entropy_on_fold_test(folds, pieces, n_range, k_val_range, prob_model='self'):\n",
    "    \"\"\"An x-fold process run multiple times to find the best params of n and k (smoothing constant)\n",
    "    for an ngram model\n",
    "    \"\"\"\n",
    "    results = {}  # store results for each model\n",
    "    for test_n in n_range:\n",
    "        pair_piece_lms = {}  # will produce all lms for each speaker for each piece with order test_n\n",
    "        for test_pair in folds.keys():\n",
    "            # test pair is the list of refs to get the entropy results on in both cases\n",
    "            # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "            # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "                  #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "            pair_piece_lms[test_pair] = {}  # key piece, value language model for that piece\n",
    "            for piece in pieces:\n",
    "                if prob_model == 'other':\n",
    "                    # train the model (smoothing can be changed as it doesn't affect training)\n",
    "\n",
    "                    training_refs = []\n",
    "                    \n",
    "                    for other_pair_num in folds.keys():\n",
    "                        if other_pair_num == test_pair:\n",
    "                            continue\n",
    "                        training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])])  \n",
    "                    training_fold = [r[0] for r in training_refs]\n",
    "                    #print(\"length of training fold for lm2\", piece, test_pair, len(training_fold))\n",
    "                    if len(training_fold) <1:\n",
    "                        print(\"not enough data\", piece, pair_num)\n",
    "                        continue\n",
    "                    pair_piece_lms[test_pair][piece] = mini_language_model(test_n, smoothing_k=0.0001)    \n",
    "                    pair_piece_lms[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "\n",
    "                elif prob_model == 'self':\n",
    "                    # just initialize lm for piece, no need to train\n",
    "                    pair_piece_lms[test_pair][piece] = mini_language_model(test_n, smoothing_k=0.0001)    \n",
    "             \n",
    "            \n",
    "        # run nelder mead to get best k for this n\n",
    "        best = optimize.minimize(\n",
    "                mean_cross_entropy_on_test_folds,\n",
    "                k_val_range[0],    # first argument that to be optimized (k)\n",
    "                args=(pair_piece_lms, folds, prob_model),  # other arguments to the function\n",
    "                method='Nelder-Mead', # use nelder mead to find minima\n",
    "                tol=0.0001, # to this degree of error\n",
    "                options={'disp': False},\n",
    "                bounds=[k_val_range]\n",
    "        )\n",
    "        best_k, best_ce = best.x[0], best.fun\n",
    "        print(\"best k\", best_k, best_ce)\n",
    "        results[test_n] = (best_k, best_ce)\n",
    "    \n",
    "    print(results)\n",
    "    n = min(results.items(), key=lambda x:x[1][1])[0]\n",
    "    k = results[n][0]\n",
    "               \n",
    "    return n, k, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r6 r3\n",
      "best k 1.0 6.86497954274358\n",
      "best k 1.0 6.494462605070569\n",
      "best k 1.0 7.703398867971607\n",
      "{1: (1.0, 6.86497954274358), 2: (1.0, 6.494462605070569), 3: (1.0, 7.703398867971607)}\n",
      "best k 1.0 8.3721941211424\n",
      "best k 0.556662793500001 7.461818951993145\n",
      "best k 0.2273443935000005 8.545814773734284\n",
      "{1: (1.0, 8.3721941211424), 2: (0.556662793500001, 7.461818951993145), 3: (0.2273443935000005, 8.545814773734284)}\n",
      "best k 1.0 6.915151572870145\n",
      "best k 1.0 6.45897208116136\n",
      "best k 1.0 7.63492546812265\n",
      "{1: (1.0, 6.915151572870145), 2: (1.0, 6.45897208116136), 3: (1.0, 7.63492546812265)}\n",
      "best k 1.0 8.323678483317178\n",
      "best k 0.594935817500001 7.391706636840973\n",
      "best k 0.24091034550000048 8.447313934773915\n",
      "{1: (1.0, 8.323678483317178), 2: (0.594935817500001, 7.391706636840973), 3: (0.24091034550000048, 8.447313934773915)}\n",
      "best k 1.0 6.889135580897849\n",
      "best k 1.0 6.614003785214069\n",
      "best k 1.0 7.7960055050801\n",
      "{1: (1.0, 6.889135580897849), 2: (1.0, 6.614003785214069), 3: (1.0, 7.7960055050801)}\n",
      "best k 1.0 8.45266107682375\n",
      "best k 1.0 7.868848520239034\n",
      "best k 0.4999741535000008 9.127115326077684\n",
      "{1: (1.0, 8.45266107682375), 2: (1.0, 7.868848520239034), 3: (0.4999741535000008, 9.127115326077684)}\n",
      "best k 1.0 6.781629028419868\n",
      "best k 1.0 6.4375906207184705\n",
      "best k 1.0 7.58012513566007\n",
      "{1: (1.0, 6.781629028419868), 2: (1.0, 6.4375906207184705), 3: (1.0, 7.58012513566007)}\n",
      "best k 1.0 8.342661566280292\n",
      "best k 1.0 7.721698806307221\n",
      "best k 0.4835246175000008 8.926273350148413\n",
      "{1: (1.0, 8.342661566280292), 2: (1.0, 7.721698806307221), 3: (0.4835246175000008, 8.926273350148413)}\n",
      "best k 1.0 6.863929755544079\n",
      "best k 1.0 6.5853462340668365\n",
      "best k 1.0 7.826040604864271\n",
      "{1: (1.0, 6.863929755544079), 2: (1.0, 6.5853462340668365), 3: (1.0, 7.826040604864271)}\n",
      "best k 1.0 8.473194354249925\n",
      "best k 0.7005798495000011 7.738748120976691\n",
      "best k 0.2929459295000006 8.965211907204134\n",
      "{1: (1.0, 8.473194354249925), 2: (0.7005798495000011, 7.738748120976691), 3: (0.2929459295000006, 8.965211907204134)}\n",
      "best k 1.0 7.034163489913868\n",
      "best k 1.0 6.743863802720121\n",
      "best k 1.0 7.966797294716599\n",
      "{1: (1.0, 7.034163489913868), 2: (1.0, 6.743863802720121), 3: (1.0, 7.966797294716599)}\n",
      "best k 1.0 8.573503095488748\n",
      "best k 0.6180044895000011 7.725283630319996\n",
      "best k 0.23907533750000048 8.863147170161483\n",
      "{1: (1.0, 8.573503095488748), 2: (0.6180044895000011, 7.725283630319996), 3: (0.23907533750000048, 8.863147170161483)}\n",
      "best k 1.0 6.838220966516921\n",
      "best k 1.0 6.483634660244188\n",
      "best k 1.0 7.63524187378808\n",
      "{1: (1.0, 6.838220966516921), 2: (1.0, 6.483634660244188), 3: (1.0, 7.63524187378808)}\n",
      "best k 1.0 8.25390041578749\n",
      "best k 0.7117209695000011 7.408056368777818\n",
      "best k 0.2989097055000006 8.555403963980364\n",
      "{1: (1.0, 8.25390041578749), 2: (0.7117209695000011, 7.408056368777818), 3: (0.2989097055000006, 8.555403963980364)}\n",
      "{'n_self': 2, 'k_self': 1.0, 'n_other': 2, 'k_other': 0.7402719885000008}\n"
     ]
    }
   ],
   "source": [
    "# Firstly, in a self-supervised way, optimize the global + local language models in terms of n and smoothing param to get the lowest cross-entropy\n",
    "# We only go for the global best average across all pairs and pieces for the rest of the experiments\n",
    "# NB we just use this as a comparative method - this is not necessarily the best params for the best result\n",
    "print(TEST, HELDOUT)\n",
    "opt_params = {}\n",
    "n_range = [1,2,3]\n",
    "k_val_range = [0.00000001, 1]\n",
    "\n",
    "l = list(filter(lambda x:x!=TEST, references_per_pair.keys()))\n",
    "\n",
    "all_n_self = []\n",
    "all_k_self = []\n",
    "all_n_other = []\n",
    "all_k_other = []\n",
    "\n",
    "for heldout in l:\n",
    "    training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [TEST, heldout], references_per_pair.keys())}\n",
    "    \n",
    "    n_self, k_self, lm_opt_results_self = get_params_for_lowest_entropy_on_fold_test(training_folds, good_pieces,\n",
    "                                                                  n_range, k_val_range, prob_model='self')\n",
    "\n",
    "\n",
    "    all_n_self.append(n_self)\n",
    "    all_k_self.append(k_self)\n",
    "\n",
    "    n_other, k_other, lm_opt_results_other = get_params_for_lowest_entropy_on_fold_test(training_folds, good_pieces,\n",
    "                                                                  n_range, k_val_range, prob_model='other')\n",
    "    \n",
    "    all_n_other.append(n_other)\n",
    "    all_k_other.append(k_other)\n",
    "    \n",
    "    \n",
    "opt_params[\"n_self\"] = round(np.mean(all_n_self))  # NB python rounds down 0.5 but shouldn't matter\n",
    "opt_params[\"k_self\"] = np.mean(all_k_self)\n",
    "\n",
    "opt_params[\"n_other\"] = round(np.mean(all_n_other)) # NB python rounds down 0.5 but shouldn't matter\n",
    "opt_params[\"k_other\"] = np.mean(all_k_other)\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_on_test_folds_joint_model(lambda_lm1, pair_piece_lms_self_orig, pair_piece_lms_other_orig,\n",
    "                                                 cutoff_word_global_lm, folds):\n",
    "    #print(\"lambda\", lambda_lm1, \"cutoff\", cutoff_word_global_lm)\n",
    "    pair_piece_lms_self = copy.deepcopy(pair_piece_lms_self_orig)\n",
    "    pair_piece_lms_other = copy.deepcopy(pair_piece_lms_other_orig)\n",
    "            \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "            # assume we know the correct ref piece\n",
    "            piece = ref_info[4]\n",
    "            \n",
    "            # get the weighted prob contribution from the self model\n",
    "            self_prob = 1\n",
    "            lm_self = pair_piece_lms_self[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm_self.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            \n",
    "            for j in range(lm_self.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm_self.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                self_prob = self_prob * lm_self.prob_lidstone(ngram, lm_self.order)\n",
    "                \n",
    "            # get the weighted prob contribution from the other model\n",
    "            other_prob = 1\n",
    "            lm_other = pair_piece_lms_other[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm_other.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            for j in range(lm_other.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm_other.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                other_prob = other_prob * lm_other.prob_lidstone(ngram, lm_other.order)\n",
    "            \n",
    "            # combine the probs with correct lambda weights\n",
    "            \n",
    "            # calculate the correct lambda weights based on the cut-off word\n",
    "            \n",
    "            lambda_global = 1- lambda_lm1\n",
    "            factor = 0 if cutoff_word_global_lm <=1 else (1-lambda_global)/(cutoff_word_global_lm-1)\n",
    "            num_refs_f = pair_piece_lms_self[pair_num][piece].num_training_sents  # + 1 # to include the current one?\n",
    "            current_lambda_lm1 = 1 - max([lambda_global + (factor * ((((cutoff_word_global_lm-1)-num_refs_f)))), lambda_global])\n",
    "\n",
    "            \n",
    "            prob = (current_lambda_lm1 * self_prob) + ((1-current_lambda_lm1) * other_prob)\n",
    "            #final_ref_info = tuple(list(ref_info) + [prob, len(ref_info[0].split()), num_refs_f])\n",
    "            #piece_references_and_probabilities_per_pair[pair_num].append(final_ref_info)\n",
    "            \n",
    "            s += log(prob)\n",
    "            count +=1\n",
    "            \n",
    "            # update self language model, which starts empty for a piece\n",
    "            # then will train as it encounters each piece\n",
    "            #print(pair_piece_lms_self[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "            pair_piece_lms_self[pair_num][piece].train([ref_info[0].split()])\n",
    "\n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "    #print(lambda_lm1, cross_entropies, np.mean(cross_entropies))\n",
    "    return np.mean(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB MAY NOT USE THIS METHOD FOR OPTIMIZING THESE TWO\n",
    "def get_best_params_cutoff_lambda_xval(n_self, n_other, k_self, k_other, pieces, folds,\n",
    "                                       cutoff_range = [1,10],\n",
    "                                       lambda_range=[0,1]):\n",
    "    \"\"\"An x-fold process run multiple times to find the best params of lambda (weighting of local language model)\n",
    "    and cut-off word at which only the local model applies (with a gradual weighting)\n",
    "    \"\"\"\n",
    "    results = {}  # store results for each model\n",
    "    \n",
    "    for test_cutoff in range(cutoff_range[0], cutoff_range[1]+1):\n",
    "        pair_piece_lms_self = {}  # will produce all lms for each speaker for each piece with order test_cutoff\n",
    "        pair_piece_lms_other = {}  # will produce all lms for each speaker for each piece with order test_cutoff\n",
    "        for test_pair in folds.keys():\n",
    "            # test pair is the list of refs to get the entropy results on in both cases\n",
    "            # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "            # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "                  #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "            pair_piece_lms_other[test_pair] = {}  # key piece, value language model for that piece\n",
    "            pair_piece_lms_self[test_pair] = {}  # key piece, value language model for that piece\n",
    "            for piece in pieces:\n",
    "                for prob_model in ['other', 'self']:\n",
    "                    if prob_model == 'other':\n",
    "                        # train the model (smoothing can be changed as it doesn't affect training)\n",
    "\n",
    "                        training_refs = []\n",
    "\n",
    "                        for other_pair_num in folds.keys():\n",
    "                            if other_pair_num == test_pair:\n",
    "                                continue\n",
    "                            training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])])  \n",
    "                        training_fold = [r[0] for r in training_refs]\n",
    "                        #print(\"length of training fold for lm2\", piece, test_pair, len(training_fold))\n",
    "                        if len(training_fold) <1:\n",
    "                            print(\"not enough data\", piece, pair_num)\n",
    "                            continue\n",
    "                        pair_piece_lms_other[test_pair][piece] = mini_language_model(n_other, smoothing_k=k_other)    \n",
    "                        pair_piece_lms_other[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "\n",
    "                    elif prob_model == 'self':\n",
    "                        # just initialize lm for piece, no need to train\n",
    "                        pair_piece_lms_self[test_pair][piece] = mini_language_model(n_self, smoothing_k=k_self)    \n",
    "\n",
    "        # run nelder mead to get best lambda\n",
    "        best = optimize.minimize(\n",
    "                mean_cross_entropy_on_test_folds_joint_model,\n",
    "                lambda_range[0],    # first argument to be optimized (lambda)\n",
    "                args=(pair_piece_lms_self, pair_piece_lms_other, test_cutoff, folds),  # other arguments to the function\n",
    "                method='Nelder-Mead', # use nelder mead to find minima\n",
    "                tol=0.0001, # to this degree of error\n",
    "                options={'disp': False},\n",
    "                bounds=[lambda_range]\n",
    "        )\n",
    "        best_lambda, best_ce = best.x[0], best.fun\n",
    "        #print(\"best lambda\", best_lambda, best_ce)\n",
    "        results[test_cutoff] = (best_lambda, best_ce)\n",
    "    \n",
    "    print(results)\n",
    "    cutoff = min(results.items(), key=lambda x:x[1][1])[0]\n",
    "    lambda_lm1 = results[cutoff][0]\n",
    "               \n",
    "    return cutoff, lambda_lm1, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_piece_lms_from_folds(n, k, pieces, folds, target_folds=None, train=False, heldout_extra_fold=None):\n",
    "    \"\"\"Returns a piece based language model objects of n=n, k=k, using the training folds\"\"\"\n",
    "    #print('num folds', len(folds.keys()))\n",
    "    pair_piece_lms_other = {}\n",
    "    for test_pair in folds.keys():\n",
    "        # test pair is the list of refs to get the entropy results on in both cases\n",
    "        # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "        # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "              #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "        if not test_pair in target_folds:\n",
    "            continue\n",
    "        pair_piece_lms_other[test_pair] = {}  # key piece, value language model for that piece\n",
    "        count = 0\n",
    "        for piece in pieces:\n",
    "            # train the model (smoothing can be changed as it doesn't affect training)\n",
    "            pair_piece_lms_other[test_pair][piece] = mini_language_model(n, smoothing_k=k)  \n",
    "            if train:\n",
    "                training_refs = []\n",
    "                non_training_pairs = [test_pair]\n",
    "                if not heldout_extra_fold is None:\n",
    "                    # hold out either the heldout extra fold or it's the test pair, another one\n",
    "                    random.seed(0)   #count)\n",
    "                    count+=1\n",
    "                    l = list(filter(lambda x:x!=test_pair,(folds.keys())))\n",
    "                    shuffle(l)  # NB this may not have been shuffled before\n",
    "                    heldout = l[0] if test_pair == heldout_extra_fold else heldout_extra_fold\n",
    "                    #heldout = l[0]\n",
    "                    non_training_pairs.append(heldout)\n",
    "                num_training_pairs = 0\n",
    "                for other_pair_num in folds.keys():\n",
    "                    if other_pair_num in non_training_pairs:\n",
    "                        continue\n",
    "                    training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])]) \n",
    "                    num_training_pairs+=1\n",
    "                #print(\"num training pairs for lm\", piece, test_pair, num_training_pairs)\n",
    "                training_fold = [r[0] for r in training_refs]\n",
    "                #print(\"length of training fold for lm\", piece, test_pair, len(training_fold))\n",
    "                if len(training_fold) <1:\n",
    "                    print(\"not enough data\", piece, test_pair)\n",
    "                    continue\n",
    "                pair_piece_lms_other[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "    return pair_piece_lms_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifier_data_from_lms_and_params(pair_piece_lms_self_orig, pair_piece_lms_other_orig, pieces, lambda_lm1, cutoff_word_global_lm, folds, lexical=True):\n",
    "    # Creates the appropriate language models for, and trains and tests classifiers for a given test pair\n",
    "    # create the speaker-wise fold language models (using all other pairs except key)\n",
    "   \n",
    "    # Record all scores (probabilities) for each piece according to the language models in the folds \n",
    "    # (Which uses both other language models from observing other pairs and the local updating language model/pact model)\n",
    "    piece_references_and_probabilities_per_pair = {}\n",
    "\n",
    "    pair_piece_lms_self = copy.deepcopy(pair_piece_lms_self_orig)\n",
    "    pair_piece_lms_other = copy.deepcopy(pair_piece_lms_other_orig)\n",
    "\n",
    "  \n",
    "    \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        piece_references_and_probabilities_per_pair[pair_num] = []\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "   \n",
    "            target_piece = ref_info[4]\n",
    "            \n",
    "            piece_prob_dict = {} # gets the raw probs assigned to ref by all models\n",
    "            \n",
    "            for piece in pieces:\n",
    "                # get the weighted prob contribution from the self model\n",
    "                self_prob = 1\n",
    "                lm_self = pair_piece_lms_self[pair_num][piece]\n",
    "                ref = [\"<s>\"] * (lm_self.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "\n",
    "                for j in range(lm_self.order-1, len(ref)):\n",
    "                    target = ref[j]\n",
    "                    context = ref[j-(lm_self.order-1):j]\n",
    "                    ngram = context + [target]\n",
    "                    self_prob = self_prob * lm_self.prob_lidstone(ngram, lm_self.order)\n",
    "\n",
    "                # get the weighted prob contribution from the other model\n",
    "                other_prob = 1\n",
    "                lm_other = pair_piece_lms_other[pair_num][piece]\n",
    "                ref = [\"<s>\"] * (lm_other.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "                for j in range(lm_other.order-1, len(ref)):\n",
    "                    target = ref[j]\n",
    "                    context = ref[j-(lm_other.order-1):j]\n",
    "                    ngram = context + [target]\n",
    "                    other_prob = other_prob * lm_other.prob_lidstone(ngram, lm_other.order)\n",
    "\n",
    "                # combine the probs with correct lambda weights\n",
    "\n",
    "                # calculate the correct lambda weights based on the cut-off word\n",
    "                lambda_global = 1- lambda_lm1\n",
    "                factor = 0 if cutoff_word_global_lm <=1 else (1-lambda_global)/(cutoff_word_global_lm-1)\n",
    "                num_refs_f = pair_piece_lms_self[pair_num][piece].num_training_sents  # + 1 # to include the current one?\n",
    "                current_lambda_lm1 = 1 - max([lambda_global + (factor * ((((cutoff_word_global_lm-1)-num_refs_f)))), lambda_global])\n",
    "\n",
    "\n",
    "                prob = (current_lambda_lm1 * self_prob) + ((1-current_lambda_lm1) * other_prob)\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                piece_prob_dict[piece] = (-log(prob)/len(ref_info[0].split()), num_refs_f)\n",
    "\n",
    "                # only get the entropy for target piece\n",
    "                if piece == target_piece:\n",
    "                    s += log(prob)\n",
    "                    count +=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            # update self language model, which starts empty for a piece\n",
    "            # then will train as it encounters each piece\n",
    "            #print(pair_piece_lms_self[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "            pair_piece_lms_self[pair_num][target_piece].train([ref_info[0].split()])\n",
    "            \n",
    "        \n",
    "            \n",
    "            # could add moving averages for how this piece has been judged probability-wise by each piece model\n",
    "            # built so far in this interaction by different models\n",
    "            # for this instance, we don't know what the correct referent is, so we only have the probs from the models\n",
    "            #  will be num shapes * num shapes\n",
    "            # assumes the latest prob dict for this piece is a new instance for all shapes:\n",
    "            init_probs_from_current = {k: [v[0]] for k, v in piece_prob_dict.items()}\n",
    "            probs_assigned_to_pieces_so_far = {k:copy.deepcopy(init_probs_from_current) for k, v in piece_prob_dict.items()}\n",
    "            # for previous instances, we do know what the correct referents were\n",
    "            # so we can check what the effect of combining the current probs to those from previous positive examples\n",
    "            # NB and negative ones too?\n",
    "            # scroll forward from start of interaction up to but not including current ref\n",
    "            for j in range(0, i):\n",
    "                back_ref_info = piece_references_and_probabilities_per_pair[pair_num][j]\n",
    "                back_ref_prob_dict = back_ref_info[5]\n",
    "                back_ref_target_piece = back_ref_info[4]\n",
    "                \n",
    "                for k, v in back_ref_prob_dict.items():\n",
    "                    if probs_assigned_to_pieces_so_far[back_ref_target_piece].get(k) is None:\n",
    "                        probs_assigned_to_pieces_so_far[back_ref_target_piece][k] = []\n",
    "                    probs_assigned_to_pieces_so_far[back_ref_target_piece][k].append(v[0])\n",
    "            \n",
    "            # will be num shapes * num shapes\n",
    "            moving_average_prob_dicts_all_lms = {k:{} for k, v in piece_prob_dict.items()}\n",
    "            for target_p in probs_assigned_to_pieces_so_far.keys():\n",
    "                for a_p in probs_assigned_to_pieces_so_far[target_p].keys():\n",
    "                    moving_average_prob_dicts_all_lms[target_p][a_p] = np.mean(probs_assigned_to_pieces_so_far[target_p][a_p])\n",
    "                    \n",
    "            \n",
    "            \n",
    "            final_ref_info = tuple(list(ref_info) + [piece_prob_dict, moving_average_prob_dicts_all_lms, len(ref_info[0].split())])\n",
    "            piece_references_and_probabilities_per_pair[pair_num].append(final_ref_info)\n",
    "            \n",
    "\n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "  \n",
    "    return piece_references_and_probabilities_per_pair\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': -0.7071067811865476, 'X': -0.7071067811865476, 'Y': 1.4142135623730951}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_zscore_dict_for_prob_dict(prob_dict):\n",
    "    probs  = [item[1] for item in sorted(prob_dict.items(), key=lambda x:x[0])] # alpha\n",
    "    zscores = list(stats.zscore(np.array(probs)))\n",
    "    def convert_nan(v):\n",
    "        if np.isnan(v):\n",
    "            return - 100\n",
    "        return v\n",
    "    return {k: convert_nan(v) for k, v in zip(sorted(prob_dict.keys()), zscores)}\n",
    "\n",
    "get_zscore_dict_for_prob_dict({\"K\": 0.33, \"Y\": 0.35, \"X\": 0.33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifier_data_from_raw(folds, lexical=True, lm_features=True, moving_average=False):\n",
    "    \"\"\"Gets derived data from raw probs from language models\"\"\"\n",
    "    final_data = []\n",
    "    current_pair_num = None\n",
    "    for current_pair_num in folds.keys():\n",
    "        raw_data = folds[current_pair_num]\n",
    "        for ref_info in raw_data:\n",
    "            #('das orange l', 'r1_B', 5887, 204.521, 'L', prob_dict, ref_length)\n",
    "\n",
    "            text, speaker, utt_id, end_time, target_piece, prob_dict, piece_lm_moving_average, ref_length = ref_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #if current_pair_piece_probs.get(piece) is None:\n",
    "            #    current_pair_piece_probs[piece] = []\n",
    "\n",
    "\n",
    "            feature_vector = {}\n",
    "\n",
    "            if lexical:\n",
    "                clean_utt = text.lower().split()\n",
    "                word_dem = len(clean_utt)\n",
    "\n",
    "                local_word_count = Counter()\n",
    "\n",
    "                for word in clean_utt:\n",
    "                    #word_counter[word] += (1/word_dem)   # for global\n",
    "                    #word_piece_counter[piece+\"__\"+word] += (1/word_dem)  #for global\n",
    "                    local_word_count[word] += 1\n",
    "\n",
    "                for k, v in local_word_count.items():\n",
    "                    feature_vector[k] = local_word_count[k]/word_dem\n",
    "\n",
    "\n",
    "\n",
    "            if lm_features:\n",
    "                length_weighted_prob_dict = {k:v[0] for k,v in prob_dict.items()}\n",
    "\n",
    "                #for k, v in length_weighted_prob_dict.items():\n",
    "                #    feature_vector['local_prob_' + k] = v\n",
    "\n",
    "                zscore_dict = get_zscore_dict_for_prob_dict(length_weighted_prob_dict)\n",
    "\n",
    "                for k, v in zscore_dict.items():\n",
    "                    feature_vector['local_zscore_prob_' + k] = v \n",
    "\n",
    "                if moving_average:\n",
    "                    #pass\n",
    "                    # to get a smoother measure of the pacts so far (after the first one), \n",
    "                    # we calculate what the moving average lm score is for each piece lm\n",
    "                    # with which previous pieces and current probs for this pair\n",
    "                    # give for the current piece (not knowing what it is and assuming it's of each type)\n",
    "                    # i.e. assuming it's of a shape (X), what's the moving average of all the piece lm's on this type of piece?\n",
    "                    # you'd need num pieces * num pieces number of moving probs for it to be fair, not just for the target\n",
    "\n",
    "                    for _target in piece_lm_moving_average.keys():\n",
    "                        #if not _target == target_piece: # NB this is cheating!\n",
    "                        #    continue\n",
    "                        zscore_dict_average = get_zscore_dict_for_prob_dict(piece_lm_moving_average[_target])\n",
    "                        for k, v in zscore_dict_average.items():  \n",
    "                            if not k == _target:\n",
    "                                continue\n",
    "                            #feature_vector['local_prob_moving_zcore' + _target + \":\" + k] = v\n",
    "                            feature_vector['local_prob_moving_prob' + _target + \":\" + k] =  piece_lm_moving_average[_target][k]\n",
    "                            feature_vector['local_prob_moving_zcore' + _target + \":\" + k] =  v\n",
    "\n",
    "                #zscore_dict_average = get_zscore_dict_for_prob_dict(piece_lm_moving_average)\n",
    "\n",
    "                #for k, v in zscore_dict_average.items():\n",
    "                #    feature_vector['local_zscore_prob_moving_' + k] = v \n",
    "\n",
    "\n",
    "            #rank_dict = get_rank_dict_for_prob_dict(piece_lm_prob_local)\n",
    "\n",
    "            #for k, v in rank_dict.items():\n",
    "            #    feature_vector['local_prob_rank' + k] = v \n",
    "            final_data.append((feature_vector, target_piece))\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def train_classifier(train_data):\n",
    "    #print(\"Training Classifier...\")\n",
    "    return SklearnClassifier(LinearSVC(loss='squared_hinge')).train(train_data)\n",
    "    #return SklearnClassifier(LogisticRegression()).train(train_data)\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Alternative method for finding best lambda local and cut-off - may not work as LMs of different size, so will always favour\n",
    "    # high lambda for the local model which will always give higher probabilities\n",
    "    # experiment with and optimize the best lambda weight on the local model and best cut-off point\n",
    "    # in process get the cross validation results\n",
    "    print(opt_params)\n",
    "    n_self = opt_params[\"n_self\"]\n",
    "    n_other = opt_params[\"n_other\"]\n",
    "    k_self = opt_params[\"k_self\"]\n",
    "    k_other = opt_params[\"k_other\"]\n",
    "    cutoff_word_global_lm, lambda_local, x_val_results = get_best_params_cutoff_lambda_xval(n_self, n_other,\n",
    "                                                                                            k_self, k_other,\n",
    "                                                                                            good_pieces, training_folds,\n",
    "                                                                                            cutoff_range=[1,5],\n",
    "                                                                                            lambda_range=[0,1]) \n",
    "    opt_params[\"lambda_local\"] = lambda_local\n",
    "    opt_params[\"cutoff_word_global_lm\"] = cutoff_word_global_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally get the test results with best language models (will have done this 7 times above in each xval)\n",
    "\n",
    "def get_fscore_and_preds(references_per_pair, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm, \n",
    "               lexical=True, \n",
    "               lm_features=True,\n",
    "               moving_average=False,\n",
    "               exhaustive_retrain=False,\n",
    "               no_train=False,\n",
    "               heldout_extra_fold=None):\n",
    "    \n",
    "    # First generate training data by getting the other/global probs from the other pair folds (k-fold style)\n",
    "    # This is to simulate what it's like \n",
    "    # So the LM sizes are num_folds - 1\n",
    "    # One problem is in the case where there is one pair to learn from, this means there is no 'other' model\n",
    "    #, only one, so it has to use its own probabilities, which aren't representative\n",
    "    tic = time.time()\n",
    "    training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [test], references_per_pair.keys())}\n",
    "    global_training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [test], references_per_pair.keys())}\n",
    "    extra_fold_penalty = 1 if heldout_extra_fold is not None else 0\n",
    "    if (len(global_training_folds)-extra_fold_penalty) == 1:\n",
    "        # this won't work at all as it needs to learn from other pairs, so just copy it to learn from its final model\n",
    "        # not ideal but at least gives values\n",
    "        global_training_folds['extra'] = global_training_folds[list(global_training_folds.keys())[0]]\n",
    "        \n",
    "    lms_global = get_new_piece_lms_from_folds(n_other, k_other, good_pieces,\n",
    "                                              global_training_folds, target_folds=global_training_folds.keys(),\n",
    "                                              train=True,\n",
    "                                              heldout_extra_fold=heldout_extra_fold)\n",
    "    #print(\"global model trained for training\")\n",
    "    lms_self = get_new_piece_lms_from_folds(n_self, k_self, good_pieces,\n",
    "                                            training_folds,\n",
    "                                            target_folds=training_folds.keys(),\n",
    "                                            train=False)\n",
    "    #print(\"local model trained for training\")\n",
    "    raw_train_data = generate_classifier_data_from_lms_and_params(lms_self,\n",
    "                                                                  lms_global,\n",
    "                                                                  good_pieces,\n",
    "                                                                  lambda_local,\n",
    "                                                                  cutoff_word_global_lm,\n",
    "                                                                  training_folds)\n",
    "\n",
    "    print(time.time()-tic, \"all lm features extracted\")\n",
    "    train_data = generate_classifier_data_from_raw(raw_train_data, lexical=lexical, lm_features=lm_features, moving_average=moving_average)\n",
    "    #print(\"length training data\", len(train_data))\n",
    "    #print(train_data[0])\n",
    "\n",
    "\n",
    "    # now generate the test data using LMs trained on all the other folds except heldout, to ensure the same sized LMs\n",
    "    \n",
    "    all_global_training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [heldout], references_per_pair.keys())}\n",
    "    extra_fold_penalty = 1 if heldout_extra_fold is not None else 0\n",
    "    if (len(all_global_training_folds)-extra_fold_penalty) == 1:\n",
    "        # this won't work at all as it needs to learn from other pairs, so just copy it to learn from its final model\n",
    "        # not ideal but at least gives values\n",
    "        all_global_training_folds['extra'] = global_training_folds['extra']\n",
    "    \n",
    "    test_folds ={k:references_per_pair[k] for k in filter(lambda x:x==test, references_per_pair.keys())}\n",
    "    \n",
    "    # if using all other folds, needs to take one other fold out for the LM training to ensure same size as training\n",
    "    lms_global = get_new_piece_lms_from_folds(n_other, k_other, good_pieces,\n",
    "                                                      all_global_training_folds, target_folds=[test], train=True,\n",
    "                                                      heldout_extra_fold=heldout_extra_fold)\n",
    "    #print(\"global model trained for testing\")\n",
    "    lms_self = get_new_piece_lms_from_folds(n_self, k_self, good_pieces,\n",
    "                                                      test_folds, target_folds=[test], train=False)\n",
    "\n",
    "    #print(\"local model trained for testing\")\n",
    "    raw_test_data = generate_classifier_data_from_lms_and_params(lms_self,\n",
    "                                                                 lms_global,\n",
    "                                                                 good_pieces,\n",
    "                                                                 lambda_local,\n",
    "                                                                 cutoff_word_global_lm,\n",
    "                                                                 test_folds)\n",
    "\n",
    "\n",
    "    test_data = generate_classifier_data_from_raw(raw_test_data, lexical=lexical, lm_features=lm_features, moving_average=moving_average)\n",
    "    #print(test_data[0])\n",
    "\n",
    "    if no_train:\n",
    "        train_data = []  # just start with no data\n",
    "    \n",
    "    prf = None\n",
    "    if exhaustive_retrain:\n",
    "        y_true = [x[1] for x in test_data]\n",
    "        y_pred = []\n",
    "        # do exhaustive incremental testing on current exmaple then retraining given last example\n",
    "        for t in range(0, len(test_data)):\n",
    "            if len(set([x[1] for x in train_data])) < 2:  # only one class seen so far, can't train yet\n",
    "                random.seed(t)  # guess a piece using index t as random seed\n",
    "                shuffle(good_pieces)\n",
    "                guess = good_pieces[0]\n",
    "                y_pred.extend(guess)\n",
    "            else:\n",
    "                # predict and train model with current data\n",
    "                cl = train_classifier(train_data)\n",
    "                y_pred.extend(predict_labels([test_data[t]], cl))\n",
    "                \n",
    "            feats, label = test_data[t]\n",
    "            feats = {k:v for k, v in feats.items()}\n",
    "            train_data.append((feats, label))  # add current example just attempted to training data\n",
    "            \n",
    "    else:\n",
    "        cl = train_classifier(train_data)\n",
    "        #print(train_data[0])\n",
    "        y_true = [x[1] for x in test_data]\n",
    "        y_pred = predict_labels(test_data, cl)\n",
    "    prf = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    #print(prf)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    return prf[2], y_pred, y_true, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do optimization to find best cut-off and lambda local param values (rather than just cross-entropy as not straightforward)\n",
    "# find best lambda local and cut-off word through cross-val testing, pick one with highest accuracy\n",
    "\n",
    "# NB got to ['n_other', 'n_self', 'cutoff_word_global_lm', 'k_other', 'k_self', 'lambda_local'] = [2, 2, 10, 0.45, 0.2, 0.1] 0.8469793454633859\n",
    "# # actual order:\n",
    "# ['n_other', 'n_self', 'k_other', 'k_self', 'cutoff_word_global_lm',  'lambda_local'] = [2, 2, 0.45, 0.2, 10,  0.1] 0.8469793454633859\n",
    "\n",
    "\n",
    "OPTIMIZING = False\n",
    "if OPTIMIZING:\n",
    "    n_other = opt_params['n_other']\n",
    "    n_self = opt_params['n_self']\n",
    "    k_other = opt_params['k_other']\n",
    "    k_self = opt_params['k_self']\n",
    "\n",
    "    print(TEST, HELDOUT)\n",
    "    cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x !=TEST, references_per_pair.keys())}\n",
    "    results = {}\n",
    "    results_file = open(\"2023-06-14T18:49:05.057567.csv\", \"a\")\n",
    "    #results_file = open(datetime.datetime.now().isoformat() + \".csv\", \"w\")\n",
    "    #results_file.write(\",\".join(['n_other', 'n_self', 'cutoff_word_global_lm', 'k_other', 'k_self', 'lambda_local', 'fscore']) + \"\\n\")\n",
    "    \n",
    "    best_f = 0\n",
    "    best_params = []\n",
    "    for n_other in range(1,3): #2\n",
    "        for n_self in range(1,3): #4\n",
    "            for k_other_raw in range(5,105,5): #80\n",
    "                k_other = k_other_raw/100\n",
    "                for k_self_raw in range(5,105,5): #1_600\n",
    "                    k_self = k_self_raw/100\n",
    "                    for cutoff_word_global_lm in range(1,11): #16_000\n",
    "                        # assume convex - not quite right but nearly \n",
    "                        prev = 0\n",
    "                        for lambda_local_raw in range(0,105,5): #320_000 params max, at least 32_000!\n",
    "                            lambda_local = lambda_local_raw/100\n",
    "                            if [n_other, n_self, k_other, k_self, cutoff_word_global_lm, lambda_local] <= [2, 2, 0.45, 0.2, 10,  0.1]:\n",
    "                                continue\n",
    "                            \n",
    "                            \n",
    "                            all_scores = []\n",
    "                            for test in cross_val_data.keys():\n",
    "                                random.seed(0)\n",
    "                                l = list(filter(lambda x:x!=test,(cross_val_data.keys())))\n",
    "                                shuffle(l)\n",
    "                                heldout = l[0] if test == HELDOUT else HELDOUT\n",
    "                                #heldout = HELDOUT\n",
    "                                #print(cross_val_data.keys(), test)\n",
    "                                fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm)\n",
    "                                all_scores.append(fscore)\n",
    "                            mean_f = np.mean(all_scores)\n",
    "                            test_params = [n_other, n_self, cutoff_word_global_lm, k_other, k_self, lambda_local]\n",
    "                            print(test_params, mean_f)\n",
    "                            if mean_f < prev: # assume convex - not quite right but nearly \n",
    "                                break\n",
    "                            prev = mean_f\n",
    "                            results[\",\".join([str(f) for f in test_params])] = mean_f\n",
    "                            if mean_f == best_f:\n",
    "                                best_params = [best_params, test_params]\n",
    "                            elif mean_f > best_f:\n",
    "                                best_f = mean_f\n",
    "                                best_params = test_params\n",
    "                                print(\"best\", best_f, best_params)\n",
    "                    #results_file = open(\"results\" + datetime.time)\n",
    "                    #for row in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "    \n",
    "    for params, result in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "        results_file.write(params + \",\" + str(result) + \"\\n\")\n",
    "    results_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_self': 1, 'k_self': 0.1, 'n_other': 1, 'k_other': 0.9, 'lambda_local': 0.1, 'cutoff_word_global_lm': 6}\n"
     ]
    }
   ],
   "source": [
    "# if already optimized look at results\n",
    "df = pd.read_csv(open(\"2023-06-14T18:49:05.057567.csv\"))\n",
    "opt_params['n_other'] = df.head(1).n_other[0]\n",
    "opt_params[\"n_self\"] = df.head(1).n_self[0]\n",
    "opt_params['k_other'] = df.head(1).k_other[0]\n",
    "opt_params[\"k_self\"] = df.head(1).k_self[0]\n",
    "opt_params['lambda_local'] = df.head(1).lambda_local[0]\n",
    "opt_params[\"cutoff_word_global_lm\"] = df.head(1).cutoff_word_global_lm[0]\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_other</th>\n",
       "      <th>n_self</th>\n",
       "      <th>cutoff_word_global_lm</th>\n",
       "      <th>k_other</th>\n",
       "      <th>k_self</th>\n",
       "      <th>lambda_local</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.865530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.865376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.865096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.865075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.865066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.865005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.864977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.864542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.864479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.864397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.864372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.864306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.864285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.864164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.864039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_other  n_self  cutoff_word_global_lm  k_other  k_self  lambda_local   \n",
       "0         1       1                      6     0.90    0.10          0.10  \\\n",
       "1         1       1                      6     0.95    0.10          0.10   \n",
       "2         1       1                      5     1.00    0.05          0.20   \n",
       "3         1       1                      5     0.95    0.05          0.20   \n",
       "4         1       1                      8     1.00    0.10          0.15   \n",
       "5         1       1                      6     1.00    0.10          0.10   \n",
       "6         1       1                      7     1.00    0.10          0.10   \n",
       "7         1       1                      7     1.00    0.05          0.15   \n",
       "8         1       1                      6     0.80    0.10          0.10   \n",
       "9         1       1                      6     0.85    0.10          0.10   \n",
       "10        1       1                      7     0.95    0.10          0.10   \n",
       "11        1       1                      2     0.85    0.05          0.05   \n",
       "12        1       1                      5     0.90    0.05          0.20   \n",
       "13        1       1                      8     0.95    0.10          0.10   \n",
       "14        1       1                      4     0.90    0.05          0.15   \n",
       "15        1       1                      4     0.95    0.05          0.15   \n",
       "16        1       1                      5     0.95    0.05          0.15   \n",
       "17        1       1                      5     1.00    0.05          0.15   \n",
       "18        1       1                      3     0.90    0.05          0.15   \n",
       "19        1       1                     10     1.00    0.05          0.20   \n",
       "\n",
       "      fscore  \n",
       "0   0.865530  \n",
       "1   0.865376  \n",
       "2   0.865096  \n",
       "3   0.865075  \n",
       "4   0.865066  \n",
       "5   0.865005  \n",
       "6   0.864977  \n",
       "7   0.864896  \n",
       "8   0.864542  \n",
       "9   0.864479  \n",
       "10  0.864397  \n",
       "11  0.864372  \n",
       "12  0.864306  \n",
       "13  0.864285  \n",
       "14  0.864282  \n",
       "15  0.864282  \n",
       "16  0.864224  \n",
       "17  0.864218  \n",
       "18  0.864164  \n",
       "19  0.864039  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: some plots on x-val data with optimized params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_piece_lm_values_for_pair_piece(data, piece, moving_average=False, fig_file=None):\n",
    "    piece_plots = {p:[] for p in data[0][5].keys()}\n",
    "    for ref_info in data:\n",
    "        text, speaker, utt_id, end_time, target_piece, prob_dict, piece_lm_moving_average, ref_length = ref_info\n",
    "        \n",
    "        if not target_piece == piece:  # only focus on target piece\n",
    "            continue\n",
    "        print(speaker.split(\"_\")[-1], \"&\", round(end_time,1),  \"&\", text, \"\\\\\\\\\")\n",
    "        if moving_average is True:\n",
    "            # how does each piece lm give a probability to THIS target piece over time\n",
    "            for lm_piece, inner_prob_dict in piece_lm_moving_average.items():\n",
    "                \n",
    "                prob_dict_inner = piece_lm_moving_average[lm_piece]\n",
    "                prob_dict_inner = get_zscore_dict_for_prob_dict(prob_dict_inner)\n",
    "                for inner_piece_lm, prob in prob_dict_inner.items():\n",
    "                    if not inner_piece_lm == piece:\n",
    "                        continue\n",
    "                    piece_plots[lm_piece].append(prob)\n",
    "        else:\n",
    "            # if just raw probs\n",
    "            for lm_piece, prob in prob_dict.items():\n",
    "                piece_plots[lm_piece].append(prob[0])\n",
    "    \n",
    "    #for k, v in piece_plots.items():\n",
    "    #    print(k,v)\n",
    "                \n",
    "    #‘b’\tblue\n",
    "    #‘g’\tgreen\n",
    "    #‘r’\tred  (255, 0, 0 )\n",
    "    #‘c’\tcyan\n",
    "    #‘m’\tmagenta\n",
    "    #‘y’\tyellow\n",
    "    #‘k’\tblack\n",
    "    #‘w’\twhite\n",
    "\n",
    "    #orange (255, 128, 0)\n",
    "    #grey (160, 160, 160)\n",
    "    #pink (255, 153, 204)\n",
    "    #brown (153, 76, 0)\n",
    "    #purple (102, 0, 204)\n",
    "\n",
    "    colour_map = {'F': (160/255, 160/255, 160/255), 'I': \"blue\" , 'L': (255/255, 128/255, 0/255),\n",
    "                  'N': (102/255, 0/255, 204/255),\n",
    "                  'P': (255/255, 153/255, 204/255), 'T': \"green\", 'U': \"yellow\", 'V': \"blue\",\n",
    "                  'W' :\"green\", 'X': \"red\", 'Y': (153/255, 76/255, 0/255), 'Z': \"blue\"}\n",
    "\n",
    "    for lm_piece in sorted(piece_plots.keys()):\n",
    "        probs = piece_plots[lm_piece]\n",
    "        target_text = \"\" if lm_piece == piece else \"\"\n",
    "        linestyle = \"solid\" if lm_piece == piece else \"dashed\"\n",
    "        marker = \"o\" if lm_piece == piece else \"none\"\n",
    "        plt.plot(probs, label=lm_piece + \" \" + target_text, color=colour_map[lm_piece],\n",
    "                linestyle=linestyle, marker=marker)\n",
    "\n",
    "\n",
    "    # plot lines\n",
    "    #plt.plot(moving_average([m[0] for m in method1]), label = \"active: mixed models with k=0.015 for previous\")\n",
    "    #plt.plot(moving_average([m[0] for m in method2]), label = \"interactive: retraining language model with incrementing data\")\n",
    "    #plt.plot(moving_average([m[0] for m in method3]), label = \"static baseline\")\n",
    "    #plt.plot(moving_average([m[0] for m in method4]), label = \"interactive: mixed models with k=0.015 for previous + recency\")\n",
    "    #plt.plot(moving_average([m[0] for m in method5]), label = \"active: retraining language model with incrementing data + recency\")\n",
    "    #plt.plot(moving_average([m[0] for m in method6]), label = \"interactive only + recency\")\n",
    "    #plt.plot(moving_average([m[0] for m in method7]), label = \"interactive only\")\n",
    "    #plt.legend(mode='expand') #bbox_to_anchor=(0.1, 1))\n",
    "    plt.xticks(fontsize=8, rotation=25)\n",
    "    plt.xticks(np.arange(len(piece_plots['X'])), np.arange(1, len(piece_plots['X'])+1))\n",
    "    \n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 0.85),\n",
    "          fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.ylabel('Negative log prob per word (Z-score)')  \n",
    "    plt.xlabel('Referring expression number to target piece in interaction')  \n",
    "\n",
    "    #plt.show()\n",
    "    if not fig_file is None:\n",
    "        plt.savefig(fig_file) \n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_self': 1, 'k_self': 0.1, 'n_other': 1, 'k_other': 0.9, 'lambda_local': 0.1, 'cutoff_word_global_lm': 6}\n",
      "[1, 1, 6, 0.9, 0.1, 0.1, True, True, False]\n",
      "F\n",
      "B & 248.2 & der graue stein \\\\\n",
      "B & 284.3 & den grauen \\\\\n",
      "B & 485.1 & der graue stein \\\\\n",
      "B & 1020.5 & der graue stein \\\\\n",
      "B & 1121.2 & der graue stein \\\\\n",
      "B & 1150.4 & dem grauen \\\\\n",
      "A & 1622.3 & den grauen \\\\\n",
      "A & 1649.5 & den grauen \\\\\n",
      "A & 1810.5 & den grauen \\\\\n",
      "A & 1940.9 & den grauen \\\\\n",
      "A & 1951.9 & der graue stein \\\\\n",
      "A & 1978.7 & den grauen \\\\\n",
      "A & 2301.9 & den grauen stein \\\\\n",
      "A & 2339.7 & von dem grauen \\\\\n",
      "A & 2419.0 & den grauen \\\\\n",
      "A & 2434.2 & dieses \\\\\n",
      "A & 2453.4 & dem grauen \\\\\n",
      "I\n",
      "A & 340.4 & der lange \\\\\n",
      "B & 378.3 & der blaue balken \\\\\n",
      "B & 846.8 & der blaue balken \\\\\n",
      "B & 862.6 & der blaue balken \\\\\n",
      "B & 883.7 & den blauen \\\\\n",
      "B & 899.6 & den blauen balken \\\\\n",
      "B & 908.8 & der balken \\\\\n",
      "B & 1117.8 & den winkel \\\\\n",
      "B & 1270.5 & der blaue balken \\\\\n",
      "A & 1441.8 & den blauen balken \\\\\n",
      "A & 1657.7 & den blauen balken \\\\\n",
      "A & 1704.7 & dem balken \\\\\n",
      "A & 1716.7 & balken \\\\\n",
      "A & 2034.6 & den blauen balken \\\\\n",
      "A & 2051.7 & den blauen balken \\\\\n",
      "A & 2233.3 & den blauen balken \\\\\n",
      "A & 2250.4 & dem blauen balken \\\\\n",
      "A & 2260.6 & den blauen balken \\\\\n",
      "A & 2271.0 & den blauen balken \\\\\n",
      "A & 2459.9 & den blauen balken \\\\\n",
      "L\n",
      "B & 148.2 & der orange stein \\\\\n",
      "A & 151.6 & das l \\\\\n",
      "B & 374.1 & der orange winkel \\\\\n",
      "B & 477.2 & der orange winkel \\\\\n",
      "B & 838.1 & der orange stein \\\\\n",
      "B & 1084.6 & der orange stein \\\\\n",
      "B & 1131.8 & den orangen \\\\\n",
      "A & 1455.3 & das l \\\\\n",
      "A & 1474.4 & das l \\\\\n",
      "A & 1662.5 & das l \\\\\n",
      "A & 1666.5 & das l \\\\\n",
      "A & 1709.7 & das orange l \\\\\n",
      "A & 1728.2 & dem orangen \\\\\n",
      "A & 1739.0 & das l \\\\\n",
      "A & 1758.5 & die orangen \\\\\n",
      "A & 2102.6 & das l \\\\\n",
      "A & 2216.2 & das l \\\\\n",
      "A & 2238.6 & dem orangen \\\\\n",
      "A & 2393.8 & das orange l \\\\\n",
      "A & 2412.5 & dem orangen \\\\\n",
      "A & 2478.0 & dem l \\\\\n",
      "A & 2505.9 & das l \\\\\n",
      "N\n",
      "B & 300.3 & der lila stein \\\\\n",
      "B & 325.1 & den lila stein \\\\\n",
      "B & 567.1 & der lila stein \\\\\n",
      "B & 916.5 & der lila stein \\\\\n",
      "B & 964.5 & den lila stein \\\\\n",
      "B & 1014.1 & dem lila stein \\\\\n",
      "B & 1099.4 & der lila stein \\\\\n",
      "B & 1104.3 & der pinke \\\\\n",
      "B & 1211.6 & der lila stein \\\\\n",
      "B & 1229.9 & dem lila \\\\\n",
      "B & 1265.3 & den lila stein \\\\\n",
      "A & 1470.9 & der lila \\\\\n",
      "A & 1482.4 & das lila \\\\\n",
      "A & 1574.4 & diesen lila stein \\\\\n",
      "A & 1604.9 & dem lila \\\\\n",
      "A & 1615.3 & dem lila \\\\\n",
      "A & 1777.0 & den lila \\\\\n",
      "A & 1800.8 & den lila \\\\\n",
      "A & 2080.8 & den lila stein \\\\\n",
      "A & 2082.6 & lila \\\\\n",
      "A & 2141.5 & dem lila \\\\\n",
      "A & 2223.7 & den lila stein \\\\\n",
      "A & 2404.1 & den lila \\\\\n",
      "A & 2437.9 & den lila \\\\\n",
      "P\n",
      "B & 350.2 & der pinke stein \\\\\n",
      "B & 354.1 & den blauen \\\\\n",
      "B & 636.1 & der pinke stein \\\\\n",
      "B & 655.2 & den pinken stein \\\\\n",
      "B & 757.9 & der pinke stein \\\\\n",
      "B & 1107.3 & der rosane \\\\\n",
      "A & 1465.4 & dem rosa \\\\\n",
      "A & 1597.0 & den rosa stein \\\\\n",
      "A & 1615.3 & dem rosa \\\\\n",
      "A & 1764.4 & der rosa \\\\\n",
      "A & 1792.0 & den rosa \\\\\n",
      "A & 1949.3 & den rosa \\\\\n",
      "A & 1978.7 & den rosa \\\\\n",
      "A & 2147.3 & den rosa \\\\\n",
      "A & 2202.8 & dem rosa \\\\\n",
      "A & 2238.6 & dem rosa \\\\\n",
      "A & 2474.8 & den rosa stein \\\\\n",
      "A & 2500.5 & dem rosa \\\\\n",
      "T\n",
      "B & 157.2 & das grüne t \\\\\n",
      "B & 287.8 & das grüne t \\\\\n",
      "B & 300.3 & das grüne t \\\\\n",
      "B & 304.1 & das t \\\\\n",
      "B & 610.2 & das grüne t \\\\\n",
      "B & 615.7 & den hellblauen \\\\\n",
      "B & 662.6 & dem grünen \\\\\n",
      "B & 834.6 & der grüne stein \\\\\n",
      "B & 838.1 & der grüne \\\\\n",
      "B & 964.5 & das t \\\\\n",
      "B & 1239.3 & das grüne t \\\\\n",
      "A & 1407.8 & ein t \\\\\n",
      "A & 1497.5 & das t \\\\\n",
      "A & 1545.9 & das t \\\\\n",
      "A & 1547.4 & das grüne \\\\\n",
      "A & 1565.3 & dem t \\\\\n",
      "A & 1589.4 & das t \\\\\n",
      "A & 1720.3 & das grüne t \\\\\n",
      "A & 1751.4 & das t \\\\\n",
      "A & 1773.6 & den grünen \\\\\n",
      "A & 1792.0 & den grünen \\\\\n",
      "A & 1926.9 & das t \\\\\n",
      "A & 1940.9 & das t \\\\\n",
      "A & 2018.3 & das t \\\\\n",
      "A & 2030.1 & t \\\\\n",
      "A & 2063.6 & den grünen \\\\\n",
      "A & 2244.3 & das t \\\\\n",
      "A & 2260.6 & das t \\\\\n",
      "A & 2283.4 & das t \\\\\n",
      "A & 2331.7 & dem t \\\\\n",
      "A & 2484.9 & den t-stein \\\\\n",
      "A & 2519.6 & das t \\\\\n",
      "A & 2523.0 & dem t \\\\\n",
      "A & 2565.5 & den grünen \\\\\n",
      "U\n",
      "B & 129.3 & gelben stein \\\\\n",
      "B & 235.5 & der gelbe stein \\\\\n",
      "B & 265.2 & den gelben \\\\\n",
      "B & 284.3 & den gelben \\\\\n",
      "B & 623.6 & der gelbe stein \\\\\n",
      "B & 629.3 & den gelben stein \\\\\n",
      "B & 825.7 & der gelbe stein \\\\\n",
      "B & 883.7 & dem gelben \\\\\n",
      "B & 1179.5 & der gelbe \\\\\n",
      "B & 1198.7 & dem gelben \\\\\n",
      "A & 1492.2 & den gelben \\\\\n",
      "A & 1539.9 & dem gelben \\\\\n",
      "A & 1870.9 & den gelben \\\\\n",
      "A & 2072.8 & den gelben stein \\\\\n",
      "A & 2093.8 & den gelben \\\\\n",
      "A & 2141.5 & dem gelben \\\\\n",
      "A & 2336.3 & den gelben stein \\\\\n",
      "A & 2561.9 & den gelben \\\\\n",
      "V\n",
      "B & 219.2 & der blaue winkel \\\\\n",
      "B & 496.2 & den winkel \\\\\n",
      "B & 541.9 & den winkel \\\\\n",
      "B & 650.2 & der blaue winkel \\\\\n",
      "B & 674.3 & den blauen winkel \\\\\n",
      "B & 700.3 & den blauen \\\\\n",
      "B & 899.6 & der blaue winkel \\\\\n",
      "B & 908.8 & den winkel \\\\\n",
      "B & 942.5 & blauen winkel \\\\\n",
      "B & 958.0 & dem winkel \\\\\n",
      "A & 1139.1 & der winkel \\\\\n",
      "B & 1229.9 & der blaue winkel \\\\\n",
      "B & 1247.7 & dem winkel \\\\\n",
      "A & 1417.9 & dem winkel \\\\\n",
      "A & 1421.0 & der winkel \\\\\n",
      "A & 1421.0 & der blaue \\\\\n",
      "A & 1666.5 & den winkel \\\\\n",
      "A & 1796.1 & den blauen winkel \\\\\n",
      "A & 2044.5 & den winkel \\\\\n",
      "A & 2045.8 & den blauen \\\\\n",
      "A & 2063.6 & den blauen \\\\\n",
      "A & 2346.5 & der winkel \\\\\n",
      "A & 2480.3 & winkel \\\\\n",
      "A & 2541.9 & den winkel \\\\\n",
      "A & 2541.9 & den in die untere ecke von der treppe \\\\\n",
      "A & 2553.1 & den winkel \\\\\n",
      "W\n",
      "B & 272.9 & der grüne stein \\\\\n",
      "B & 532.5 & der hellgrüne stein \\\\\n",
      "A & 533.5 & die treppe \\\\\n",
      "B & 535.3 & die treppe \\\\\n",
      "B & 545.6 & die treppe \\\\\n",
      "B & 774.6 & die grüne treppe \\\\\n",
      "B & 792.0 & die treppe \\\\\n",
      "A & 805.5 & die treppe \\\\\n",
      "B & 810.7 & die treppe \\\\\n",
      "B & 883.7 & den grünen \\\\\n",
      "B & 883.7 & die grüne treppe \\\\\n",
      "B & 1190.2 & die grüne treppe \\\\\n",
      "B & 1211.6 & die grüne treppe \\\\\n",
      "A & 1609.4 & die treppe \\\\\n",
      "A & 1822.2 & die treppe \\\\\n",
      "A & 1870.9 & den grünen \\\\\n",
      "A & 1918.8 & die treppe \\\\\n",
      "A & 1930.3 & die treppe \\\\\n",
      "A & 2266.0 & die treppe \\\\\n",
      "A & 2510.2 & die treppe \\\\\n",
      "X\n",
      "B & 141.7 & das rote kreuz \\\\\n",
      "B & 311.5 & das rote kreuz \\\\\n",
      "B & 325.1 & das kreuz \\\\\n",
      "B & 338.2 & das kreuz \\\\\n",
      "B & 629.3 & das kreuz \\\\\n",
      "B & 645.9 & das kreuz \\\\\n",
      "B & 768.7 & das kreuz \\\\\n",
      "B & 825.7 & das kreuz \\\\\n",
      "B & 862.6 & dem roten kreuz \\\\\n",
      "B & 862.6 & dem kreuz \\\\\n",
      "B & 1265.3 & das kreuz \\\\\n",
      "A & 1488.4 & das x \\\\\n",
      "A & 1490.0 & das kreuz \\\\\n",
      "A & 1539.9 & das x \\\\\n",
      "A & 1553.4 & das kreuz \\\\\n",
      "A & 1565.3 & dem x \\\\\n",
      "A & 1815.6 & das kreuz \\\\\n",
      "A & 1839.2 & dem x \\\\\n",
      "A & 1945.0 & das kreuz \\\\\n",
      "A & 2068.4 & das kreuz \\\\\n",
      "A & 2260.6 & das kreuz \\\\\n",
      "A & 2271.0 & kreuz \\\\\n",
      "A & 2283.4 & das kreuz \\\\\n",
      "A & 2294.2 & das x \\\\\n",
      "A & 2385.6 & das kreuz \\\\\n",
      "A & 2416.2 & dem kreuz \\\\\n",
      "Y\n",
      "B & 325.1 & der braune stein \\\\\n",
      "B & 338.2 & den braunen stein \\\\\n",
      "B & 374.1 & den braunen stein \\\\\n",
      "B & 674.3 & der braune stein \\\\\n",
      "B & 990.5 & dem braunen stein \\\\\n",
      "B & 1014.1 & dem brauen stein \\\\\n",
      "B & 1161.2 & der braune stein \\\\\n",
      "B & 1179.5 & den braunen stein \\\\\n",
      "B & 1185.7 & braunen \\\\\n",
      "B & 1211.6 & dem braunen stein \\\\\n",
      "A & 1479.7 & den braunen \\\\\n",
      "A & 1644.9 & den braunen \\\\\n",
      "A & 1857.4 & den braunen stein \\\\\n",
      "A & 2063.6 & den braunen stein \\\\\n",
      "A & 2280.3 & der braune stein \\\\\n",
      "A & 2294.2 & den braunen \\\\\n",
      "A & 2331.7 & dem braunen stein \\\\\n",
      "A & 2549.6 & den braunen stein \\\\\n",
      "A & 2565.5 & den braunen \\\\\n",
      "Z\n",
      "B & 338.2 & der blaue stein \\\\\n",
      "B & 597.6 & der hellblaue stein \\\\\n",
      "B & 599.6 & der andere \\\\\n",
      "B & 976.9 & der blaue stein \\\\\n",
      "B & 1136.5 & der hellblaue stein \\\\\n",
      "A & 1431.3 & der andere blaue \\\\\n",
      "A & 1431.3 & s \\\\\n",
      "A & 1431.3 & z \\\\\n",
      "A & 1450.0 & das z \\\\\n",
      "A & 1561.6 & der z-stein \\\\\n",
      "A & 1745.0 & dieses s \\\\\n",
      "A & 1745.0 & z-steinchen \\\\\n",
      "A & 1792.0 & blauen \\\\\n",
      "A & 2157.9 & das z \\\\\n",
      "A & 2288.9 & der z-stein \\\\\n",
      "A & 2443.6 & diesen z-stein \\\\\n",
      "A & 2443.6 & s-stein \\\\\n",
      "A & 2445.9 & ein z \\\\\n",
      "A & 2463.8 & dem z \\\\\n",
      "A & 2519.6 & dieses z \\\\\n",
      "A & 2523.0 & dem z \\\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x !=TEST, references_per_pair.keys())}\n",
    "\n",
    "print(opt_params)\n",
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, True, False]\n",
    "print(exp_params)\n",
    "\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "for test in ['r2']:\n",
    "    \n",
    "    heldout = EXTRA_HELDOUT if test == HELDOUT else HELDOUT\n",
    "    training_folds = {k:cross_val_data[k] for k in filter(lambda x:x not in [test], cross_val_data.keys())}\n",
    "    lms_global = get_new_piece_lms_from_folds(n_other, k_other, good_pieces,\n",
    "                                                      training_folds, target_folds=training_folds.keys(), train=True)\n",
    "    #print(\"global model trained for training\")\n",
    "    lms_self = get_new_piece_lms_from_folds(n_self, k_self, good_pieces,\n",
    "                                                      training_folds, target_folds=training_folds.keys(), train=False)\n",
    "    #print(\"local model trained for training\")\n",
    "    raw_train_data = generate_classifier_data_from_lms_and_params(lms_self,\n",
    "                                                                  lms_global,\n",
    "                                                                  good_pieces,\n",
    "                                                                  lambda_local,\n",
    "                                                                  cutoff_word_global_lm,\n",
    "                                                                  training_folds)\n",
    "    \n",
    "    for _piece in sorted(good_pieces):\n",
    "        print(_piece)\n",
    "        _pair = 'r3'\n",
    "        plot_piece_lm_values_for_pair_piece(raw_train_data[_pair], _piece, moving_average=True, fig_file=_pair + _piece + '.pdf')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 final runs/results (on xval then test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_self': 1, 'k_self': 0.1, 'n_other': 1, 'k_other': 0.9, 'lambda_local': 0.1, 'cutoff_word_global_lm': 6}\n",
      "1.420598030090332 all lm features extracted\n",
      "1.4563159942626953 all lm features extracted\n",
      "1.3999531269073486 all lm features extracted\n",
      "1.3520081043243408 all lm features extracted\n",
      "1.3699538707733154 all lm features extracted\n",
      "1.0786170959472656 all lm features extracted\n",
      "1.1116728782653809 all lm features extracted\n",
      "11.885232210159302\n",
      "[0.87, 0.8176470588235294, 0.8943396226415095, 0.8842105263157894, 0.8951048951048951, 0.8892045454545454, 0.7944785276073619] 0.8635693108496615\n"
     ]
    }
   ],
   "source": [
    "# 1) re-run best cross-val result\n",
    "# get extra heldout fold to ensure we're still using 5 folds for training\n",
    "cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x !=TEST, references_per_pair.keys())}\n",
    "\n",
    "print(opt_params)\n",
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "tic = time.time()\n",
    "for test in cross_val_data.keys():\n",
    "    heldout = EXTRA_HELDOUT if test == HELDOUT else HELDOUT  # only not the normal heldout when that's being tested on\n",
    "    #print(\"heldout\", heldout) \n",
    "    f, preds, y, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, True, False]\n",
    "xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.368596076965332 all lm features extracted\n",
      "1.350687026977539 all lm features extracted\n",
      "1.1674659252166748 all lm features extracted\n",
      "1.1486430168151855 all lm features extracted\n",
      "1.3383722305297852 all lm features extracted\n",
      "1.1068978309631348 all lm features extracted\n",
      "1.1096389293670654 all lm features extracted\n",
      "10.140897989273071\n",
      "[0.71, 0.8, 0.8943396226415095, 0.8912280701754386, 0.8531468531468531, 0.8579545454545454, 0.7699386503067485] 0.8252296773892994\n"
     ]
    }
   ],
   "source": [
    "# 2) Get the baseline of lexical info only, no updating/retraining or lms\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "tic = time.time()\n",
    "for test in cross_val_data.keys():\n",
    "    heldout = EXTRA_HELDOUT if test == HELDOUT else HELDOUT  # only not the normal heldout when that's being tested on\n",
    "    #print(\"heldout\", heldout) \n",
    "    f, preds, y, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                      lm_features = False)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, False, False]\n",
    "xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.568730115890503"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train_time = sum([1.3798320293426514, 1.296260118484497,\n",
    "1.1890807151794434, 1.2357509136199951,\n",
    "1.289025068283081,1.067748785018921,\n",
    "1.1144702434539795])\n",
    "toc-tic-lm_train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3765959739685059 all lm features extracted\n",
      "1.3137869834899902 all lm features extracted\n",
      "1.201883316040039 all lm features extracted\n",
      "1.2006549835205078 all lm features extracted\n",
      "1.3009788990020752 all lm features extracted\n",
      "1.092986822128296 all lm features extracted\n",
      "1.115159034729004 all lm features extracted\n",
      "31.92237901687622\n",
      "[0.86, 0.8352941176470589, 0.909433962264151, 0.8982456140350877, 0.8741258741258742, 0.8920454545454546, 0.7822085889570553] 0.8644790873678118\n"
     ]
    }
   ],
   "source": [
    "# 3) see what difference exahustive retraining makes, firstly lexical only, no lm features\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "tic = time.time()\n",
    "for test in cross_val_data.keys():\n",
    "    heldout = EXTRA_HELDOUT if test == HELDOUT else HELDOUT  # only not the normal heldout when that's being tested on\n",
    "    #print(\"heldout\", heldout) \n",
    "    f, preds, y, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                      lm_features=False, exhaustive_retrain=True)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, False, True]\n",
    "xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.350211143493652"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc-tic-lm_train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=21.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=23.000, p-value=0.672\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=9.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "3\n",
      "1 ) 1,1,6,0.9,0.1,0.1,True,True,False V 1,1,6,0.9,0.1,0.1,True,False,False [0.864, 0.825, 21.0, 4.968015141912969e-06, True, True, True]\n",
      "2 ) 1,1,6,0.9,0.1,0.1,True,True,False V 1,1,6,0.9,0.1,0.1,True,False,True [0.864, 0.864, 23.0, 0.6718110337653656, False, False, False]\n",
      "3 ) 1,1,6,0.9,0.1,0.1,True,False,False V 1,1,6,0.9,0.1,0.1,True,False,True [0.825, 0.864, 9.0, 3.542223380175076e-09, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "comparison_results = {}\n",
    "for sys1 in xval_results.keys():\n",
    "    for sys2 in xval_results.keys():\n",
    "        if sys1 == sys2:\n",
    "            continue\n",
    "        if sys1 + \" V \" + sys2 in comparison_results.keys() or sys2 + \" V \" + sys1 in comparison_results.keys():\n",
    "            continue\n",
    "        cl1_preds = xval_results[sys1][2]\n",
    "        cl2_preds = xval_results[sys2][2]\n",
    "        sig = calculate_mcnemar_test(cl1_preds, cl2_preds, all_y, alpha=0.05, exact=True)\n",
    "        comparison_results[sys1 + \" V \" + sys2] = [round(xval_results[sys1][1],3), round(xval_results[sys2][1],3)] + list(sig)\n",
    "        \n",
    "print(len(comparison_results))\n",
    "count = 1\n",
    "for k, v in comparison_results.items():\n",
    "    print(count, \")\", k, v)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r6 r3 r7\n",
      "{'n_self': 1, 'k_self': 0.1, 'n_other': 1, 'k_other': 0.9, 'lambda_local': 0.1, 'cutoff_word_global_lm': 6}\n",
      "1.4918701648712158 all lm features extracted\n",
      "[1, 1, 6, 0.9, 0.1, 0.1, True, True, False]\n",
      "[0.8798449612403101] 0.8798449612403101\n"
     ]
    }
   ],
   "source": [
    "print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "\n",
    "print(opt_params)\n",
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "for test in [TEST]:\n",
    "    #print(\"heldout\", heldout)\n",
    "    f, preds, y, accuracy = get_fscore_and_preds(references_per_pair, test, HELDOUT, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                      heldout_extra_fold=EXTRA_HELDOUT)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, True, False]\n",
    "print(exp_params)\n",
    "test_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4520947933197021 all lm features extracted\n",
      "[0.8333333333333334] 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# 2) Get the baseline of lexical info only, no updating/retraining or lms\n",
    "# 2) Get the baseline of lexical info only, no updating/retraining or lms\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "for test in [TEST]:\n",
    "    f, preds, y, accuracy = get_fscore_and_preds(references_per_pair, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                      lm_features = False)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, False, False]\n",
    "test_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4181408882141113 all lm features extracted\n",
      "[0.8992248062015504] 0.8992248062015504\n"
     ]
    }
   ],
   "source": [
    "# 3) see what difference exahustive retraining makes, firstly lexical only, no lm features\n",
    "cross_val_results = []\n",
    "all_preds = []\n",
    "all_y = []\n",
    "for test in [TEST]:\n",
    "    f, preds, y, accuracy = get_fscore_and_preds(references_per_pair, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                      lm_features=False, exhaustive_retrain=True, heldout_extra_fold=EXTRA_HELDOUT)\n",
    "    all_preds.extend(preds)\n",
    "    all_y.extend(y)\n",
    "    cross_val_results.append(accuracy)\n",
    "exp_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, True, False, True]\n",
    "test_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "print(cross_val_results, np.mean(cross_val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=0.000, p-value=0.062\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 1,1,6,0.9,0.1,0.1,True,True,False V 1,1,6,0.9,0.1,0.1,True,False,False [0.88, 0.833, 0.0, 0.00048828125, True, True, True]\n",
      "2 ) 1,1,6,0.9,0.1,0.1,True,True,False V 1,1,6,0.9,0.1,0.1,True,False,True [0.88, 0.899, 0.0, 0.0625, False, False, False]\n",
      "3 ) 1,1,6,0.9,0.1,0.1,True,False,False V 1,1,6,0.9,0.1,0.1,True,False,True [0.833, 0.899, 0.0, 1.52587890625e-05, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "comparison_results_test = {}\n",
    "for sys1 in test_results.keys():\n",
    "    for sys2 in test_results.keys():\n",
    "        if sys1 == sys2:\n",
    "            continue\n",
    "        if sys1 + \" V \" + sys2 in comparison_results_test.keys() or sys2 + \" V \" + sys1 in comparison_results_test.keys():\n",
    "            continue\n",
    "        cl1_preds = test_results[sys1][2]\n",
    "        cl2_preds = test_results[sys2][2]\n",
    "        sig = calculate_mcnemar_test(cl1_preds, cl2_preds, all_y, alpha=0.05, exact=True)\n",
    "        comparison_results_test[sys1 + \" V \" + sys2] = [round(test_results[sys1][1],3), round(test_results[sys2][1],3)] + list(sig)      \n",
    "\n",
    "count = 1\n",
    "for k, v in comparison_results_test.items():\n",
    "    print(count, \")\", k, v)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Removing the training data by one pair each time, down to 1 pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do optimization to find best cut-off and lambda local param values (rather than just cross-entropy as not straightforward)\n",
    "# find best lambda local and cut-off word through cross-val testing, pick one with highest accuracy\n",
    "OPTIMIZING = False\n",
    "if OPTIMIZING:\n",
    "    \n",
    "    # assume these are the same (could be incorrect)\n",
    "    n_self = opt_params['n_self'] \n",
    "    k_self = opt_params['k_self']\n",
    "\n",
    "    print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "    \n",
    "    results = {}\n",
    "    results_file = open(datetime.datetime.now().isoformat() + \"_reduced.csv\", \"w\")\n",
    "    results_file.write(\",\".join(['num_pairs', 'n_other', 'n_self', 'cutoff_word_global_lm', 'k_other', 'k_self', 'lambda_local', 'fscore']) + \"\\n\")\n",
    "    \n",
    "    for num_pairs in range(1,6):  # Full model is using 6 folds for training in cross val\n",
    "        \n",
    "        #cross_val_pairs = list(filter(lambda x:x not in [TEST, HELDOUT, EXTRA_HELDOUT], references_per_pair.keys()))[:num_pairs-1]\n",
    "        #cross_val_pairs.extend([HELDOUT, EXTRA_HELDOUT])\n",
    "        #print(num_pairs, cross_val_pairs)\n",
    "        #cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        \n",
    "        \n",
    "        best_f = 0\n",
    "        best_params = []\n",
    "        for n_other in range(1,2): #1\n",
    "            for k_other_raw in range(5,105,5): #20\n",
    "                k_other = k_other_raw/100\n",
    "                for cutoff_word_global_lm in range(1,11): #200\n",
    "                    # assume convex - not quite right but nearly \n",
    "                    prev = 0\n",
    "                    for lambda_local_raw in range(0,105,5): #4_000 params max, at least 4_00!\n",
    "                        lambda_local = lambda_local_raw/100\n",
    "                        all_scores = []\n",
    "                        for test in list(filter(lambda x:x!=TEST, references_per_pair.keys())):\n",
    "                            heldout = HELDOUT if test!=HELDOUT else EXTRA_HELDOUT # use the heldout when training on full dataset to simulate same lm sizes as in development\n",
    "                            other_pairs = list(filter(lambda x:x not in [heldout, test], \n",
    "                                                      [HELDOUT, EXTRA_HELDOUT] + list(filter(lambda x:x not in [HELDOUT, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']))))\n",
    "                            cross_val_pairs = [heldout, test] + other_pairs[:num_pairs-1]\n",
    "                            #print(cross_val_pairs, test, heldout)\n",
    "                            cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "                            #print(\"heldout\", heldout) \n",
    "                            #print(cross_val_data.keys(), test)\n",
    "                            fscore, preds, labels = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                                        )\n",
    "                            all_scores.append(fscore)\n",
    "                        mean_f = np.mean(all_scores)\n",
    "                        test_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other, k_self, lambda_local]\n",
    "                        print(test_params, mean_f)\n",
    "                        if mean_f < prev: # assume convex - not quite right but nearly \n",
    "                            break\n",
    "                        prev = mean_f\n",
    "                        results[\",\".join([str(f) for f in test_params])] = mean_f\n",
    "                        if mean_f == best_f:\n",
    "                            best_params = [best_params, test_params]\n",
    "                        elif mean_f > best_f:\n",
    "                            best_f = mean_f\n",
    "                            best_params = test_params\n",
    "                            print(\"best\", best_f, best_params)\n",
    "                #results_file = open(\"results\" + datetime.time)\n",
    "                #for row in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "    \n",
    "    for params, result in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "        results_file.write(params + \",\" + str(result) + \"\\n\")\n",
    "    results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pairs</th>\n",
       "      <th>n_other</th>\n",
       "      <th>n_self</th>\n",
       "      <th>cutoff_word_global_lm</th>\n",
       "      <th>k_other</th>\n",
       "      <th>k_self</th>\n",
       "      <th>lambda_local</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.783648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.783520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.782771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.781791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.781598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.603963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.603963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.603963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.603963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.603963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_pairs  n_other  n_self  cutoff_word_global_lm  k_other  k_self   \n",
       "0            1        1       1                      7     0.10     0.1  \\\n",
       "1            1        1       1                      8     0.20     0.1   \n",
       "2            1        1       1                      8     0.10     0.1   \n",
       "3            1        1       1                      6     0.10     0.1   \n",
       "4            1        1       1                      4     0.15     0.1   \n",
       "..         ...      ...     ...                    ...      ...     ...   \n",
       "754          1        1       1                      6     1.00     0.1   \n",
       "755          1        1       1                      7     1.00     0.1   \n",
       "756          1        1       1                      8     1.00     0.1   \n",
       "757          1        1       1                      9     1.00     0.1   \n",
       "758          1        1       1                     10     1.00     0.1   \n",
       "\n",
       "     lambda_local    fscore  \n",
       "0            0.10  0.783648  \n",
       "1            0.25  0.783520  \n",
       "2            0.10  0.782771  \n",
       "3            0.10  0.781791  \n",
       "4            0.05  0.781598  \n",
       "..            ...       ...  \n",
       "754          0.00  0.603963  \n",
       "755          0.00  0.603963  \n",
       "756          0.00  0.603963  \n",
       "757          0.00  0.603963  \n",
       "758          0.00  0.603963  \n",
       "\n",
       "[759 rows x 8 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if already optimized look at results\n",
    "df = pd.read_csv(open(\"2023-06-21T08:17:12.999981_reduced.csv\"))\n",
    "local_df = df[df['num_pairs']==1]\n",
    "local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_reduced = {}\n",
    "for num_training_pairs in range(1,6):\n",
    "    local_df = df[df['num_pairs']==num_training_pairs]\n",
    "    opt_params_reduced['n_self_{}'.format(num_training_pairs)] = int(local_df.iloc[0].n_self)\n",
    "    opt_params_reduced['k_self_{}'.format(num_training_pairs)] = local_df.iloc[0].k_self\n",
    "    opt_params_reduced['n_other_{}'.format(num_training_pairs)] = int(local_df.iloc[0].n_other)\n",
    "    opt_params_reduced['k_other_{}'.format(num_training_pairs)] = local_df.iloc[0].k_other\n",
    "    opt_params_reduced['lambda_local_{}'.format(num_training_pairs)] = local_df.iloc[0].lambda_local\n",
    "    opt_params_reduced['cutoff_word_global_lm_{}'.format(num_training_pairs)] = int(local_df.iloc[0].cutoff_word_global_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the cutoff and lambda for 6 pairs is the same as the global, as trained on all\n",
    "opt_params_reduced['n_self_{}'.format(6)] = opt_params['n_self']\n",
    "opt_params_reduced['k_self_{}'.format(6)] = opt_params['k_self']\n",
    "opt_params_reduced['n_other_{}'.format(6)] = opt_params['n_other']\n",
    "opt_params_reduced['k_other_{}'.format(6)] = opt_params['k_other']\n",
    "opt_params_reduced['lambda_local_{}'.format(6)] = opt_params['lambda_local']\n",
    "opt_params_reduced['cutoff_word_global_lm_{}'.format(6)] = opt_params['cutoff_word_global_lm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_self_1 1\n",
      "k_self_1 0.1\n",
      "n_other_1 1\n",
      "k_other_1 0.1\n",
      "lambda_local_1 0.1\n",
      "cutoff_word_global_lm_1 7\n",
      "n_self_2 1\n",
      "k_self_2 0.1\n",
      "n_other_2 1\n",
      "k_other_2 1.0\n",
      "lambda_local_2 0.8\n",
      "cutoff_word_global_lm_2 6\n",
      "n_self_3 1\n",
      "k_self_3 0.1\n",
      "n_other_3 1\n",
      "k_other_3 0.95\n",
      "lambda_local_3 0.65\n",
      "cutoff_word_global_lm_3 8\n",
      "n_self_4 1\n",
      "k_self_4 0.1\n",
      "n_other_4 1\n",
      "k_other_4 0.75\n",
      "lambda_local_4 0.4\n",
      "cutoff_word_global_lm_4 5\n",
      "n_self_5 1\n",
      "k_self_5 0.1\n",
      "n_other_5 1\n",
      "k_other_5 0.7\n",
      "lambda_local_5 0.3\n",
      "cutoff_word_global_lm_5 7\n",
      "n_self_6 1\n",
      "k_self_6 0.1\n",
      "n_other_6 1\n",
      "k_other_6 0.9\n",
      "lambda_local_6 0.1\n",
      "cutoff_word_global_lm_6 6\n"
     ]
    }
   ],
   "source": [
    "for param in opt_params_reduced:\n",
    "    print(param, opt_params_reduced[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_xval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r6 r3 r7\n",
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, True, False]\n",
      "['r3', 'r1'] r1 r3\n",
      "0.2286689281463623 all lm features extracted\n",
      "['r3', 'r2'] r2 r3\n",
      "0.22969603538513184 all lm features extracted\n",
      "['r7', 'r3'] r3 r7\n",
      "0.32456493377685547 all lm features extracted\n",
      "['r3', 'r4'] r4 r3\n",
      "0.22815895080566406 all lm features extracted\n",
      "['r3', 'r5'] r5 r3\n",
      "0.22738289833068848 all lm features extracted\n",
      "['r3', 'r7'] r7 r3\n",
      "0.23005104064941406 all lm features extracted\n",
      "['r3', 'r8'] r8 r3\n",
      "0.22387003898620605 all lm features extracted\n",
      "[0.77, 0.711764705882353, 0.8377358490566038, 0.8456140350877193, 0.7692307692307693, 0.8039772727272727, 0.7392638036809815] 0.7825123479522428\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, True, False]\n",
      "['r3', 'r1', 'r7'] r1 r3\n",
      "0.5509071350097656 all lm features extracted\n",
      "['r3', 'r2', 'r7'] r2 r3\n",
      "0.5464608669281006 all lm features extracted\n",
      "['r7', 'r3', 'r1'] r3 r7\n",
      "0.3910789489746094 all lm features extracted\n",
      "['r3', 'r4', 'r7'] r4 r3\n",
      "0.5445067882537842 all lm features extracted\n",
      "['r3', 'r5', 'r7'] r5 r3\n",
      "0.5343880653381348 all lm features extracted\n",
      "['r3', 'r7', 'r1'] r7 r3\n",
      "0.29266834259033203 all lm features extracted\n",
      "['r3', 'r8', 'r7'] r8 r3\n",
      "0.5471389293670654 all lm features extracted\n",
      "[0.86, 0.7705882352941177, 0.8754716981132076, 0.8631578947368421, 0.8741258741258742, 0.8068181818181818, 0.7392638036809815] 0.8270608125384579\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, True, False]\n",
      "['r3', 'r1', 'r7', 'r2'] r1 r3\n",
      "0.6695277690887451 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1'] r2 r3\n",
      "0.614382266998291 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2'] r3 r7\n",
      "0.5142629146575928 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1'] r4 r3\n",
      "0.6439018249511719 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1'] r5 r3\n",
      "0.6046726703643799 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2'] r7 r3\n",
      "0.41945886611938477 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1'] r8 r3\n",
      "0.6041409969329834 all lm features extracted\n",
      "[0.87, 0.7588235294117647, 0.8716981132075472, 0.8596491228070176, 0.8811188811188811, 0.8295454545454546, 0.754601226993865] 0.8322051897263615\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, True, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4'] r1 r3\n",
      "0.9304032325744629 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4'] r2 r3\n",
      "0.8619749546051025 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4'] r3 r7\n",
      "0.77034592628479 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2'] r4 r3\n",
      "0.7354533672332764 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2'] r5 r3\n",
      "0.7459027767181396 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4'] r7 r3\n",
      "0.6854140758514404 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2'] r8 r3\n",
      "0.7353861331939697 all lm features extracted\n",
      "[0.86, 0.8, 0.9018867924528302, 0.8842105263157894, 0.8881118881118881, 0.875, 0.7668711656441718] 0.8537257675035256\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, True, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5'] r1 r3\n",
      "1.0200190544128418 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5'] r2 r3\n",
      "0.9568910598754883 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5'] r3 r7\n",
      "0.9133422374725342 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5'] r4 r3\n",
      "0.8442926406860352 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4'] r5 r3\n",
      "0.9820606708526611 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r7 r3\n",
      "0.7895388603210449 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4'] r8 r3\n",
      "0.9902651309967041 all lm features extracted\n",
      "[0.86, 0.8058823529411765, 0.8641509433962264, 0.8807017543859649, 0.8741258741258742, 0.875, 0.754601226993865] 0.8449231645490153\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, True, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5', 'r8'] r1 r3\n",
      "1.3302438259124756 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5', 'r8'] r2 r3\n",
      "1.2826099395751953 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5', 'r8'] r3 r7\n",
      "1.1920108795166016 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5', 'r8'] r4 r3\n",
      "1.1546950340270996 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4', 'r8'] r5 r3\n",
      "1.3078358173370361 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r7 r3\n",
      "1.0878527164459229 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4', 'r5'] r8 r3\n",
      "1.1146612167358398 all lm features extracted\n",
      "[0.87, 0.8176470588235294, 0.8943396226415095, 0.8842105263157894, 0.8951048951048951, 0.8892045454545454, 0.7944785276073619] 0.8635693108496615\n",
      "{1: 0.7825123479522428, 2: 0.8270608125384579, 3: 0.8322051897263615, 4: 0.8537257675035256, 5: 0.8449231645490153, 6: 0.8635693108496615}\n"
     ]
    }
   ],
   "source": [
    "# 1) re-run best cross-val result\n",
    "print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "\n",
    "local_results = {}\n",
    "for num_pairs in range(1,7):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = True\n",
    "    retrain = False\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate(['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        heldout = HELDOUT if test!=HELDOUT else EXTRA_HELDOUT # use the heldout when training on full dataset to simulate same lm sizes as in development\n",
    "        other_pairs = list(filter(lambda x:x not in [heldout, test], \n",
    "                                  [HELDOUT, EXTRA_HELDOUT] + list(filter(lambda x:x not in [HELDOUT, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']))))\n",
    "        cross_val_pairs = [heldout, test] + other_pairs[:num_pairs-1]\n",
    "        print(cross_val_pairs, test, heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r6 r3 r7\n",
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, False, False]\n",
      "['r3', 'r1'] r1 r3\n",
      "0.2290349006652832 all lm features extracted\n",
      "['r3', 'r2'] r2 r3\n",
      "0.22480511665344238 all lm features extracted\n",
      "['r7', 'r3'] r3 r7\n",
      "0.31954193115234375 all lm features extracted\n",
      "['r3', 'r4'] r4 r3\n",
      "0.22333598136901855 all lm features extracted\n",
      "['r3', 'r5'] r5 r3\n",
      "0.22367405891418457 all lm features extracted\n",
      "['r3', 'r7'] r7 r3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22140789031982422 all lm features extracted\n",
      "['r3', 'r8'] r8 r3\n",
      "0.2231738567352295 all lm features extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67, 0.5411764705882353, 0.6867924528301886, 0.7578947368421053, 0.6713286713286714, 0.7272727272727273, 0.5644171779141104] 0.6598403195394341\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, False, False]\n",
      "['r3', 'r1', 'r7'] r1 r3\n",
      "0.5361559391021729 all lm features extracted\n",
      "['r3', 'r2', 'r7'] r2 r3\n",
      "0.537459135055542 all lm features extracted\n",
      "['r7', 'r3', 'r1'] r3 r7\n",
      "0.38897705078125 all lm features extracted\n",
      "['r3', 'r4', 'r7'] r4 r3\n",
      "0.5315558910369873 all lm features extracted\n",
      "['r3', 'r5', 'r7'] r5 r3\n",
      "0.5318489074707031 all lm features extracted\n",
      "['r3', 'r7', 'r1'] r7 r3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28885889053344727 all lm features extracted\n",
      "['r3', 'r8', 'r7'] r8 r3\n",
      "0.5316002368927002 all lm features extracted\n",
      "[0.72, 0.5352941176470588, 0.8150943396226416, 0.8350877192982457, 0.7832167832167832, 0.7698863636363636, 0.5858895705521472] 0.7206384134247487\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, False, False]\n",
      "['r3', 'r1', 'r7', 'r2'] r1 r3\n",
      "0.6618103981018066 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1'] r2 r3\n",
      "0.605445146560669 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2'] r3 r7\n",
      "0.5120658874511719 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1'] r4 r3\n",
      "0.6473898887634277 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1'] r5 r3\n",
      "0.6048290729522705 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2'] r7 r3\n",
      "0.4182169437408447 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1'] r8 r3\n",
      "0.6028010845184326 all lm features extracted\n",
      "[0.72, 0.5352941176470588, 0.8528301886792453, 0.8350877192982457, 0.7762237762237763, 0.7897727272727273, 0.6012269938650306] 0.7300622175694406\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, False, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4'] r1 r3\n",
      "0.9175796508789062 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4'] r2 r3\n",
      "0.8527960777282715 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4'] r3 r7\n",
      "0.7583508491516113 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2'] r4 r3\n",
      "0.7399301528930664 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2'] r5 r3\n",
      "0.7388088703155518 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4'] r7 r3\n",
      "0.6679151058197021 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2'] r8 r3\n",
      "0.7736508846282959 all lm features extracted\n",
      "[0.76, 0.7, 0.879245283018868, 0.8842105263157894, 0.8391608391608392, 0.8181818181818182, 0.6993865030674846] 0.7971692813921142\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, False, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5'] r1 r3\n",
      "1.0292003154754639 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5'] r2 r3\n",
      "0.9906339645385742 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5'] r3 r7\n",
      "0.9878368377685547 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5'] r4 r3\n",
      "0.9874649047851562 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4'] r5 r3\n",
      "1.1805520057678223 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r7 r3\n",
      "0.8896329402923584 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4'] r8 r3\n",
      "1.162282943725586 all lm features extracted\n",
      "[0.71, 0.7176470588235294, 0.8528301886792453, 0.8807017543859649, 0.8391608391608392, 0.8181818181818182, 0.7331288343558282] 0.7930929276553178\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, False, False]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5', 'r8'] r1 r3\n",
      "1.3362040519714355 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5', 'r8'] r2 r3\n",
      "1.2760729789733887 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5', 'r8'] r3 r7\n",
      "1.201348066329956 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5', 'r8'] r4 r3\n",
      "1.16581392288208 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4', 'r8'] r5 r3\n",
      "1.377957820892334 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r7 r3\n",
      "1.0741229057312012 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4', 'r5'] r8 r3\n",
      "1.1042711734771729 all lm features extracted\n",
      "[0.71, 0.8, 0.8943396226415095, 0.8912280701754386, 0.8531468531468531, 0.8579545454545454, 0.7699386503067485] 0.8252296773892994\n",
      "{1: 0.6598403195394341, 2: 0.7206384134247487, 3: 0.7300622175694406, 4: 0.7971692813921142, 5: 0.7930929276553178, 6: 0.8252296773892994}\n"
     ]
    }
   ],
   "source": [
    "# 2) lexical only baseline\n",
    "print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "\n",
    "local_results = {}\n",
    "for num_pairs in range(1,7):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = False\n",
    "    retrain = False\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate(['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        heldout = HELDOUT if test!=HELDOUT else EXTRA_HELDOUT # use the heldout when training on full dataset to simulate same lm sizes as in development\n",
    "        other_pairs = list(filter(lambda x:x not in [heldout, test], \n",
    "                                  [HELDOUT, EXTRA_HELDOUT] + list(filter(lambda x:x not in [HELDOUT, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']))))\n",
    "        cross_val_pairs = [heldout, test] + other_pairs[:num_pairs-1]\n",
    "        print(cross_val_pairs, test, heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r6 r3 r7\n",
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, False, True]\n",
      "['r3', 'r1'] r1 r3\n",
      "0.23499011993408203 all lm features extracted\n",
      "['r3', 'r2'] r2 r3\n",
      "0.2220621109008789 all lm features extracted\n",
      "['r7', 'r3'] r3 r7\n",
      "0.3152937889099121 all lm features extracted\n",
      "['r3', 'r4'] r4 r3\n",
      "0.21932411193847656 all lm features extracted\n",
      "['r3', 'r5'] r5 r3\n",
      "0.2200171947479248 all lm features extracted\n",
      "['r3', 'r7'] r7 r3\n",
      "0.2262861728668213 all lm features extracted\n",
      "['r3', 'r8'] r8 r3\n",
      "0.2205650806427002 all lm features extracted\n",
      "[0.82, 0.7705882352941177, 0.879245283018868, 0.8526315789473684, 0.8181818181818182, 0.8323863636363636, 0.7361963190184049] 0.8156042282995629\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, False, True]\n",
      "['r3', 'r1', 'r7'] r1 r3\n",
      "0.5421440601348877 all lm features extracted\n",
      "['r3', 'r2', 'r7'] r2 r3\n",
      "0.5365328788757324 all lm features extracted\n",
      "['r7', 'r3', 'r1'] r3 r7\n",
      "0.3889789581298828 all lm features extracted\n",
      "['r3', 'r4', 'r7'] r4 r3\n",
      "0.5377721786499023 all lm features extracted\n",
      "['r3', 'r5', 'r7'] r5 r3\n",
      "0.6103308200836182 all lm features extracted\n",
      "['r3', 'r7', 'r1'] r7 r3\n",
      "0.30638909339904785 all lm features extracted\n",
      "['r3', 'r8', 'r7'] r8 r3\n",
      "0.5355567932128906 all lm features extracted\n",
      "[0.87, 0.7705882352941177, 0.8905660377358491, 0.8912280701754386, 0.8531468531468531, 0.8636363636363636, 0.7300613496932515] 0.8384609870974105\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, False, True]\n",
      "['r3', 'r1', 'r7', 'r2'] r1 r3\n",
      "0.67807936668396 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1'] r2 r3\n",
      "0.6292760372161865 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2'] r3 r7\n",
      "0.5212647914886475 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1'] r4 r3\n",
      "0.6129281520843506 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1'] r5 r3\n",
      "0.6274957656860352 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2'] r7 r3\n",
      "0.43956589698791504 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1'] r8 r3\n",
      "0.6630368232727051 all lm features extracted\n",
      "[0.86, 0.7764705882352941, 0.8943396226415095, 0.887719298245614, 0.8671328671328671, 0.8579545454545454, 0.7361963190184049] 0.8399733201040336\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, False, True]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4'] r1 r3\n",
      "0.9138798713684082 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4'] r2 r3\n",
      "0.8546991348266602 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4'] r3 r7\n",
      "0.7805721759796143 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2'] r4 r3\n",
      "0.7717568874359131 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2'] r5 r3\n",
      "0.7561531066894531 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4'] r7 r3\n",
      "0.6779510974884033 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2'] r8 r3\n",
      "0.756267786026001 all lm features extracted\n",
      "[0.87, 0.8, 0.909433962264151, 0.8912280701754386, 0.8601398601398601, 0.8835227272727273, 0.7515337423312883] 0.8522654803119236\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, False, True]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5'] r1 r3\n",
      "1.0349948406219482 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5'] r2 r3\n",
      "0.986832857131958 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5'] r3 r7\n",
      "0.8876490592956543 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5'] r4 r3\n",
      "0.9076719284057617 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4'] r5 r3\n",
      "0.9993560314178467 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r7 r3\n",
      "0.792104959487915 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4'] r8 r3\n",
      "1.0259678363800049 all lm features extracted\n",
      "[0.86, 0.8117647058823529, 0.9018867924528302, 0.887719298245614, 0.8741258741258742, 0.8835227272727273, 0.7638036809815951] 0.854689011280142\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, False, True]\n",
      "['r3', 'r1', 'r7', 'r2', 'r4', 'r5', 'r8'] r1 r3\n",
      "1.3616976737976074 all lm features extracted\n",
      "['r3', 'r2', 'r7', 'r1', 'r4', 'r5', 'r8'] r2 r3\n",
      "1.3371589183807373 all lm features extracted\n",
      "['r7', 'r3', 'r1', 'r2', 'r4', 'r5', 'r8'] r3 r7\n",
      "1.244084119796753 all lm features extracted\n",
      "['r3', 'r4', 'r7', 'r1', 'r2', 'r5', 'r8'] r4 r3\n",
      "1.1687321662902832 all lm features extracted\n",
      "['r3', 'r5', 'r7', 'r1', 'r2', 'r4', 'r8'] r5 r3\n",
      "1.4367012977600098 all lm features extracted\n",
      "['r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r7 r3\n",
      "1.2864272594451904 all lm features extracted\n",
      "['r3', 'r8', 'r7', 'r1', 'r2', 'r4', 'r5'] r8 r3\n",
      "1.2291700839996338 all lm features extracted\n",
      "[0.86, 0.8352941176470589, 0.909433962264151, 0.8982456140350877, 0.8741258741258742, 0.8920454545454546, 0.7822085889570553] 0.8644790873678118\n",
      "{1: 0.8156042282995629, 2: 0.8384609870974105, 3: 0.8399733201040336, 4: 0.8522654803119236, 5: 0.854689011280142, 6: 0.8644790873678118}\n"
     ]
    }
   ],
   "source": [
    "# 3) Exhaustive retraining competitor\n",
    "print(TEST, HELDOUT, EXTRA_HELDOUT)\n",
    "\n",
    "local_results = {}\n",
    "for num_pairs in range(1,7):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = False\n",
    "    retrain = True\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate(['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        heldout = HELDOUT if test!=HELDOUT else EXTRA_HELDOUT # use the heldout when training on full dataset to simulate same lm sizes as in development\n",
    "        other_pairs = list(filter(lambda x:x not in [heldout, test], \n",
    "                                  [HELDOUT, EXTRA_HELDOUT] + list(filter(lambda x:x not in [HELDOUT, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r7', 'r8']))))\n",
    "        cross_val_pairs = [heldout, test] + other_pairs[:num_pairs-1]\n",
    "        print(cross_val_pairs, test, heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_xval_results[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pairs 1\n",
      "statistic=71.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=53.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=21.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 1,1,1,7,0.1,0.1,0.1,True,True,False V 1,1,1,7,0.1,0.1,0.1,True,False,False [0.783, 0.66, 71.0, 4.6274975325333254e-29, True, True, True]\n",
      "2 ) 1,1,1,7,0.1,0.1,0.1,True,True,False V 1,1,1,7,0.1,0.1,0.1,True,False,True [0.783, 0.816, 53.0, 0.0004097727655174, True, True, True]\n",
      "3 ) 1,1,1,7,0.1,0.1,0.1,True,False,False V 1,1,1,7,0.1,0.1,0.1,True,False,True [0.66, 0.816, 21.0, 1.7951571978789752e-55, True, True, True]\n",
      "******************************\n",
      "num pairs 2\n",
      "statistic=53.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=40.000, p-value=0.012\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=19.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 2,1,1,6,1.0,0.1,0.8,True,True,False V 2,1,1,6,1.0,0.1,0.8,True,False,False [0.827, 0.721, 53.0, 1.1167071840999235e-22, True, True, True]\n",
      "2 ) 2,1,1,6,1.0,0.1,0.8,True,True,False V 2,1,1,6,1.0,0.1,0.8,True,False,True [0.827, 0.838, 40.0, 0.011598202043786152, True, False, False]\n",
      "3 ) 2,1,1,6,1.0,0.1,0.8,True,False,False V 2,1,1,6,1.0,0.1,0.8,True,False,True [0.721, 0.838, 19.0, 2.8351872518370702e-39, True, True, True]\n",
      "******************************\n",
      "num pairs 3\n",
      "statistic=48.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=42.000, p-value=0.092\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=15.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 3,1,1,8,0.95,0.1,0.65,True,True,False V 3,1,1,8,0.95,0.1,0.65,True,False,False [0.832, 0.73, 48.0, 1.3869362755418012e-21, True, True, True]\n",
      "2 ) 3,1,1,8,0.95,0.1,0.65,True,True,False V 3,1,1,8,0.95,0.1,0.65,True,False,True [0.832, 0.84, 42.0, 0.09183767826908244, False, False, False]\n",
      "3 ) 3,1,1,8,0.95,0.1,0.65,True,False,False V 3,1,1,8,0.95,0.1,0.65,True,False,True [0.73, 0.84, 15.0, 2.6948949525688824e-36, True, True, True]\n",
      "******************************\n",
      "num pairs 4\n",
      "statistic=25.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=35.000, p-value=1.000\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=7.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 4,1,1,5,0.75,0.1,0.4,True,True,False V 4,1,1,5,0.75,0.1,0.4,True,False,False [0.854, 0.797, 25.0, 2.7976456925235377e-13, True, True, True]\n",
      "2 ) 4,1,1,5,0.75,0.1,0.4,True,True,False V 4,1,1,5,0.75,0.1,0.4,True,False,True [0.854, 0.852, 35.0, 0.9999999999999998, False, False, False]\n",
      "3 ) 4,1,1,5,0.75,0.1,0.4,True,False,False V 4,1,1,5,0.75,0.1,0.4,True,False,True [0.797, 0.852, 7.0, 6.048586330822969e-19, True, True, True]\n",
      "******************************\n",
      "num pairs 5\n",
      "statistic=23.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=29.000, p-value=0.040\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=7.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 5,1,1,7,0.7,0.1,0.3,True,True,False V 5,1,1,7,0.7,0.1,0.3,True,False,False [0.845, 0.793, 23.0, 3.795723347076606e-10, True, True, True]\n",
      "2 ) 5,1,1,7,0.7,0.1,0.3,True,True,False V 5,1,1,7,0.7,0.1,0.3,True,False,True [0.845, 0.855, 29.0, 0.03953953287797775, True, False, False]\n",
      "3 ) 5,1,1,7,0.7,0.1,0.3,True,False,False V 5,1,1,7,0.7,0.1,0.3,True,False,True [0.793, 0.855, 7.0, 9.440274241692034e-20, True, True, True]\n",
      "******************************\n",
      "num pairs 6\n",
      "statistic=21.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=23.000, p-value=0.672\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=9.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 6,1,1,6,0.9,0.1,0.1,True,True,False V 6,1,1,6,0.9,0.1,0.1,True,False,False [0.864, 0.825, 21.0, 4.968015141912969e-06, True, True, True]\n",
      "2 ) 6,1,1,6,0.9,0.1,0.1,True,True,False V 6,1,1,6,0.9,0.1,0.1,True,False,True [0.864, 0.864, 23.0, 0.6718110337653656, False, False, False]\n",
      "3 ) 6,1,1,6,0.9,0.1,0.1,True,False,False V 6,1,1,6,0.9,0.1,0.1,True,False,True [0.825, 0.864, 9.0, 3.542223380175076e-09, True, True, True]\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for num_pairs in range(1,7):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "    reduced_comparison_results = {}\n",
    "    for sys1 in reduced_xval_results.keys():\n",
    "        if not int(sys1.split(\",\")[0]) == num_pairs:\n",
    "            continue\n",
    "        for sys2 in reduced_xval_results.keys():\n",
    "            if not int(sys2.split(\",\")[0]) == num_pairs:\n",
    "                continue\n",
    "            if sys1 == sys2:\n",
    "                continue\n",
    "            if sys1 + \" V \" + sys2 in reduced_comparison_results.keys() or sys2 + \" V \" + sys1 in reduced_comparison_results.keys():\n",
    "                continue\n",
    "            cl1_preds = reduced_xval_results[sys1][2]\n",
    "            cl2_preds = reduced_xval_results[sys2][2]\n",
    "            sig = calculate_mcnemar_test(cl1_preds, cl2_preds, all_y, alpha=0.05, exact=True)\n",
    "            reduced_comparison_results[sys1 + \" V \" + sys2] = [round(reduced_xval_results[sys1][1],3), round(reduced_xval_results[sys2][1],3)] + list(sig)\n",
    "\n",
    "\n",
    "    count = 1\n",
    "    for k, v in reduced_comparison_results.items():\n",
    "        print(count, \")\", k, v)\n",
    "        count+=1\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get results on the test data\n",
    "reduced_results_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, True, False]\n",
      "['r6', 'r3'] r6 r3 None\n",
      "0.2276449203491211 all lm features extracted\n",
      "[0.813953488372093] 0.813953488372093\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, True, False]\n",
      "['r6', 'r3', 'r7'] r6 r3 None\n",
      "0.5350580215454102 all lm features extracted\n",
      "[0.8488372093023255] 0.8488372093023255\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, True, False]\n",
      "['r6', 'r3', 'r7', 'r1'] r6 r3 None\n",
      "0.6088743209838867 all lm features extracted\n",
      "[0.8527131782945736] 0.8527131782945736\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, True, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2'] r6 r3 None\n",
      "0.7388148307800293 all lm features extracted\n",
      "[0.8255813953488372] 0.8255813953488372\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, True, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4'] r6 r3 None\n",
      "0.9938099384307861 all lm features extracted\n",
      "[0.8410852713178295] 0.8410852713178295\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, True, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r6 r3 None\n",
      "1.1429212093353271 all lm features extracted\n",
      "[0.8410852713178295] 0.8410852713178295\n",
      "num pairs 7\n",
      "[7, 1, 1, 6, 0.9, 0.1, 0.1, True, True, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r6 r3 r7\n",
      "1.3985469341278076 all lm features extracted\n",
      "[0.8798449612403101] 0.8798449612403101\n",
      "{1: 0.813953488372093, 2: 0.8488372093023255, 3: 0.8527131782945736, 4: 0.8255813953488372, 5: 0.8410852713178295, 6: 0.8410852713178295, 7: 0.8798449612403101}\n"
     ]
    }
   ],
   "source": [
    "# (1) optimized with lm features \n",
    "local_results = {}\n",
    "\n",
    "for num_pairs in range(1,8):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "    \n",
    "    #if num_pairs in [4,5,6]:\n",
    "    #    n_other = opt_params_reduced['n_other_{}'.format(6)]\n",
    "    #    k_other = opt_params_reduced['k_other_{}'.format(6)]\n",
    "     #   lambda_local = opt_params_reduced['lambda_local_{}'.format(6)]\n",
    "     #   cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(6)]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = True\n",
    "    retrain = False\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate([TEST]):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        other_pairs = list(filter(lambda x:x not in [HELDOUT, TEST, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8']))\n",
    "        #random.seed(num_pairs)\n",
    "        #shuffle(other_pairs)\n",
    "        cross_val_pairs = [TEST, HELDOUT, EXTRA_HELDOUT] + other_pairs\n",
    "        cross_val_pairs = cross_val_pairs[:num_pairs+1]\n",
    "        extra_heldout = EXTRA_HELDOUT  if num_pairs > 6 else None # HELDOUT\n",
    "        print(cross_val_pairs, test, HELDOUT, extra_heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        \n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, HELDOUT, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                     heldout_extra_fold=extra_heldout,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_results_test[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, False, False]\n",
      "['r6', 'r3'] r6 r3 None\n",
      "0.22547507286071777 all lm features extracted\n",
      "[0.5968992248062015] 0.5968992248062015\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, False, False]\n",
      "['r6', 'r3', 'r7'] r6 r3 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5361320972442627 all lm features extracted\n",
      "[0.7170542635658915] 0.7170542635658915\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, False, False]\n",
      "['r6', 'r3', 'r7', 'r1'] r6 r3 None\n",
      "0.6036379337310791 all lm features extracted\n",
      "[0.7286821705426356] 0.7286821705426356\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, False, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2'] r6 r3 None\n",
      "0.7428689002990723 all lm features extracted\n",
      "[0.8294573643410853] 0.8294573643410853\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, False, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4'] r6 r3 None\n",
      "0.9828410148620605 all lm features extracted\n",
      "[0.8294573643410853] 0.8294573643410853\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, False, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r6 r3 None\n",
      "1.1102240085601807 all lm features extracted\n",
      "[0.8333333333333334] 0.8333333333333334\n",
      "num pairs 7\n",
      "[7, 1, 1, 6, 0.9, 0.1, 0.1, True, False, False]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r6 r3 r7\n",
      "1.4151499271392822 all lm features extracted\n",
      "[0.8333333333333334] 0.8333333333333334\n",
      "{1: 0.5968992248062015, 2: 0.7170542635658915, 3: 0.7286821705426356, 4: 0.8294573643410853, 5: 0.8294573643410853, 6: 0.8333333333333334, 7: 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "# (2) lexical only baseline\n",
    "\n",
    "local_results = {}\n",
    "\n",
    "for num_pairs in range(1,8):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "    \n",
    "    #if num_pairs in [4,5,6]:\n",
    "    #    n_other = opt_params_reduced['n_other_{}'.format(6)]\n",
    "    #    k_other = opt_params_reduced['k_other_{}'.format(6)]\n",
    "     #   lambda_local = opt_params_reduced['lambda_local_{}'.format(6)]\n",
    "     #   cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(6)]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = False\n",
    "    retrain = False\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate([TEST]):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        other_pairs = list(filter(lambda x:x not in [HELDOUT, TEST, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8']))\n",
    "        #random.seed(num_pairs)\n",
    "        #shuffle(other_pairs)\n",
    "        cross_val_pairs = [TEST, HELDOUT, EXTRA_HELDOUT] + other_pairs\n",
    "        cross_val_pairs = cross_val_pairs[:num_pairs+1]\n",
    "        extra_heldout = EXTRA_HELDOUT  if num_pairs > 6 else None # HELDOUT\n",
    "        print(cross_val_pairs, test, HELDOUT, extra_heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        \n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, HELDOUT, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                     heldout_extra_fold=extra_heldout,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_results_test[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pairs 1\n",
      "[1, 1, 1, 7, 0.1, 0.1, 0.1, True, False, True]\n",
      "['r6', 'r3'] r6 r3 None\n",
      "0.22508001327514648 all lm features extracted\n",
      "[0.8565891472868217] 0.8565891472868217\n",
      "num pairs 2\n",
      "[2, 1, 1, 6, 1.0, 0.1, 0.8, True, False, True]\n",
      "['r6', 'r3', 'r7'] r6 r3 None\n",
      "0.5474739074707031 all lm features extracted\n",
      "[0.8837209302325582] 0.8837209302325582\n",
      "num pairs 3\n",
      "[3, 1, 1, 8, 0.95, 0.1, 0.65, True, False, True]\n",
      "['r6', 'r3', 'r7', 'r1'] r6 r3 None\n",
      "0.6108858585357666 all lm features extracted\n",
      "[0.8875968992248062] 0.8875968992248062\n",
      "num pairs 4\n",
      "[4, 1, 1, 5, 0.75, 0.1, 0.4, True, False, True]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2'] r6 r3 None\n",
      "0.7434990406036377 all lm features extracted\n",
      "[0.8992248062015504] 0.8992248062015504\n",
      "num pairs 5\n",
      "[5, 1, 1, 7, 0.7, 0.1, 0.3, True, False, True]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4'] r6 r3 None\n",
      "1.0084960460662842 all lm features extracted\n",
      "[0.9031007751937985] 0.9031007751937985\n",
      "num pairs 6\n",
      "[6, 1, 1, 6, 0.9, 0.1, 0.1, True, False, True]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5'] r6 r3 None\n",
      "1.142138957977295 all lm features extracted\n",
      "[0.9069767441860465] 0.9069767441860465\n",
      "num pairs 7\n",
      "[7, 1, 1, 6, 0.9, 0.1, 0.1, True, False, True]\n",
      "['r6', 'r3', 'r7', 'r1', 'r2', 'r4', 'r5', 'r8'] r6 r3 r7\n",
      "1.4005019664764404 all lm features extracted\n",
      "[0.8992248062015504] 0.8992248062015504\n",
      "{1: 0.8565891472868217, 2: 0.8837209302325582, 3: 0.8875968992248062, 4: 0.8992248062015504, 5: 0.9031007751937985, 6: 0.9069767441860465, 7: 0.8992248062015504}\n"
     ]
    }
   ],
   "source": [
    "# (3) Exhausitve retrain lexical only\n",
    "\n",
    "local_results = {}\n",
    "\n",
    "for num_pairs in range(1,8):\n",
    "    print(\"num pairs\", num_pairs)\n",
    "    #print(opt_params_reduced)\n",
    "\n",
    "    n_self = opt_params_reduced['n_self_{}'.format(min([6,num_pairs]))]\n",
    "    k_self = opt_params_reduced['k_self_{}'.format(min([6,num_pairs]))]\n",
    "    n_other = opt_params_reduced['n_other_{}'.format(min([6,num_pairs]))]\n",
    "    k_other = opt_params_reduced['k_other_{}'.format(min([6,num_pairs]))]\n",
    "\n",
    "\n",
    "    lambda_local = opt_params_reduced['lambda_local_{}'.format(min([6,num_pairs]))]\n",
    "    cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(min([6,num_pairs]))]\n",
    "    \n",
    "    \n",
    "    #if num_pairs in [4,5,6]:\n",
    "    #    n_other = opt_params_reduced['n_other_{}'.format(6)]\n",
    "    #    k_other = opt_params_reduced['k_other_{}'.format(6)]\n",
    "     #   lambda_local = opt_params_reduced['lambda_local_{}'.format(6)]\n",
    "     #   cutoff_word_global_lm = opt_params_reduced['cutoff_word_global_lm_{}'.format(6)]\n",
    "\n",
    "    lexical = True\n",
    "    lm_features = False\n",
    "    retrain = True\n",
    "\n",
    "    exp_params = [num_pairs, n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local, lexical, lm_features, retrain]\n",
    "    print(exp_params)\n",
    "    cross_val_results = []\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for i, test in enumerate([TEST]):\n",
    "        #random.seed(0)  # note in training this was always 0\n",
    "        #shuffle(training_folds)\n",
    "        \n",
    "        other_pairs = list(filter(lambda x:x not in [HELDOUT, TEST, EXTRA_HELDOUT], ['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8']))\n",
    "        #random.seed(num_pairs)\n",
    "        #shuffle(other_pairs)\n",
    "        cross_val_pairs = [TEST, HELDOUT, EXTRA_HELDOUT] + other_pairs\n",
    "        cross_val_pairs = cross_val_pairs[:num_pairs+1]\n",
    "        extra_heldout = EXTRA_HELDOUT  if num_pairs > 6 else None # HELDOUT\n",
    "        print(cross_val_pairs, test, HELDOUT, extra_heldout)\n",
    "        cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x in cross_val_pairs, references_per_pair.keys())}\n",
    "        #print(\"heldout\", heldout) \n",
    "        #print(cross_val_data.keys(), test)\n",
    "        \n",
    "        fscore, preds, labels, accuracy = get_fscore_and_preds(cross_val_data, test, HELDOUT, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm,\n",
    "                                                     heldout_extra_fold=extra_heldout,\n",
    "                                                      lexical=lexical, lm_features=lm_features, exhaustive_retrain=retrain,\n",
    "                                                      no_train=False)\n",
    "                                                    \n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_y.extend(labels)\n",
    "        cross_val_results.append(accuracy)\n",
    "\n",
    "    reduced_results_test[','.join([str(p) for p in exp_params])] = (cross_val_results, np.mean(cross_val_results),all_preds)\n",
    "    print(cross_val_results, np.mean(cross_val_results))\n",
    "    local_results[num_pairs] = np.mean(cross_val_results)\n",
    "print(local_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training pairs 1\n",
      "statistic=8.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=6.000, p-value=0.035\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=6.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 1,1,1,7,0.1,0.1,0.1,True,True,False V 1,1,1,7,0.1,0.1,0.1,True,False,False [0.814, 0.597, 8.0, 5.765519304519332e-12, True, True, True]\n",
      "2 ) 1,1,1,7,0.1,0.1,0.1,True,True,False V 1,1,1,7,0.1,0.1,0.1,True,False,True [0.814, 0.857, 6.0, 0.03468966484069824, True, False, False]\n",
      "3 ) 1,1,1,7,0.1,0.1,0.1,True,False,False V 1,1,1,7,0.1,0.1,0.1,True,False,True [0.597, 0.857, 6.0, 9.995152261577009e-16, True, True, True]\n",
      "******************************\n",
      "num training pairs 2\n",
      "statistic=4.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=1.000, p-value=0.012\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=1.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 2,1,1,6,1.0,0.1,0.8,True,True,False V 2,1,1,6,1.0,0.1,0.8,True,False,False [0.849, 0.717, 4.0, 5.65314621780999e-08, True, True, True]\n",
      "2 ) 2,1,1,6,1.0,0.1,0.8,True,True,False V 2,1,1,6,1.0,0.1,0.8,True,False,True [0.849, 0.884, 1.0, 0.01171875, True, False, False]\n",
      "3 ) 2,1,1,6,1.0,0.1,0.8,True,False,False V 2,1,1,6,1.0,0.1,0.8,True,False,True [0.717, 0.884, 1.0, 2.6147972675971687e-12, True, True, True]\n",
      "******************************\n",
      "num training pairs 3\n",
      "statistic=3.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=1.000, p-value=0.012\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 3,1,1,8,0.95,0.1,0.65,True,True,False V 3,1,1,8,0.95,0.1,0.65,True,False,False [0.853, 0.729, 3.0, 6.677873898297548e-08, True, True, True]\n",
      "2 ) 3,1,1,8,0.95,0.1,0.65,True,True,False V 3,1,1,8,0.95,0.1,0.65,True,False,True [0.853, 0.888, 1.0, 0.01171875, True, False, False]\n",
      "3 ) 3,1,1,8,0.95,0.1,0.65,True,False,False V 3,1,1,8,0.95,0.1,0.65,True,False,True [0.729, 0.888, 0.0, 9.094947017729282e-13, True, True, True]\n",
      "******************************\n",
      "num training pairs 4\n",
      "statistic=5.000, p-value=1.000\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=3.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 4,1,1,5,0.75,0.1,0.4,True,True,False V 4,1,1,5,0.75,0.1,0.4,True,False,False [0.826, 0.829, 5.0, 1.0, False, False, False]\n",
      "2 ) 4,1,1,5,0.75,0.1,0.4,True,True,False V 4,1,1,5,0.75,0.1,0.4,True,False,True [0.826, 0.899, 0.0, 3.814697265625e-06, True, True, True]\n",
      "3 ) 4,1,1,5,0.75,0.1,0.4,True,False,False V 4,1,1,5,0.75,0.1,0.4,True,False,True [0.829, 0.899, 3.0, 0.0002771615982055664, True, True, True]\n",
      "******************************\n",
      "num training pairs 5\n",
      "statistic=5.000, p-value=0.581\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=1.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 5,1,1,7,0.7,0.1,0.3,True,True,False V 5,1,1,7,0.7,0.1,0.3,True,False,False [0.841, 0.829, 5.0, 0.5810546875, False, False, False]\n",
      "2 ) 5,1,1,7,0.7,0.1,0.3,True,True,False V 5,1,1,7,0.7,0.1,0.3,True,False,True [0.841, 0.903, 0.0, 3.0517578125e-05, True, True, True]\n",
      "3 ) 5,1,1,7,0.7,0.1,0.3,True,False,False V 5,1,1,7,0.7,0.1,0.3,True,False,True [0.829, 0.903, 1.0, 2.09808349609375e-05, True, True, True]\n",
      "******************************\n",
      "num training pairs 6\n",
      "statistic=3.000, p-value=0.727\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=1.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=1.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 6,1,1,6,0.9,0.1,0.1,True,True,False V 6,1,1,6,0.9,0.1,0.1,True,False,False [0.841, 0.833, 3.0, 0.7265625, False, False, False]\n",
      "2 ) 6,1,1,6,0.9,0.1,0.1,True,True,False V 6,1,1,6,0.9,0.1,0.1,True,False,True [0.841, 0.907, 1.0, 7.62939453125e-05, True, True, True]\n",
      "3 ) 6,1,1,6,0.9,0.1,0.1,True,False,False V 6,1,1,6,0.9,0.1,0.1,True,False,True [0.833, 0.907, 1.0, 2.09808349609375e-05, True, True, True]\n",
      "******************************\n",
      "num training pairs 7\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "statistic=0.000, p-value=0.062\n",
      "Same proportions of errors (fail to reject H0)\n",
      "statistic=0.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "1 ) 7,1,1,6,0.9,0.1,0.1,True,True,False V 7,1,1,6,0.9,0.1,0.1,True,False,False [0.88, 0.833, 0.0, 0.00048828125, True, True, True]\n",
      "2 ) 7,1,1,6,0.9,0.1,0.1,True,True,False V 7,1,1,6,0.9,0.1,0.1,True,False,True [0.88, 0.899, 0.0, 0.0625, False, False, False]\n",
      "3 ) 7,1,1,6,0.9,0.1,0.1,True,False,False V 7,1,1,6,0.9,0.1,0.1,True,False,True [0.833, 0.899, 0.0, 1.52587890625e-05, True, True, True]\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for num_training_pairs in range(1,8):\n",
    "    print(\"num training pairs\", num_training_pairs)\n",
    "    reduced_comparison_results_test = {}\n",
    "    for sys1 in reduced_results_test.keys():\n",
    "        #print(sys1)\n",
    "        if not int(sys1.split(\",\")[0]) == num_training_pairs:\n",
    "            continue\n",
    "        #print(\"getting 1\")\n",
    "        for sys2 in reduced_results_test.keys():\n",
    "            if not int(sys2.split(\",\")[0]) == num_training_pairs:\n",
    "                continue\n",
    "            #print(\"getting 2\")\n",
    "            if sys1 == sys2:\n",
    "                continue\n",
    "            #print(\"getting 3\")\n",
    "            if sys1 + \" V \" + sys2 in reduced_comparison_results_test.keys() or sys2 + \" V \" + sys1 in reduced_comparison_results_test.keys():\n",
    "                continue\n",
    "            #print(\"getting 4\")\n",
    "            cl1_preds = reduced_results_test[sys1][2]\n",
    "            cl2_preds = reduced_results_test[sys2][2]\n",
    "            sig = calculate_mcnemar_test(cl1_preds, cl2_preds, all_y, alpha=0.05, exact=True)\n",
    "            reduced_comparison_results_test[sys1 + \" V \" + sys2] = [round(reduced_results_test[sys1][1],3), round(reduced_results_test[sys2][1],3)] + list(sig)\n",
    "\n",
    "    count = 1\n",
    "    for k, v in reduced_comparison_results_test.items():\n",
    "        print(count, \")\", k, v)\n",
    "        count+=1\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
