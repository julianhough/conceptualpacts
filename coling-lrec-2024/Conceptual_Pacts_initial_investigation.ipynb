{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra/initial work for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google_trans_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python/\")\n",
    "import pentoref.IO as IO\n",
    "import sqlite3 as sqlite\n",
    "#from google_trans_new import google_translator\n",
    "from deep_translator import GoogleTranslator\n",
    "import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#l = LabelEncoder()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "#nltk.download('punkt')  # if using stemming in German\n",
    "\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy import optimize\n",
    "from math import log\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pentoref.IOutils import clean_utt\n",
    "from machine_learning_utils import calculate_mcnemar_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_reparanda(utt):\n",
    "    \"removes the content between ( and + though leaves those intact\"\n",
    "    repair_depth = 0\n",
    "    cleaned_utt = \"\"\n",
    "    for c in utt:\n",
    "        if c == \"+\":\n",
    "            repair_depth-=1\n",
    "        elif c == \"(\":\n",
    "            repair_depth+=1\n",
    "        elif repair_depth>0:\n",
    "            continue\n",
    "        cleaned_utt+=c\n",
    "    assert repair_depth==0, \"repair depth not 0:\" + utt\n",
    "    return cleaned_utt\n",
    "clean_utt(remove_reparanda(\"(( hello + hello) + {F um } hello)\"))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create databases if required\n",
    "if False:   # make True if you need to create the databases from the derived data\n",
    "    for corpus_name in [\"TAKE\", \"TAKECV\", \"PENTOCV\"]:\n",
    "        data_dir = \"../../../pentoref/{0}_PENTOREF\".format(corpus_name)\n",
    "        dfwords, dfutts, dfrefs, dfscenes, dfactions = IO.convert_subcorpus_raw_data_to_dataframes(data_dir)\n",
    "        IO.write_corpus_to_database(\"{0}.db\".format(corpus_name),\n",
    "                                    corpus_name, dfwords, dfutts, dfrefs, dfscenes, dfactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utts ['gameID', 'uttID', 'starttime', 'endtime', 'utt', 'utt_clean', 'role', 'speaker']\n",
      "words ['gameID', 'uttID', 'position', 'word', 'lemma', 'tag']\n",
      "refs ['refID', 'gameID', 'uttID', 'text', 'id', 'piece', 'location']\n",
      "scenes ['timestampID', 'gameID', 'pieceID', 'position_global', 'position_x', 'position_y', 'shape', 'shape_distribution', 'shape_orientation', 'shape_skewness_horizontal', 'shape_skewness_vertical', 'shape_edges', 'colour', 'colour_distribution', 'colour_hsv', 'colour_rgb']\n",
      "actions ['gameID', 'starttime', 'endtime', 'hand', 'action', 'piece']\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "CORPUS = \"PENTOCV\"\n",
    "db = sqlite.connect(\"{0}.db\".format(CORPUS))\n",
    "cursor = db.cursor()\n",
    "# get the table column header names\n",
    "print(\"utts\", [x[1] for x in cursor.execute(\"PRAGMA table_info(utts)\")])\n",
    "print(\"words\", [x[1] for x in cursor.execute(\"PRAGMA table_info(words)\")])\n",
    "print(\"refs\", [x[1] for x in cursor.execute(\"PRAGMA table_info(refs)\")])\n",
    "print(\"scenes\", [x[1] for x in cursor.execute(\"PRAGMA table_info(scenes)\")])\n",
    "print(\"actions\", [x[1] for x in cursor.execute(\"PRAGMA table_info(actions)\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get utterances from certain time periods in each experiment or for certain episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    translator = GoogleTranslator(source='de', target='en')\n",
    "\n",
    "    for row in db.execute(\"SELECT gameID, starttime, speaker, utt_clean, utt FROM utts\" + \\\n",
    "                       # \" WHERE starttime >= 200 AND starttime <= 300\" + \\\n",
    "                         ' WHERE gameID = \"r1_1_1_b\"' + \\\n",
    "                        \" ORDER BY gameID, starttime\"):\n",
    "        print(row)\n",
    "        line = row[3]\n",
    "        print(line)\n",
    "        if not line:\n",
    "            continue\n",
    "        translate_text = translator.translate(line,lang_src='de',lang_tgt='en') \n",
    "        print(translate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just focus on single pieces, not references to multiple pieces\n",
    "good_pieces = [\"X\", \"Y\", \"P\", \"N\", \"U\", \"F\", \"Z\", \"L\", \"T\", \"I\", \"W\", \"V\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing with (der + (das + (das + das ))) grüne $m) at r4_1_3_b 10823\n",
      "replacing with ((der {f äh:m:} + der) + der) winkel at r4_1_3_b 10880\n",
      "replacing with das (br- + blaue) Lange at r7_1_1_b 1215\n",
      "replacing with das: ({f äh} ja . + ) andere blaue $z at r8_1_3_s 2454\n",
      "replacing with den (ist das rosa oben + ) block at r8_1_5_b 4428\n",
      "replacing with (<p=\"dieses\">die-</p> (genau  +) + das) element at r8_1_8_b 4919\n",
      "replacing with das $t (dieser blaue <p=\"senkrecht\">senk-</p>+) at r8_2_17_b 3696\n",
      "dict_keys(['r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8'])\n",
      "[('gelben stein', 'r3_B', 8886, 129.257, 'U'), ('das rote kreuz', 'r3_B', 8893, 141.68, 'X'), ('daran', 'r3_B', 8897, 148.215, 'U'), ('der orange stein', 'r3_B', 8897, 148.215, 'L'), ('das l', 'r3_A', 7900, 151.582, 'L'), ('daran', 'r3_B', 8902, 157.211, 'X'), ('das grüne t', 'r3_B', 8902, 157.211, 'T'), ('der blaue winkel', 'r3_B', 8931, 219.17, 'V'), ('das', 'r3_B', 8936, 226.213, 'V'), ('daran', 'r3_B', 8940, 235.547, 'V'), ('der gelbe stein', 'r3_B', 8940, 235.547, 'U'), ('er', 'r3_B', 8943, 244.98, 'U'), ('der graue stein', 'r3_B', 8946, 248.207, 'F'), ('den gelben', 'r3_B', 8953, 265.246, 'U'), ('der grüne stein', 'r3_B', 8959, 272.863, 'W'), ('den gelben', 'r3_B', 8967, 284.31, 'U'), ('den grauen', 'r3_B', 8967, 284.31, 'F'), ('das grüne t', 'r3_B', 8970, 287.759, 'T'), ('das grüne t', 'r3_B', 8977, 300.292, 'T'), ('der lila stein', 'r3_B', 8977, 300.292, 'N')]\n"
     ]
    }
   ],
   "source": [
    "references_per_pair = {}  # all data will be stored here with keys =pairnum\n",
    "for row in db.execute(\"SELECT id, gameID, text, uttID FROM refs\" + \\\n",
    "#for row in db.execute(\"SELECT shape, colour, orientation, gridPosition, gameID, pieceID FROM scenes\" + \\\n",
    "                     \" ORDER by gameID\"):\n",
    "    if False: \n",
    "        # TAKE\n",
    "        isTarget = db.execute('SELECT refID FROM refs WHERE gameID =\"' + row[4] + '\" AND pieceID =\"' + row[5] + '\"')\n",
    "        target = False \n",
    "        for r1 in isTarget:\n",
    "            target = True\n",
    "        if not target:\n",
    "            continue\n",
    "\n",
    "    #TAKE\n",
    "    #shape, colour, orientation, gridPosition, gameID, pieceID = row\n",
    "    #piece = colour  #+ \"_\" + shape #shape + \"_\" + colour\n",
    "    \n",
    "    #PENTOCV\n",
    "    piece, gameID, text, uttID = row\n",
    "    \n",
    "    # some manual corrections of disfluencies\n",
    "    error_found = False\n",
    "    if \"(der + (das + das + das) grüne $m)\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"(der + (das + (das + das ))) grüne $m)\"\n",
    "    elif \"(der {f äh:m:} + der) + der) winkel\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"((der {f äh:m:} + der) + der) winkel\"\n",
    "    elif \"das {br- + blaue} lange\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"das (br- + blaue) Lange\"\n",
    "    elif \"das: ({f äh} ja .) andere blaue $z\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"das: ({f äh} ja . + ) andere blaue $z\"\n",
    "    elif \"den (ist das rosa oben) block\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"den (ist das rosa oben + ) block\"\n",
    "    elif \"\"\"(<p=\"dieses\">die-</p> (genau) + das) element\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"(<p=\"dieses\">die-</p> (genau  +) + das) element\"\"\"\n",
    "    elif \"\"\"das $t (dieser blaue <p=\"senkrecht\">senk-</p>\"\"\" in text.lower():\n",
    "        error_found = True\n",
    "        text = \"\"\"das $t (dieser blaue <p=\"senkrecht\">senk-</p>+)\"\"\"\n",
    "        \n",
    "    if error_found:\n",
    "        print(\"replacing with\", text, \"at\", gameID, uttID)\n",
    "        \n",
    "        \n",
    "    \n",
    "    clean_text = clean_utt(remove_reparanda(text.lower()))\n",
    "    assert clean_text!=\"\", count\n",
    "    \n",
    "    \n",
    "    pair_num = gameID.split(\"_\")[0]\n",
    "    # get speaker\n",
    "    speaker = list(db.execute('SELECT speaker FROM utts WHERE uttID =' + str(uttID)))[0][0]\n",
    "    speaker = pair_num + \"_\" + speaker\n",
    "    \n",
    "    end_time =  float(list(db.execute('SELECT endtime FROM utts WHERE uttID =' + str(uttID)))[0][0])\n",
    "    \n",
    "    #if not pair_num == PAIR_NUM:\n",
    "    #    continue\n",
    "    \n",
    "        \n",
    "    if piece not in good_pieces:\n",
    "        continue\n",
    "        \n",
    "    if not references_per_pair.get(pair_num):\n",
    "        references_per_pair[pair_num] = []\n",
    "    \n",
    "    \n",
    "    if \"_s\" in gameID:\n",
    "        continue # just get the build phases for now due to inconsistent labelling\n",
    "\n",
    "    references_per_pair[pair_num].append((clean_text, speaker, uttID, end_time, piece))\n",
    "\n",
    "\n",
    "    # sort by end time\n",
    "for pair_num in references_per_pair.keys():\n",
    "    ref_list = references_per_pair[pair_num]\n",
    "    ref_list = sorted(ref_list, key=lambda x:x[3])\n",
    "    references_per_pair[pair_num] = ref_list\n",
    "\n",
    "print(references_per_pair.keys())\n",
    "print(references_per_pair['r3'][0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create language models and language model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hyp 1: the referring expression is no less predictable given the other mentions for a given piece at a given time point\n",
    "# regardless of order (so like a language model, all mentions are equally likely and there's no correlation with time)\n",
    "# of how p(ref|previous) =< p(ref|all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUMPTION: only dealing with full names, not anaphors # TODO for this paper may actually leave them in\n",
    "if True: # False TODO for this paper may actually leave them in\n",
    "    anaphors = [\"es\", \"das\", 'er', \"da\", \"der\", \"ihn\", \"den\", \"sie\", \"die\", \"damit\", \"daran\", \"dem\"]\n",
    "    for pair_num in references_per_pair.keys():\n",
    "        refs = references_per_pair[pair_num]\n",
    "        references_per_pair[pair_num] = list(filter(lambda x:x[0] not in anaphors, refs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL REFERENCES 1899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('F', 134),\n",
       " ('I', 139),\n",
       " ('L', 161),\n",
       " ('N', 197),\n",
       " ('P', 147),\n",
       " ('T', 191),\n",
       " ('U', 144),\n",
       " ('V', 162),\n",
       " ('W', 170),\n",
       " ('X', 177),\n",
       " ('Y', 162),\n",
       " ('Z', 115)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats for whole corpus and pieces\n",
    "piece_counter = Counter()\n",
    "for pair_num in references_per_pair.keys():\n",
    "    list_refs = [ref[4] for ref in references_per_pair[pair_num]]\n",
    "    piece_counter.update(list_refs) \n",
    "print(\"TOTAL REFERENCES\", piece_counter.total())\n",
    "sorted(piece_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('der blaue stein', 'r3_B', 8998, 338.218, 'Z')\n",
      "('der hellblaue stein', 'r3_B', 9101, 597.635, 'Z')\n",
      "('der andere', 'r3_B', 9103, 599.63, 'Z')\n",
      "('der blaue stein', 'r3_B', 9295, 976.864, 'Z')\n",
      "('der hellblaue stein', 'r3_B', 9373, 1136.494, 'Z')\n",
      "('der andere blaue', 'r3_A', 8155, 1431.262, 'Z')\n",
      "('s', 'r3_A', 8155, 1431.262, 'Z')\n",
      "('z', 'r3_A', 8155, 1431.262, 'Z')\n",
      "('das z', 'r3_A', 8165, 1450.016, 'Z')\n",
      "('der z-stein', 'r3_A', 8228, 1561.559, 'Z')\n",
      "('dieses s', 'r3_A', 8357, 1745.013, 'Z')\n",
      "('z-steinchen', 'r3_A', 8357, 1745.013, 'Z')\n",
      "('blauen', 'r3_A', 8395, 1792.021, 'Z')\n",
      "('das z', 'r3_A', 8636, 2157.882, 'Z')\n",
      "('der z-stein', 'r3_A', 8700, 2288.859, 'Z')\n",
      "('diesen z-stein', 'r3_A', 8790, 2443.581, 'Z')\n",
      "('s-stein', 'r3_A', 8790, 2443.581, 'Z')\n",
      "('ein z', 'r3_A', 8793, 2445.882, 'Z')\n",
      "('dem z', 'r3_A', 8804, 2463.788, 'Z')\n",
      "('dieses z', 'r3_A', 8839, 2519.642, 'Z')\n",
      "('dem z', 'r3_A', 8841, 2522.952, 'Z')\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at how one pair talks about a piece over time\n",
    "for ref in filter(lambda x:x[4] == 'Z', references_per_pair['r3']):\n",
    "    print(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_language_model():\n",
    "    \n",
    "    def __init__(self, order, smoothing_k=0.0001):\n",
    "        self.order = order\n",
    "        self.ngrams = {}\n",
    "        self.num_training_sents = 0\n",
    "        self.smoothing_k = smoothing_k\n",
    "        for o in range(1, order+1):\n",
    "            self.ngrams[o] = {}\n",
    "        self.unigram_denom = 0\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    \n",
    "    def add_ngram_to_model(self, ngram, order):\n",
    "        ngram_text = \"_\".join(ngram)\n",
    "        if not self.ngrams.get(order).get(ngram_text):\n",
    "            self.ngrams[order][ngram_text] = 0\n",
    "        self.ngrams[order][ngram_text]+=1\n",
    "\n",
    "        \n",
    "    def minus_ngram_from_model(self, ngram, order):\n",
    "        ngram_text = \"_\".join(ngram)\n",
    "        if not self.ngrams.get(order).get(ngram_text):\n",
    "            self.ngrams[order][ngram_text] = 0\n",
    "            \n",
    "        self.ngrams[order][ngram_text]-=1\n",
    "      \n",
    "    \n",
    "    def add_counts_from_other_model(self, other_lm):\n",
    "        assert(other_lm.order == self.order)\n",
    "        for n in range(1, self.order+1):\n",
    "            #print(\"ngrams before\", self.ngrams)\n",
    "            combined_keys = list(self.ngrams[n].keys() | other_lm.ngrams[n].keys())\n",
    "            #print(\"other after\", self.ngrams)\n",
    "            for key in combined_keys:\n",
    "                \n",
    "                count_self = self.ngrams[n].get(key)\n",
    "                if not count_self:\n",
    "                    count_self = 0\n",
    "                count_other = other_lm.ngrams[n].get(key)\n",
    "                if not count_other:\n",
    "                    count_other = 0\n",
    "                #print(key, count_self, count_other)\n",
    "                self.ngrams[n][key] = count_self + count_other\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(\"update\")\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def train(self, sents):\n",
    "        for sent in sents:\n",
    "            padded = [\"<s>\"] * (self.order -1) + sent + [\"</s>\"]\n",
    "            for i in range(self.order-1, len(padded)):\n",
    "                for n in range(1, self.order+1):\n",
    "                    target = padded[i]\n",
    "                    context = padded[i-(n-1):i]\n",
    "                    self.add_ngram_to_model(context + [target], n)\n",
    "                    if n> 1:\n",
    "                        self.add_ngram_to_model(context, n-1)\n",
    "                    \n",
    "            self.num_training_sents += 1\n",
    "        #print(self.ngrams)\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def de_train(self, sents):\n",
    "        # take away these counts\n",
    "        for sent in sents:\n",
    "            padded = [\"<s>\"] * (self.order -1) + sent + [\"</s>\"]\n",
    "            for i in range(self.order-1, len(padded)):\n",
    "                for n in range(1, self.order+1):\n",
    "                    target = padded[i]\n",
    "                    context = padded[i-(n-1):i]\n",
    "                    self.minus_ngram_from_model(context + [target], n)\n",
    "        #print(self.ngrams)\n",
    "        self.unigram_denom = sum(self.ngrams[1].values())\n",
    "        self.vocab_size = len(self.ngrams[1])\n",
    "        #print(self.unigram_denom, \"unigram counts\")\n",
    "        #print(self.vocab_size, \"vocab size\")\n",
    "        \n",
    "    def prob_lidstone(self, ngram, order):\n",
    "        \"\"\"Add-k smoothing using the discount parameter for this model.\"\"\"\n",
    "        ngram_text =  \"_\".join(ngram)\n",
    "        ngram_count = self.ngrams[order].get(ngram_text)\n",
    "        if not ngram_count:\n",
    "            ngram_count = 0\n",
    "        num = ngram_count + self.smoothing_k\n",
    "        if order == 1:\n",
    "            denom = self.unigram_denom + (self.smoothing_k * self.vocab_size)\n",
    "            if self.unigram_denom == 0:\n",
    "                #print(\"warning no training, returning k/40/10\")\n",
    "                denom = 150 + (self.smoothing_k * 15)\n",
    "            #print(denom)\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_text = \"_\".join(context)\n",
    "            #print(context_text)\n",
    "            context_count = self.ngrams[order-1].get(context_text)\n",
    "            if not context_count:\n",
    "                context_count = 0\n",
    "            #print(context_count)\n",
    "            denom = context_count + (self.smoothing_k * self.vocab_size)\n",
    "            if self.unigram_denom == 0 or context_count == 0:\n",
    "                #print(\"warning no training, returning k/40/10\")\n",
    "                denom = 40 + (self.smoothing_k * 10)\n",
    "            #print(denom)\n",
    "        assert num/denom <=1 and num/denom >0, str(num) + \" \" + str(denom) + \" \" + str(num/denom) + \" \" + str(ngram) + \" n=\" + str(order) + \" k=\" + str(self.smoothing_k)\n",
    "        return num/denom\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'I': 4, '<s>': 2, 'like': 4, 'bill': 2, '</s>': 2, 'mary': 2}, 2: {'<s>_I': 4, '<s>_<s>': 2, 'I_like': 4, 'like_bill': 2, 'bill_</s>': 1, 'like_mary': 2, 'mary_</s>': 1}, 3: {'<s>_<s>_I': 2, '<s>_I_like': 2, 'I_like_bill': 1, 'like_bill_</s>': 1, 'I_like_mary': 1, 'like_mary_</s>': 1}}\n",
      "0.23913043478260873\n",
      "{1: {'I': 4, '<s>': 2, 'like': 4, 'bill': 2, 'today': 2, '</s>': 2, 'mary': 2}, 2: {'<s>_I': 4, '<s>_<s>': 2, 'I_like': 4, 'like_bill': 2, 'bill_today': 2, 'today_</s>': 1, 'like_mary': 2, 'mary_</s>': 1}, 3: {'<s>_<s>_I': 2, '<s>_I_like': 2, 'I_like_bill': 1, 'like_bill_today': 1, 'bill_today_</s>': 1, 'I_like_mary': 1, 'like_mary_</s>': 1}}\n",
      "{1: {'I': 8, '<s>': 4, 'like': 8, 'bill': 4, '</s>': 4, 'mary': 4, 'today': 2}, 2: {'<s>_I': 8, '<s>_<s>': 4, 'I_like': 8, 'like_bill': 4, 'bill_</s>': 1, 'like_mary': 4, 'mary_</s>': 2, 'bill_today': 2, 'today_</s>': 1}, 3: {'<s>_<s>_I': 4, '<s>_I_like': 4, 'I_like_bill': 2, 'like_bill_</s>': 1, 'I_like_mary': 2, 'like_mary_</s>': 2, 'bill_today_</s>': 1, 'like_bill_today': 1}}\n"
     ]
    }
   ],
   "source": [
    "m = mini_language_model(3, smoothing_k=0.1)\n",
    "m.train([[\"I\", \"like\", \"bill\"], [\"I\", \"like\", \"mary\"]])\n",
    "print(m.ngrams)\n",
    "print(m.prob_lidstone([\"I\", \"like\", \"mary\"], 3))\n",
    "\n",
    "m2 = mini_language_model(3)\n",
    "m2.train([[\"I\", \"like\", \"bill\", 'today'], [\"I\", \"like\", \"mary\"]])\n",
    "print(m2.ngrams)\n",
    "m.add_counts_from_other_model(m2)\n",
    "print(m.ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs_in_timerange_for_piece(pair_num, piece, time_uttend_start, time_uttend_end, references_per_pair):\n",
    "    \"\"\"Returns a list of the refs from a certain time start to a time end in order for a given piece.\n",
    "    NOTE FOR NOW refs in the same utterance are either all included or all excluded from this method\"\"\"\n",
    "    if not references_per_pair.get(pair_num):\n",
    "        return []\n",
    "    return list(filter(lambda x:(x[3]>=time_uttend_start and x[3]<time_uttend_end) and x[4]==piece,\n",
    "                       references_per_pair[pair_num]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die eins', 'r2_B', 13319, 282.291, 'L'),\n",
       " ('orange', 'r2_A', 12552, 294.243, 'L'),\n",
       " ('dem orangen', 'r2_B', 13396, 410.399, 'L'),\n",
       " ('der eins', 'r2_B', 13396, 410.399, 'L'),\n",
       " ('diesem l', 'r2_B', 13417, 441.653, 'L'),\n",
       " ('die eins die orange', 'r2_B', 13570, 678.237, 'L')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_refs_in_timerange_for_piece('r2', 'L', 0, 687.481, references_per_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_probs_of_two_lms(ngram, lm1, lm2, weight_lm1=0.5):\n",
    "    # combine the probabilities together from two language models\n",
    "    # challenge is they have potentially different vocab lengths, which affects smoothing\n",
    "    weight_lm2 = 1 - weight_lm1\n",
    "    if False: # Attempt to normalize to a proper probability \n",
    "        # Difficulty is the smoothing doesn't work the same for both models normally\n",
    "        order = len(ngram)  # TODO should we have this as a param?\n",
    "        total_unnormalized_distribution = {}\n",
    "        all_ngrams = list(lm1.ngrams[order].keys() | lm2.ngrams[order].keys() | set([\"_\".join(ngram)]))\n",
    "        #print(len(all_ngrams), \"combined ngrams\")\n",
    "        # First get whole distribution over joint vocab\n",
    "        total_counts = 0\n",
    "        for a_ngram_text in all_ngrams:\n",
    "            a_ngram = a_ngram_text.split()\n",
    "            count_1 = lm1.ngrams[order].get(a_ngram_text)\n",
    "            if not count_1:\n",
    "                count_1 = 0\n",
    "            count_2 = lm2.ngrams[order].get(a_ngram_text)\n",
    "            if not count_2:\n",
    "                count_2 = 0\n",
    "            total_counts += (count_1 + count_2)\n",
    "            #rawprob = (weight_lm1 * count_1) + (weight_lm2 * count_2)\n",
    "            #total_unnormalized_distribution[a_ngram_text] = rawprob\n",
    "            prob_lm1 = lm1.prob_lidstone(a_ngram, order)\n",
    "            prob_lm2 = lm2.prob_lidstone(a_ngram, order)\n",
    "            rawprob = (weight_lm1 * prob_lm1) + (weight_lm2 * prob_lm2)\n",
    "            total_unnormalized_distribution[a_ngram_text] = rawprob\n",
    "        #print(total_counts, \"combined counts\")\n",
    "\n",
    "        # because laplace smoothed should always be non-0\n",
    "        prob = total_unnormalized_distribution[\"_\".join(ngram)]\n",
    "        denom = sum(total_unnormalized_distribution.values())\n",
    "    \n",
    "    prob_lm1 = lm1.prob_lidstone(a_ngram, lm1.order)\n",
    "    prob_lm2 = lm2.prob_lidstone(a_ngram, lm2.order)\n",
    "    joint_score = (weight_lm1 * prob_lm1) + (weight_lm2 * prob_lm2)\n",
    "    return joint_score\n",
    "                            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_for_all_pieces_up_to_current_time_point(order, smoothing_k, pair_num, time_uttend_end, references_per_pair, lm_other=None):\n",
    "    piece_lms = {}\n",
    "    for piece in references_per_pair.keys():\n",
    "        #print(piece)\n",
    "        data = get_refs_in_timerange_for_piece(pair_num, piece, 0, time_uttend_end, references_per_pair)\n",
    "        train_data = [x[0].split() for x in data]\n",
    "        piece_lms[piece] = mini_language_model(order, smoothing_k=smoothing_k)\n",
    "        piece_lms[piece].train(train_data)\n",
    "    return piece_lms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_lms = lms_for_all_pieces_up_to_current_time_point(1, 0.0001, 'r1', 823.29, references_per_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Is there an increasing similarity to the previous references as the dialogue progresses?\n",
    "# Test 2: does a reference have higher average similarity to other references in the same dialogue compared to those in different dialogyes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_of_each_reference_with_lms_trained_on_interaction_so_far(piece, pair_num, piece_references_per_pair, order=1, smoothing_k=0.0001, lm_other=None, lambda_lm1=None, cutoff_word_i=5):\n",
    "    \"\"\"Focusses on a particular pair number with possible training on the other pairs + the interaction so far for the conceptual pact model.\n",
    "    Sees how the evolution of the perplexity for the target piece's language model develops over time.\n",
    "    Sees how the evolution of the perplexity of the distractor pieces' language models develops over time on those refs.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_probs = []\n",
    "    list_of_ref_lengths = []\n",
    "\n",
    "    list_of_refs = piece_references_per_pair[piece][pair_num]\n",
    "    \n",
    "    for i in range(0, len(list_of_refs)):\n",
    "        target = list_of_refs[i][0]\n",
    "        #training = list_of_refs[:i] # + list_of_refs[i+1:]\n",
    "        #training = [ref[0] for ref in training]\n",
    "        time_uttend_end = list_of_refs[i][3]\n",
    "        \n",
    "        #print(len(training), \"training instances\")\n",
    "        #lm = mini_language_model(1, smoothing_k=0.0001)\n",
    "        #print(\"length of training data\", len(training))\n",
    "        #lm.train([ref.split() for ref in training])\n",
    "        \n",
    "        # always retrain entire model (not incremental currently but easy to make so as it's counts)\n",
    "        lms_all_pieces = lms_for_all_pieces_up_to_current_time_point(order, smoothing_k, pair_num, time_uttend_end, piece_references_per_pair)\n",
    "        \n",
    "        if i ==0:\n",
    "            assert len(lms_all_pieces[piece].ngrams[order])==0, str(lms_all_pieces[piece].ngrams.items()) +  piece +  pair_num\n",
    "        \n",
    "        \n",
    "        #lm.de_train([ref.split() for ref in list_of_refs[i+1:]])  # take away the future counts, though leave the vocab there\n",
    "        \n",
    "        TRAIN_BY_RECENCY = False\n",
    "        if TRAIN_BY_RECENCY:\n",
    "            # ADD EXTRA COUNTS IN PROPORTION TO RECENCY\n",
    "            top_count = 12\n",
    "            for j in range(i-1,-1,-1):\n",
    "                ref = list_of_refs[j]\n",
    "                for k in range(0,top_count):\n",
    "                    lm.train([ref.split()])\n",
    "                if top_count > 1:\n",
    "                    top_count-=4\n",
    "        # \n",
    "        \n",
    "        # option 1 add counts\n",
    "        #if lm_other is not None:\n",
    "        #    lm.add_counts_from_other_model(lm_other)\n",
    "        \n",
    "        ref = [\"<s>\"] * (order-1)  + target.split() + ['</s>']\n",
    "        \n",
    "        \n",
    "        # get probs for all classifiers\n",
    "        \n",
    "        probs_all_pieces = {piece:log(1) for piece in lms_all_pieces.keys()}\n",
    "        \n",
    "        \n",
    "        # go through the ref and get the ngrams\n",
    "        for j in range(order-1, len(ref)):\n",
    "            target = ref[j]\n",
    "            context = ref[j-(order-1):j]\n",
    "            ngram = context + [target]\n",
    "            for a_piece in lms_all_pieces.keys():\n",
    "                lm = lms_all_pieces[a_piece]\n",
    "                if lm_other is None:\n",
    "                    \n",
    "                    probs_all_pieces[a_piece] = probs_all_pieces[a_piece] + log(lm.prob_lidstone(ngram, order))\n",
    "                else:\n",
    "                    # option 1 add counts\n",
    "                    #prob_uni = prob_uni * lm.prob_lidstone([word], 1)\n",
    "                    #option 2: combination\n",
    "                    # NB FOR TRUST NEED TO KEEP TRACK OF HOW MANY INSTANCES OF EVERY PIECE, NOT JUST THIS ONE\n",
    "                    factor = 0 if cutoff_word_i <=1 else (1-lambda_lm1)/(cutoff_word_i-1)\n",
    "                    num_refs_f = lm.num_training_sents\n",
    "                    current_lambda_lm1 = max([lambda_lm1 + (factor * ((((cutoff_word_i-1)-num_refs_f)))), lambda_lm1])\n",
    "                    #current_lambda_lm1 = lambda_lm1  # switch to this for no influence of large language model\n",
    "                    \n",
    "                    probs_all_pieces[a_piece] = probs_all_pieces[a_piece] + log(combined_probs_of_two_lms(ngram, lm_other[pair_num][a_piece], lm, weight_lm1=current_lambda_lm1))\n",
    "                    #probs_all_pieces[a_piece] = probs_all_pieces[a_piece] + log(lm_other[pair_num][a_piece].prob_lidstone(ngram, order))\n",
    "                # update for this word and this piece\n",
    "               \n",
    "        #print(target, prob_ref_text)\n",
    "        list_of_probs.append(probs_all_pieces)\n",
    "        list_of_ref_lengths.append(tuple([len(ref)] +  list(list_of_refs[i])))\n",
    "    list_of_refs_with_probs = [({piece: -prob/l[0] for piece, prob in p_dict.items()}, l[1], l[2], l[3], l[4]) for p_dict, l in zip(list_of_probs, list_of_ref_lengths)]\n",
    "    piece_running_totals = {piece:0 for piece in piece_references_per_pair.keys()}\n",
    "    # add running average in too\n",
    "    for i in range(0,len(list_of_refs_with_probs)):\n",
    "        ref = list(list_of_refs_with_probs[i])\n",
    "        piece_running_average = {}\n",
    "        for piece, prob in ref[0].items():\n",
    "            new_total = prob + piece_running_totals[piece]\n",
    "            piece_running_average[piece] = new_total/(i+1)\n",
    "            piece_running_totals[piece] = new_total\n",
    "        list_of_refs_with_probs[i] = tuple([ref[0], piece_running_average, ref[1], ref[2], ref[3], ref[4]])\n",
    "            \n",
    "        \n",
    "    #print(list_of_refs_with_probs)\n",
    "    return list_of_refs_with_probs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one of the pairs as a test, rest is for LOSO-CV development\n",
    "test_pair_num = [\"r2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create reference resolution training + test data with features from the model\n",
    "* TODO setting - exclude and re-compute based on pieces 'in play', or assume the robot doesn't know - try both\n",
    "* Try 1) global model only without active learning, 2) global + local standard 3) global and local incrementally/dynamically updating, see the difference in ref res. Measures on accuracy + surprisal.\n",
    "* Naive bayes with individual language models for each piece + marginalization? OR model can work independently with any probabilistic classifier as a joint probability?\n",
    "* Get an optimal weighting between the local, incremental model and global models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get_predictions_for_test_pair(training_pair_folds, test_pair_data, params):\n",
    "    # Creates the appropriate language models for, and trains and tests classifiers for a given test pair\n",
    "    # create the speaker-wise fold language models (using all other pairs except key)\n",
    "    # will take a training_\n",
    "    # STEP 1. CREATE THE SPEAKER LMS\n",
    "    speaker_fold_lms = {}\n",
    "    # Train a separate global language model for each piece (?) in the training data\n",
    "    for piece in training_pair_folds.keys():\n",
    "\n",
    "        for pair_num in training_pair_folds[piece]:\n",
    "            if speaker_fold_lms.get(pair_num) is None:\n",
    "                speaker_fold_lms[pair_num] = {}\n",
    "            if speaker_fold_lms[pair_num].get(piece) is None:\n",
    "                speaker_fold_lms[pair_num][piece] = {}\n",
    "            # iterate over the other \n",
    "            training_refs = []\n",
    "            for other_pair_num in training_pair_folds[piece]:\n",
    "                if pair_num == other_pair_num:\n",
    "                    continue\n",
    "                training_refs.extend(training_pair_folds[piece][other_pair_num])  \n",
    "            training_fold = [r[0] for r in training_refs]\n",
    "            #print(\"length of training fold for lm2\", piece, pair_num, len(training_fold))\n",
    "            if len(training_fold) <2:\n",
    "                print(\"not enough data\", piece, pair_num)\n",
    "                continue\n",
    "            speaker_fold_lms[pair_num][piece] = mini_language_model(ORDER, smoothing_k=SMOOTHING_K)\n",
    "            speaker_fold_lms[pair_num][piece].train([sent.split() for sent in training_fold])\n",
    "\n",
    "\n",
    "    # STEP 2. Create the data for classification using features from the speaker fold lms        \n",
    "    piece_references_and_probabilities_per_pair = {}\n",
    "    total_p = 0\n",
    "\n",
    "    for piece in test_pair_data.keys():\n",
    "        print(\"PIECE\", piece)\n",
    "        total_perplexity = 0\n",
    "        for pair_num in training_pair_folds[piece]:\n",
    "\n",
    "            print(\"PAIR_NUM\", pair_num)\n",
    "            training_refs = []\n",
    "            for other_pair_num in training_pair_folds[piece]:\n",
    "                if pair_num == other_pair_num:\n",
    "                    continue\n",
    "                training_refs.extend(training_pair_folds[piece][other_pair_num])\n",
    "\n",
    "            # create global refs lms for combination with\n",
    "\n",
    "\n",
    "            refs = training_pair_folds[piece][pair_num]\n",
    "            #print(refs)\n",
    "            if len(refs) < 2 or (piece not in good_pieces):\n",
    "                print(\"insufficient data for piece for pair\", pair_num, \"for piece\", piece)\n",
    "                continue\n",
    "            print(\"length of training data\", len(refs)-1)\n",
    "            list_of_refs_with_probs = mean_cross_entropy_of_each_reference_with_lms_trained_on_interaction_so_far(piece,\n",
    "                                                                                                                pair_num,\n",
    "                                                                                                                training_pair_folds,\n",
    "                                                                                                                order=params[\"order\"],\n",
    "                                                                                                                smoothing_k=params[\"smoothing_k\"],\n",
    "                                                                                                                #lm_other=None)\n",
    "                                                                                                                lm_other=speaker_fold_lms,\n",
    "                                                                                                                lambda_lm1=params[\"local_lambda\"],\n",
    "                                                                                                                #lambda_lm1=1)\n",
    "                                                                                                                ##lambda_lm1=0,\n",
    "                                                                                                                cutoff_word_i=params[\"cut_off_local_i\"])\n",
    "            #print(similarity)\n",
    "            #total_perplexity+=similarity\n",
    "            if not piece_references_and_probabilities_per_pair.get(pair_num):\n",
    "                piece_references_and_probabilities_per_pair[pair_num] = {}\n",
    "            if not piece_references_and_probabilities_per_pair[pair_num].get(piece):\n",
    "                piece_references_and_probabilities_per_pair[pair_num][piece] = list_of_refs_with_probs\n",
    "\n",
    "\n",
    "\n",
    "            #break\n",
    "        print(\"total\", total_perplexity, \"*\" * 3)\n",
    "        #total_p+=total_perplexity\n",
    "    print(\"cross-entropy ALL PAIRS\", total_p)\n",
    "    \n",
    "    # STEP 3: CREATE THE CLASSIFIER BASED ON THE TRAINING DATA AND TEST ON TEST DATA\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = []\n",
    "    return preds\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': -0.7071067811865476, 'X': -0.7071067811865476, 'Y': 1.4142135623730951}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_zscore_dict_for_prob_dict(prob_dict):\n",
    "    probs  = [item[1] for item in sorted(prob_dict.items(), key=lambda x:x[0])] # alpha\n",
    "    zscores = list(stats.zscore(np.array(probs)))\n",
    "    def convert_nan(v):\n",
    "        if np.isnan(v):\n",
    "            return - 100\n",
    "        return v\n",
    "    return {k: convert_nan(v) for k, v in zip(sorted(prob_dict.keys()), zscores)}\n",
    "\n",
    "get_zscore_dict_for_prob_dict({\"K\": 0.33, \"Y\": 0.35, \"X\": 0.33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': 1, 'K': 2, 'Y': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rank_dict_for_prob_dict(prob_dict):\n",
    "    rank_dict = {}\n",
    "    rank = 0\n",
    "    for item in sorted(prob_dict.items(), key=lambda x:x[1]):\n",
    "        rank+=1\n",
    "        rank_dict[item[0]] = rank\n",
    "    return rank_dict\n",
    "get_rank_dict_for_prob_dict({\"K\": 0.34, \"Y\": 0.35, \"X\": 0.33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a_list):\n",
    "\n",
    "    averages = []\n",
    "    total = 0\n",
    "    for i, a in enumerate(a_list):\n",
    "        total+=a\n",
    "        averages.append(total/(i+1))\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Do a cross-fold model by speaker (and get word MI info for referent type)\n",
    "    results = []\n",
    "    piece_counter = Counter()\n",
    "    word_counter = Counter()\n",
    "    word_piece_counter = Counter()\n",
    "\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "\n",
    "    if False:\n",
    "    #for PAIR_NUM in piece_references_and_probabilities_per_pair['X'].keys():\n",
    "        print(PAIR_NUM)\n",
    "        if PAIR_NUM in exclude:\n",
    "            pass\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "\n",
    "        for piece in piece_references_and_probabilities_per_pair.keys():\n",
    "            #print(piece)\n",
    "            if not piece_references_and_probabilities_per_pair.get(piece):\n",
    "                print(\"not enough data\", piece)\n",
    "                continue\n",
    "            for pair_num in piece_references_and_probabilities_per_pair[piece]:\n",
    "                if pair_num in exclude:\n",
    "                    continue\n",
    "                #print(pair_num)\n",
    "                count_ref_so_far = 0\n",
    "                for ref_info in piece_references_and_probabilities_per_pair[piece][pair_num]:\n",
    "                    piece_lm_prob_local, piece_lm_moving_average, text, speaker, utt_id, end_time = ref_info\n",
    "                    piece_counter[piece] += 1\n",
    "                    if CORPUS in [\"TAKECV\", \"TAKE\"]:\n",
    "                        local_word_count = Counter()\n",
    "                        feature_vector = {}\n",
    "                        for f in db.execute('SELECT word from words WHERE gameID =\"' + str(gameID) + '\"'):\n",
    "                            #print(f)\n",
    "                            for word in clean_utt(f[0].lower()).split():\n",
    "                                local_word_count[word] += 1\n",
    "                        word_dem = len(local_word_count.values())\n",
    "                        for k, v in local_word_count.items():\n",
    "                            feature_vector[k] = local_word_count[k]/word_dem\n",
    "                            word_counter[k] += (v/word_dem)\n",
    "                            word_piece_counter[piece+\"__\"+k] += (v/word_dem)\n",
    "                    elif CORPUS == \"PENTOCV\":\n",
    "                        feature_vector = {}\n",
    "\n",
    "                        clean_utt = text.lower().split()\n",
    "                        word_dem = len(clean_utt)\n",
    "\n",
    "                        local_word_count = Counter()\n",
    "\n",
    "                        for word in clean_utt:\n",
    "                            word_counter[word] += (1/word_dem)   # for global\n",
    "                            word_piece_counter[piece+\"__\"+word] += (1/word_dem)  #for global\n",
    "                            local_word_count[word] += 1\n",
    "\n",
    "                        for k, v in local_word_count.items():\n",
    "                            feature_vector[k] = local_word_count[k]/word_dem\n",
    "\n",
    "                        LOCAL = True\n",
    "                        if LOCAL:\n",
    "                            #for k, v in piece_lm_prob_local.items():\n",
    "                            #    feature_vector['local_prob_' + k] = v\n",
    "\n",
    "                            zscore_dict = get_zscore_dict_for_prob_dict(piece_lm_prob_local)\n",
    "\n",
    "                            #for k, v in zscore_dict.items():\n",
    "                            #    feature_vector['local_zscore_prob_' + k] = v \n",
    "\n",
    "\n",
    "                            #for k, v in piece_lm_moving_average.items():\n",
    "                            #    feature_vector['local_prob_moving_' + k] = v\n",
    "\n",
    "\n",
    "                            zscore_dict_average = get_zscore_dict_for_prob_dict(piece_lm_moving_average)\n",
    "\n",
    "                            for k, v in zscore_dict_average.items():\n",
    "                                feature_vector['local_zscore_prob_moving_' + k] = v \n",
    "\n",
    "\n",
    "                            #rank_dict = get_rank_dict_for_prob_dict(piece_lm_prob_local)\n",
    "\n",
    "                            #for k, v in rank_dict.items():\n",
    "                            #    feature_vector['local_prob_rank' + k] = v \n",
    "\n",
    "                        count_ref_so_far+=1\n",
    "                        if count_ref_so_far > 7:\n",
    "                            pass\n",
    "                        #feature_vector[\"count_ref_so_far\"] = count_ref_so_far\n",
    "\n",
    "                        if pair_num == PAIR_NUM:\n",
    "                            test_data.append((feature_vector, piece))\n",
    "                        else:\n",
    "                            train_data.append((feature_vector, piece))\n",
    "        cl = trainClassifier(train_data)\n",
    "        print(train_data[0])\n",
    "        y_true = [x[1] for x in test_data]\n",
    "        all_y.extend(y_true)\n",
    "        y_pred = predictLabels(test_data, cl)\n",
    "        all_preds.extend(y_pred)\n",
    "        prf = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "        results.append(prf)\n",
    "        print(prf)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    print(np.mean([x[0] for x in results]))\n",
    "    print(np.mean([x[1] for x in results]))\n",
    "    print(np.mean([x[2] for x in results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_on_test_folds(k, pair_piece_lms_orig, folds, prob_model):\n",
    "    \"\"\"Returns the mean of the cross entropy on the folds using the appropriate lm.\n",
    "    Assumes folds will be in correct order w.r.t. time of reference.\"\"\"\n",
    "    # firstly, duplicate the language model to avoid any side effects\n",
    "    #print(k)\n",
    "    pair_piece_lms = copy.deepcopy(pair_piece_lms_orig)\n",
    "    # Set the k for all to be the same\n",
    "    for pair_num in pair_piece_lms.keys():\n",
    "        for piece in pair_piece_lms[pair_num].keys():\n",
    "            pair_piece_lms[pair_num][piece].smoothing_k = k[0]\n",
    "            \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "            # assume we know the correct ref piece\n",
    "            piece = ref_info[4]\n",
    "            lm = pair_piece_lms[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            prob = 0\n",
    "            for j in range(lm.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                prob += log(lm.prob_lidstone(ngram, lm.order))\n",
    "            s += prob\n",
    "            count +=1\n",
    "            if prob_model == 'self':\n",
    "                # self language model, starting empty for a piece\n",
    "                # then will train as it encounters each piece\n",
    "                #print(pair_piece_lms[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "                pair_piece_lms[pair_num][piece].train([ref_info[0].split()])\n",
    "    \n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "    #print(k, cross_entropies, np.mean(cross_entropies))\n",
    "    return np.mean(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r1', 'r2', 'r4', 'r5', 'r6', 'r7', 'r8'])\n",
      "100 [('das orange l', 'r1_B', 5887, 204.521, 'L'), ('das kreuz', 'r1_B', 5893, 219.022, 'X'), ('das grüne t', 'r1_B', 5901, 228.117, 'T'), ('das t', 'r1_B', 5903, 231.474, 'T'), ('das t', 'r1_B', 5907, 241.893, 'T'), ('das t', 'r1_B', 5907, 241.893, 'T'), ('das gelbe', 'r1_B', 5919, 258.434, 'U'), ('die brücke', 'r1_B', 5921, 260.36, 'U'), ('das pinkfarbene teil', 'r1_B', 5924, 266.23, 'P'), ('den blauen balken', 'r1_B', 5935, 304.366, 'I')]\n"
     ]
    }
   ],
   "source": [
    "TEST = 'r3'\n",
    "training_folds = {k:references_per_pair[k] for k in filter(lambda x:x!=TEST, references_per_pair.keys())}\n",
    "print(training_folds.keys())\n",
    "print(len(training_folds['r1']), training_folds['r1'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_for_lowest_entropy_on_fold_test(folds, pieces, n_range, k_val_range, prob_model='self'):\n",
    "    \"\"\"An x-fold process run multiple times to find the best params of n and k (smoothing constant)\n",
    "    for an ngram model\n",
    "    \"\"\"\n",
    "    results = {}  # store results for each model\n",
    "    for test_n in n_range:\n",
    "        pair_piece_lms = {}  # will produce all lms for each speaker for each piece with order test_n\n",
    "        for test_pair in folds.keys():\n",
    "            # test pair is the list of refs to get the entropy results on in both cases\n",
    "            # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "            # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "                  #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "            pair_piece_lms[test_pair] = {}  # key piece, value language model for that piece\n",
    "            for piece in pieces:\n",
    "                if prob_model == 'other':\n",
    "                    # train the model (smoothing can be changed as it doesn't affect training)\n",
    "\n",
    "                    training_refs = []\n",
    "                    \n",
    "                    for other_pair_num in folds.keys():\n",
    "                        if other_pair_num == test_pair:\n",
    "                            continue\n",
    "                        training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])])  \n",
    "                    training_fold = [r[0] for r in training_refs]\n",
    "                    #print(\"length of training fold for lm2\", piece, test_pair, len(training_fold))\n",
    "                    if len(training_fold) <1:\n",
    "                        print(\"not enough data\", piece, pair_num)\n",
    "                        continue\n",
    "                    pair_piece_lms[test_pair][piece] = mini_language_model(test_n, smoothing_k=0.0001)    \n",
    "                    pair_piece_lms[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "\n",
    "                elif prob_model == 'self':\n",
    "                    # just initialize lm for piece, no need to train\n",
    "                    pair_piece_lms[test_pair][piece] = mini_language_model(test_n, smoothing_k=0.0001)    \n",
    "             \n",
    "            \n",
    "        # run nelder mead to get best k for this n\n",
    "        best = optimize.minimize(\n",
    "                mean_cross_entropy_on_test_folds,\n",
    "                k_val_range[0],    # first argument that to be optimized (k)\n",
    "                args=(pair_piece_lms, folds, prob_model),  # other arguments to the function\n",
    "                method='Nelder-Mead', # use nelder mead to find minima\n",
    "                tol=0.0001, # to this degree of error\n",
    "                options={'disp': False},\n",
    "                bounds=[k_val_range]\n",
    "        )\n",
    "        best_k, best_ce = best.x[0], best.fun\n",
    "        print(\"best k\", best_k, best_ce)\n",
    "        results[test_n] = (best_k, best_ce)\n",
    "    \n",
    "    print(results)\n",
    "    n = min(results.items(), key=lambda x:x[1][1])[0]\n",
    "    k = results[n][0]\n",
    "               \n",
    "    return n, k, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k 1.0 6.73712157905053\n",
      "best k 1.0 6.253234714310423\n",
      "best k 1.0 6.821026665546368\n",
      "{1: (1.0, 6.73712157905053), 2: (1.0, 6.253234714310423), 3: (1.0, 6.821026665546368)}\n",
      "best k 1.0 8.189708894975334\n",
      "best k 0.618332169500001 7.2548672865266335\n",
      "best k 0.6607339615000012 7.620654164968658\n",
      "{1: (1.0, 8.189708894975334), 2: (0.618332169500001, 7.2548672865266335), 3: (0.6607339615000012, 7.620654164968658)}\n",
      "{'n_self': 2, 'k_self': 1.0, 'n_other': 2, 'k_other': 0.618332169500001}\n"
     ]
    }
   ],
   "source": [
    "# Firstly, in a self-supervised way, optimize the global + local language models in terms of n and smoothing param to get the lowest cross-entropy\n",
    "# We only go for the global best average across all pairs and pieces for the rest of the experiments\n",
    "TEST = 'r2'\n",
    "training_folds = {k:references_per_pair[k] for k in filter(lambda x:x!=TEST, references_per_pair.keys())}\n",
    "\n",
    "opt_params = {}\n",
    "n_range = [1,2,3]\n",
    "k_val_range = [0.00000001, 1]\n",
    "\n",
    "if True:\n",
    "    n_self, k_self, lm_opt_results_self = get_params_for_lowest_entropy_on_fold_test(training_folds, good_pieces,\n",
    "                                                                  n_range, k_val_range, prob_model='self')\n",
    "\n",
    "    opt_params[\"n_self\"] = n_self\n",
    "    opt_params[\"k_self\"] = k_self\n",
    "\n",
    "\n",
    "n_other, k_other, lm_opt_results_other = get_params_for_lowest_entropy_on_fold_test(training_folds, good_pieces,\n",
    "                                                              n_range, k_val_range, prob_model='other')\n",
    "opt_params[\"n_other\"] = n_other\n",
    "opt_params[\"k_other\"] = k_other\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_on_test_folds_joint_model(lambda_lm1, pair_piece_lms_self_orig, pair_piece_lms_other_orig,\n",
    "                                                 cutoff_word_global_lm, folds):\n",
    "    #print(\"lambda\", lambda_lm1, \"cutoff\", cutoff_word_global_lm)\n",
    "    pair_piece_lms_self = copy.deepcopy(pair_piece_lms_self_orig)\n",
    "    pair_piece_lms_other = copy.deepcopy(pair_piece_lms_other_orig)\n",
    "            \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "            # assume we know the correct ref piece\n",
    "            piece = ref_info[4]\n",
    "            \n",
    "            # get the weighted prob contribution from the self model\n",
    "            self_prob = 1\n",
    "            lm_self = pair_piece_lms_self[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm_self.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            \n",
    "            for j in range(lm_self.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm_self.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                self_prob = self_prob * lm_self.prob_lidstone(ngram, lm_self.order)\n",
    "                \n",
    "            # get the weighted prob contribution from the other model\n",
    "            other_prob = 1\n",
    "            lm_other = pair_piece_lms_other[pair_num][piece]\n",
    "            ref = [\"<s>\"] * (lm_other.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "            for j in range(lm_other.order-1, len(ref)):\n",
    "                target = ref[j]\n",
    "                context = ref[j-(lm_other.order-1):j]\n",
    "                ngram = context + [target]\n",
    "                other_prob = other_prob * lm_other.prob_lidstone(ngram, lm_other.order)\n",
    "            \n",
    "            # combine the probs with correct lambda weights\n",
    "            \n",
    "            # calculate the correct lambda weights based on the cut-off word\n",
    "            \n",
    "            lambda_global = 1- lambda_lm1\n",
    "            factor = 0 if cutoff_word_global_lm <=1 else (1-lambda_global)/(cutoff_word_global_lm-1)\n",
    "            num_refs_f = pair_piece_lms_self[pair_num][piece].num_training_sents  # + 1 # to include the current one?\n",
    "            current_lambda_lm1 = 1 - max([lambda_global + (factor * ((((cutoff_word_global_lm-1)-num_refs_f)))), lambda_global])\n",
    "\n",
    "            \n",
    "            prob = (current_lambda_lm1 * self_prob) + ((1-current_lambda_lm1) * other_prob)\n",
    "            #final_ref_info = tuple(list(ref_info) + [prob, len(ref_info[0].split()), num_refs_f])\n",
    "            #piece_references_and_probabilities_per_pair[pair_num].append(final_ref_info)\n",
    "            \n",
    "            s += log(prob)\n",
    "            count +=1\n",
    "            \n",
    "            # update self language model, which starts empty for a piece\n",
    "            # then will train as it encounters each piece\n",
    "            #print(pair_piece_lms_self[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "            pair_piece_lms_self[pair_num][piece].train([ref_info[0].split()])\n",
    "\n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "    #print(lambda_lm1, cross_entropies, np.mean(cross_entropies))\n",
    "    return np.mean(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_cutoff_lambda_xval(n_self, n_other, k_self, k_other, pieces, folds,\n",
    "                                       cutoff_range = [1,10],\n",
    "                                       lambda_range=[0,1]):\n",
    "    \"\"\"An x-fold process run multiple times to find the best params of lambda (weighting of local language model)\n",
    "    and cut-off word at which only the local model applies (with a gradual weighting)\n",
    "    \"\"\"\n",
    "    results = {}  # store results for each model\n",
    "    \n",
    "    for test_cutoff in range(cutoff_range[0], cutoff_range[1]+1):\n",
    "        pair_piece_lms_self = {}  # will produce all lms for each speaker for each piece with order test_cutoff\n",
    "        pair_piece_lms_other = {}  # will produce all lms for each speaker for each piece with order test_cutoff\n",
    "        for test_pair in folds.keys():\n",
    "            # test pair is the list of refs to get the entropy results on in both cases\n",
    "            # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "            # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "                  #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "            pair_piece_lms_other[test_pair] = {}  # key piece, value language model for that piece\n",
    "            pair_piece_lms_self[test_pair] = {}  # key piece, value language model for that piece\n",
    "            for piece in pieces:\n",
    "                for prob_model in ['other', 'self']:\n",
    "                    if prob_model == 'other':\n",
    "                        # train the model (smoothing can be changed as it doesn't affect training)\n",
    "\n",
    "                        training_refs = []\n",
    "\n",
    "                        for other_pair_num in folds.keys():\n",
    "                            if other_pair_num == test_pair:\n",
    "                                continue\n",
    "                            training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])])  \n",
    "                        training_fold = [r[0] for r in training_refs]\n",
    "                        #print(\"length of training fold for lm2\", piece, test_pair, len(training_fold))\n",
    "                        if len(training_fold) <1:\n",
    "                            print(\"not enough data\", piece, pair_num)\n",
    "                            continue\n",
    "                        pair_piece_lms_other[test_pair][piece] = mini_language_model(n_other, smoothing_k=k_other)    \n",
    "                        pair_piece_lms_other[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "\n",
    "                    elif prob_model == 'self':\n",
    "                        # just initialize lm for piece, no need to train\n",
    "                        pair_piece_lms_self[test_pair][piece] = mini_language_model(n_self, smoothing_k=k_self)    \n",
    "\n",
    "        # run nelder mead to get best lambda\n",
    "        best = optimize.minimize(\n",
    "                mean_cross_entropy_on_test_folds_joint_model,\n",
    "                lambda_range[0],    # first argument to be optimized (lambda)\n",
    "                args=(pair_piece_lms_self, pair_piece_lms_other, test_cutoff, folds),  # other arguments to the function\n",
    "                method='Nelder-Mead', # use nelder mead to find minima\n",
    "                tol=0.0001, # to this degree of error\n",
    "                options={'disp': False},\n",
    "                bounds=[lambda_range]\n",
    "        )\n",
    "        best_lambda, best_ce = best.x[0], best.fun\n",
    "        #print(\"best lambda\", best_lambda, best_ce)\n",
    "        results[test_cutoff] = (best_lambda, best_ce)\n",
    "    \n",
    "    print(results)\n",
    "    cutoff = min(results.items(), key=lambda x:x[1][1])[0]\n",
    "    lambda_lm1 = results[cutoff][0]\n",
    "               \n",
    "    return cutoff, lambda_lm1, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_piece_lms_from_folds(n, k, pieces, folds, target_folds=None, train=False):\n",
    "    \"\"\"Returns a piece based language model objects of n=n, k=k, using the training folds\"\"\"\n",
    "    pair_piece_lms_other = {}\n",
    "    for test_pair in folds.keys():\n",
    "        # test pair is the list of refs to get the entropy results on in both cases\n",
    "        # in 'other' model, referent_lm model is trained on refs on all other folds\n",
    "        # in 'self' model, referent_lm model is trained on test_pair incrementally dynamically\n",
    "              #i.e. retrained with the ref added to the counts after that ref is tested\n",
    "        if not test_pair in target_folds:\n",
    "            continue\n",
    "        pair_piece_lms_other[test_pair] = {}  # key piece, value language model for that piece\n",
    "        for piece in pieces:\n",
    "            # train the model (smoothing can be changed as it doesn't affect training)\n",
    "            pair_piece_lms_other[test_pair][piece] = mini_language_model(n, smoothing_k=k)  \n",
    "            if train:\n",
    "                training_refs = []\n",
    "                for other_pair_num in folds.keys():\n",
    "                    if other_pair_num == test_pair:\n",
    "                        continue\n",
    "                    training_refs.extend([ref for ref in filter(lambda x:x[4]==piece, folds[other_pair_num])])  \n",
    "                training_fold = [r[0] for r in training_refs]\n",
    "                #print(\"length of training fold for lm\", piece, test_pair, len(training_fold))\n",
    "                if len(training_fold) <1:\n",
    "                    print(\"not enough data\", piece, test_pair)\n",
    "                    continue\n",
    "                pair_piece_lms_other[test_pair][piece].train([sent.split() for sent in training_fold])\n",
    "    return pair_piece_lms_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifier_data_from_lms_and_params(pair_piece_lms_self_orig, pair_piece_lms_other_orig, pieces, lambda_lm1, cutoff_word_global_lm, folds, lexical=True):\n",
    "    # Creates the appropriate language models for, and trains and tests classifiers for a given test pair\n",
    "    # create the speaker-wise fold language models (using all other pairs except key)\n",
    "   \n",
    "    # Record all scores (probabilities) for each piece according to the language models in the folds \n",
    "    # (Which uses both other language models from observing other pairs and the local updating language model/pact model)\n",
    "    piece_references_and_probabilities_per_pair = {}\n",
    "\n",
    "    pair_piece_lms_self = copy.deepcopy(pair_piece_lms_self_orig)\n",
    "    pair_piece_lms_other = copy.deepcopy(pair_piece_lms_other_orig)\n",
    "\n",
    "  \n",
    "    \n",
    "    cross_entropies = []  # get the cross-entropy for each fold\n",
    "    for pair_num in folds.keys():\n",
    "        test_set = folds[pair_num]\n",
    "        piece_references_and_probabilities_per_pair[pair_num] = []\n",
    "        s = 0\n",
    "        count = 0\n",
    "        for i, ref_info in enumerate(test_set):\n",
    "   \n",
    "            target_piece = ref_info[4]\n",
    "            \n",
    "            piece_prob_dict = {} # gets the raw probs assigned to ref by all models\n",
    "            \n",
    "            for piece in pieces:\n",
    "                # get the weighted prob contribution from the self model\n",
    "                self_prob = 1\n",
    "                lm_self = pair_piece_lms_self[pair_num][piece]\n",
    "                ref = [\"<s>\"] * (lm_self.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "\n",
    "                for j in range(lm_self.order-1, len(ref)):\n",
    "                    target = ref[j]\n",
    "                    context = ref[j-(lm_self.order-1):j]\n",
    "                    ngram = context + [target]\n",
    "                    self_prob = self_prob * lm_self.prob_lidstone(ngram, lm_self.order)\n",
    "\n",
    "                # get the weighted prob contribution from the other model\n",
    "                other_prob = 1\n",
    "                lm_other = pair_piece_lms_other[pair_num][piece]\n",
    "                ref = [\"<s>\"] * (lm_other.order -1) + ref_info[0].split() + [\"</s>\"]\n",
    "                for j in range(lm_other.order-1, len(ref)):\n",
    "                    target = ref[j]\n",
    "                    context = ref[j-(lm_other.order-1):j]\n",
    "                    ngram = context + [target]\n",
    "                    other_prob = other_prob * lm_other.prob_lidstone(ngram, lm_other.order)\n",
    "\n",
    "                # combine the probs with correct lambda weights\n",
    "\n",
    "                # calculate the correct lambda weights based on the cut-off word\n",
    "                lambda_global = 1- lambda_lm1\n",
    "                factor = 0 if cutoff_word_global_lm <=1 else (1-lambda_global)/(cutoff_word_global_lm-1)\n",
    "                num_refs_f = pair_piece_lms_self[pair_num][piece].num_training_sents  # + 1 # to include the current one?\n",
    "                current_lambda_lm1 = 1 - max([lambda_global + (factor * ((((cutoff_word_global_lm-1)-num_refs_f)))), lambda_global])\n",
    "\n",
    "\n",
    "                prob = (current_lambda_lm1 * self_prob) + ((1-current_lambda_lm1) * other_prob)\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                piece_prob_dict[piece] = (-log(prob)/len(ref_info[0].split()), num_refs_f)\n",
    "\n",
    "                # only get the entropy for target piece\n",
    "                if piece == target_piece:\n",
    "                    s += log(prob)\n",
    "                    count +=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            # update self language model, which starts empty for a piece\n",
    "            # then will train as it encounters each piece\n",
    "            #print(pair_piece_lms_self[pair_num][piece].num_training_sents, \"train sents so far before\")\n",
    "            pair_piece_lms_self[pair_num][target_piece].train([ref_info[0].split()])\n",
    "            \n",
    "        \n",
    "            \n",
    "            # could add moving averages for how this piece has been judged probability-wise by each piece model\n",
    "            # built so far in this interaction by different models\n",
    "            # for this instance, we don't know what the correct referent is, so we only have the probs from the models\n",
    "            #  will be num shapes * num shapes\n",
    "            # assumes the latest prob dict for this piece is a new instance for all shapes:\n",
    "            init_probs_from_current = {k: [v[0]] for k, v in piece_prob_dict.items()}\n",
    "            probs_assigned_to_pieces_so_far = {k:copy.deepcopy(init_probs_from_current) for k, v in piece_prob_dict.items()}\n",
    "            # for previous instances, we do know what the correct referents were\n",
    "            # so we can check what the effect of combining the current probs to those from previous positive examples\n",
    "            # NB and negative ones too?\n",
    "            # scroll forward from start of interaction up to but not including current ref\n",
    "            for j in range(0, i):\n",
    "                back_ref_info = piece_references_and_probabilities_per_pair[pair_num][j]\n",
    "                back_ref_prob_dict = back_ref_info[5]\n",
    "                back_ref_target_piece = back_ref_info[4]\n",
    "                \n",
    "                for k, v in back_ref_prob_dict.items():\n",
    "                    if probs_assigned_to_pieces_so_far[back_ref_target_piece].get(k) is None:\n",
    "                        probs_assigned_to_pieces_so_far[back_ref_target_piece][k] = []\n",
    "                    probs_assigned_to_pieces_so_far[back_ref_target_piece][k].append(v[0])\n",
    "            \n",
    "            # will be num shapes * num shapes\n",
    "            moving_average_prob_dicts_all_lms = {k:{} for k, v in piece_prob_dict.items()}\n",
    "            for target_p in probs_assigned_to_pieces_so_far.keys():\n",
    "                for a_p in probs_assigned_to_pieces_so_far[target_p].keys():\n",
    "                    moving_average_prob_dicts_all_lms[target_p][a_p] = np.mean(probs_assigned_to_pieces_so_far[target_p][a_p])\n",
    "                    \n",
    "            \n",
    "            \n",
    "            final_ref_info = tuple(list(ref_info) + [piece_prob_dict, moving_average_prob_dicts_all_lms, len(ref_info[0].split())])\n",
    "            piece_references_and_probabilities_per_pair[pair_num].append(final_ref_info)\n",
    "            \n",
    "\n",
    "        cross_entropy = -s / count\n",
    "        cross_entropies.append(cross_entropy)\n",
    "  \n",
    "    return piece_references_and_probabilities_per_pair\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K': -0.7071067811865476, 'X': -0.7071067811865476, 'Y': 1.4142135623730951}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_zscore_dict_for_prob_dict(prob_dict):\n",
    "    probs  = [item[1] for item in sorted(prob_dict.items(), key=lambda x:x[0])] # alpha\n",
    "    zscores = list(stats.zscore(np.array(probs)))\n",
    "    def convert_nan(v):\n",
    "        if np.isnan(v):\n",
    "            return - 100\n",
    "        return v\n",
    "    return {k: convert_nan(v) for k, v in zip(sorted(prob_dict.keys()), zscores)}\n",
    "\n",
    "get_zscore_dict_for_prob_dict({\"K\": 0.33, \"Y\": 0.35, \"X\": 0.33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifier_data_from_raw(folds, lexical=True, lm_features=True, moving_average=False):\n",
    "    \"\"\"Gets derived data from raw probs from language models\"\"\"\n",
    "    final_data = []\n",
    "    current_pair_num = None\n",
    "    for current_pair_num in folds.keys():\n",
    "        raw_data = folds[current_pair_num]\n",
    "        for ref_info in raw_data:\n",
    "            #('das orange l', 'r1_B', 5887, 204.521, 'L', prob_dict, ref_length)\n",
    "\n",
    "            text, speaker, utt_id, end_time, target_piece, prob_dict, piece_lm_moving_average, ref_length = ref_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #if current_pair_piece_probs.get(piece) is None:\n",
    "            #    current_pair_piece_probs[piece] = []\n",
    "\n",
    "\n",
    "            feature_vector = {}\n",
    "\n",
    "            if lexical:\n",
    "                clean_utt = text.lower().split()\n",
    "                word_dem = len(clean_utt)\n",
    "\n",
    "                local_word_count = Counter()\n",
    "\n",
    "                for word in clean_utt:\n",
    "                    #word_counter[word] += (1/word_dem)   # for global\n",
    "                    #word_piece_counter[piece+\"__\"+word] += (1/word_dem)  #for global\n",
    "                    local_word_count[word] += 1\n",
    "\n",
    "                for k, v in local_word_count.items():\n",
    "                    feature_vector[k] = local_word_count[k]/word_dem\n",
    "\n",
    "\n",
    "\n",
    "            if lm_features:\n",
    "                length_weighted_prob_dict = {k:v[0] for k,v in prob_dict.items()}\n",
    "\n",
    "                #for k, v in length_weighted_prob_dict.items():\n",
    "                #   feature_vector['local_prob_' + k] = v\n",
    "\n",
    "                zscore_dict = get_zscore_dict_for_prob_dict(length_weighted_prob_dict)\n",
    "\n",
    "                for k, v in zscore_dict.items():\n",
    "                    feature_vector['local_zscore_prob_' + k] = v \n",
    "\n",
    "                if moving_average:\n",
    "                    #pass\n",
    "                    # to get a smoother measure of the pacts so far (after the first one), \n",
    "                    # we calculate what the moving average lm score is for each piece lm\n",
    "                    # with which previous pieces and current probs for this pair\n",
    "                    # give for the current piece (not knowing what it is and assuming it's of each type)\n",
    "                    # i.e. assuming it's of a shape (X), what's the moving average of all the piece lm's on this type of piece?\n",
    "                    # you'd need num pieces * num pieces number of moving probs for it to be fair, not just for the target\n",
    "\n",
    "                    for _target in piece_lm_moving_average.keys():\n",
    "                        #if not _target == target_piece: # NB this is cheating!\n",
    "                        #    continue\n",
    "                        zscore_dict_average = get_zscore_dict_for_prob_dict(piece_lm_moving_average[_target])\n",
    "                        for k, v in zscore_dict_average.items():  \n",
    "                            if not k == _target:\n",
    "                                continue\n",
    "                            #feature_vector['local_prob_moving_zcore' + _target + \":\" + k] = v\n",
    "                            #feature_vector['local_prob_moving_zcore' + _target + \":\" + k] =  piece_lm_moving_average[_target][k]\n",
    "                            feature_vector['local_prob_moving_zcore' + _target + \":\" + k] =  v\n",
    "\n",
    "                #zscore_dict_average = get_zscore_dict_for_prob_dict(piece_lm_moving_average)\n",
    "\n",
    "                #for k, v in zscore_dict_average.items():\n",
    "                #    feature_vector['local_zscore_prob_moving_' + k] = v \n",
    "\n",
    "\n",
    "            #rank_dict = get_rank_dict_for_prob_dict(piece_lm_prob_local)\n",
    "\n",
    "            #for k, v in rank_dict.items():\n",
    "            #    feature_vector['local_prob_rank' + k] = v \n",
    "            final_data.append((feature_vector, target_piece))\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def train_classifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    return SklearnClassifier(LinearSVC(loss='squared_hinge')).train(trainData)\n",
    "    #return SklearnClassifier(LogisticRegression()).train(trainData)\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_self': 2, 'k_self': 1.0, 'n_other': 2, 'k_other': 0.618332169500001}\n",
      "{1: (0.7490625000000009, 5.865760074071328), 2: (0.8391875000000009, 5.7896640372730195), 3: (0.8640625000000008, 5.785753739659943), 4: (0.880875000000001, 5.787766806993678), 5: (0.8880000000000009, 5.799222345385503)}\n"
     ]
    }
   ],
   "source": [
    "# experiment with and optimize the best lambda weight on the local model and best cut-off point\n",
    "# in process get the cross validation results\n",
    "print(opt_params)\n",
    "n_self = opt_params[\"n_self\"]\n",
    "n_other = opt_params[\"n_other\"]\n",
    "k_self = opt_params[\"k_self\"]\n",
    "k_other = opt_params[\"k_other\"]\n",
    "cutoff_word_global_lm, lambda_local, x_val_results = get_best_params_cutoff_lambda_xval(n_self, n_other,\n",
    "                                                                                        k_self, k_other,\n",
    "                                                                                        good_pieces, training_folds,\n",
    "                                                                                        cutoff_range=[1,5],\n",
    "                                                                                        lambda_range=[0,1]) \n",
    "opt_params[\"lambda_local\"] = lambda_local\n",
    "opt_params[\"cutoff_word_global_lm\"] = cutoff_word_global_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_self': 2, 'k_self': 1.0, 'n_other': 2, 'k_other': 0.618332169500001, 'lambda_local': 0.8640625000000008, 'cutoff_word_global_lm': 3}\n"
     ]
    }
   ],
   "source": [
    "# finally get the test results with best language models (will have done this 7 times above in each xval)\n",
    "TEST = 'r2'\n",
    "HELDOUT = 'r8'  # take out one of the training folds for the lms?\n",
    "print(opt_params)\n",
    "# {'n_self': 2, 'k_self': 1.0, 'n_other': 2, 'k_other': 0.618332169500001, 'lambda_local': 0.8640625000000008, 'cutoff_word_global_lm': 3}\n",
    "#opt_params['lambda_local'] = 0.8640625000000008\n",
    "#opt_params['cutoff_word_global_lm'] = 3\n",
    "\n",
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "\n",
    "def get_fscore(references_per_pair, test, heldout, n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm):\n",
    "    # TODO could randomly take one out r7 so it's just 6 for training, else language model is different size\n",
    "    training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [test], references_per_pair.keys())}\n",
    "    lms_global = get_new_piece_lms_from_folds(n_other, k_other, good_pieces,\n",
    "                                                      training_folds, target_folds=training_folds.keys(), train=True)\n",
    "    lms_self = get_new_piece_lms_from_folds(n_self, k_self, good_pieces,\n",
    "                                                      training_folds, target_folds=training_folds.keys(), train=False)\n",
    "\n",
    "    raw_train_data = generate_classifier_data_from_lms_and_params(lms_self,\n",
    "                                                                  lms_global,\n",
    "                                                                  good_pieces,\n",
    "                                                                  lambda_local,\n",
    "                                                                  cutoff_word_global_lm,\n",
    "                                                                  training_folds)\n",
    "\n",
    "    train_data = generate_classifier_data_from_raw(raw_train_data, lexical=True, lm_features=True, moving_average=True)\n",
    "    #print(\"length training data\", len(train_data))\n",
    "    #print(train_data[0])\n",
    "\n",
    "\n",
    "    # now generate the test data using LMs trained on all the other folds\n",
    "    \n",
    "    all_training_folds = {k:references_per_pair[k] for k in filter(lambda x:x not in [heldout], references_per_pair.keys())}\n",
    "    test_folds ={k:references_per_pair[k] for k in filter(lambda x:x==test, references_per_pair.keys())}\n",
    "\n",
    "    lms_global = get_new_piece_lms_from_folds(n_other, k_other, good_pieces,\n",
    "                                                      all_training_folds, target_folds=[test], train=True)\n",
    "    lms_self = get_new_piece_lms_from_folds(n_self, k_self, good_pieces,\n",
    "                                                      test_folds, target_folds=[test], train=False)\n",
    "\n",
    "\n",
    "    raw_test_data = generate_classifier_data_from_lms_and_params(lms_self,\n",
    "                                                                 lms_global,\n",
    "                                                                 good_pieces,\n",
    "                                                                 lambda_local,\n",
    "                                                                 cutoff_word_global_lm,\n",
    "                                                                 test_folds)\n",
    "\n",
    "\n",
    "    test_data = generate_classifier_data_from_raw(raw_test_data, lexical=True, lm_features=True, moving_average=True)\n",
    "    #print(test_data[0])\n",
    "\n",
    "\n",
    "    cl = train_classifier(train_data)\n",
    "    #print(train_data[0])\n",
    "    y_true = [x[1] for x in test_data]\n",
    "    y_pred = predict_labels(test_data, cl)\n",
    "    prf = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(prf)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    return prf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8253030303030303, 0.78, 0.779216276477146, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9166345190654965, 0.9132075471698113, 0.9130725322766144, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9044023256920704, 0.887719298245614, 0.8914993743133893, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9032582801813569, 0.8881118881118881, 0.8895411562019954, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8833943240082681, 0.8682170542635659, 0.8658178141796523, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8876336425274548, 0.8636363636363636, 0.8575700778096376, None)\n",
      "Training Classifier...\n",
      "(0.7771469178497548, 0.7668711656441718, 0.761291587495105, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "\n",
    "for test in ['r1', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8']:\n",
    "    get_fscore(references_per_pair, test, \"\", n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7558866133866133, 0.71, 0.7113646981038286, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9156466676835816, 0.8981132075471698, 0.8969616973323173, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8962309837972213, 0.887719298245614, 0.8905707277479048, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8706085581085581, 0.8531468531468531, 0.8575328025499532, None)\n",
      "Training Classifier...\n",
      "(0.7964580964128828, 0.7403100775193798, 0.7166355215128715, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "(0.8627141293050383, 0.8522727272727273, 0.851100426624943, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "(0.7841101654841741, 0.7361963190184049, 0.7341779527716643, None)\n",
      "[1, 1, 1, 0.05, 0.05, 0.0] 0.8083348323776404\n",
      "best 0.8083348323776404 [1, 1, 1, 0.05, 0.05, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7438790376290377, 0.7, 0.7024714174714174, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9156466676835816, 0.8981132075471698, 0.8969616973323173, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8962309837972213, 0.887719298245614, 0.8905707277479048, None)\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8706085581085581, 0.8531468531468531, 0.8575328025499532, None)\n",
      "Training Classifier...\n",
      "(0.7964580964128828, 0.7403100775193798, 0.7166355215128715, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8627141293050383, 0.8522727272727273, 0.851100426624943, None)\n",
      "Training Classifier...\n",
      "(0.7841101654841741, 0.7361963190184049, 0.7341779527716643, None)\n",
      "[1, 1, 2, 0.05, 0.05, 0.0] 0.8070643637158674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_other = opt_params['n_other']\n",
    "n_self = opt_params['n_self']\n",
    "k_other = opt_params['k_other']\n",
    "k_self = opt_params['k_self']\n",
    "lambda_local = opt_params['lambda_local']\n",
    "cutoff_word_global_lm = opt_params[\"cutoff_word_global_lm\"]\n",
    "\n",
    "cross_val_data = {k:references_per_pair[k] for k in filter(lambda x:x !='r2', references_per_pair.keys())}\n",
    "results = {}\n",
    "\n",
    "best_f = 0\n",
    "best_params = []\n",
    "for cutoff_word_global_lm in range(1,11):\n",
    "    for lambda_local_raw in range(0,105,5):\n",
    "        lambda_local = lambda_local_raw/100\n",
    "        all_scores = []\n",
    "        for test in ['r1', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8']:\n",
    "            fscore = get_fscore(cross_val_data, test, \"\", n_other, n_self, k_other, k_self, lambda_local, cutoff_word_global_lm)\n",
    "            all_scores.append(fscore)\n",
    "        mean_f = np.mean(all_scores)\n",
    "        test_params = [n_other, n_self, cutoff_word_global_lm, k_other,k_self, lambda_local]\n",
    "        print(test_params, mean_f)\n",
    "        results[\",\".join([str(f) for f in test_params])] = mean_f\n",
    "        if mean_f == best_f:\n",
    "            best_params = [best_params, test_params]\n",
    "        elif mean_f > best_f:\n",
    "            best_f = mean_f\n",
    "            best_params = test_params\n",
    "            print(\"best\", best_f, best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#results_file = open(\"results\" + datetime.time)\n",
    "#for row in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "results_file = open(datetime.datetime.now().isoformat() + \".csv\", \"w\")\n",
    "results_file.write(\",\".join(['n_other', 'n_self', 'cutoff_word_global_lm', 'k_other', 'k_self', 'lambda_local', 'fscore']) + \"\\n\")\n",
    "for params, result in sorted(results.items(), key=lambda x:x[1], reverse=True):\n",
    "    results_file.write(params + \",\" + str(result) + \"\\n\")\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_other</th>\n",
       "      <th>n_self</th>\n",
       "      <th>cutoff_word_global_lm</th>\n",
       "      <th>k_other</th>\n",
       "      <th>k_self</th>\n",
       "      <th>lambda_local</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_other  n_self  cutoff_word_global_lm  k_other  k_self  lambda_local   \n",
       "0        1       1                      1     0.05    0.05           0.0  \\\n",
       "1        1       1                      2     0.05    0.05           0.0   \n",
       "\n",
       "     fscore  \n",
       "0  0.808335  \n",
       "1  0.807064  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#pd.read_csv(open(\"2023-06-11T23:29:46.235511\"))\n",
    "print(\"finished\")\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
